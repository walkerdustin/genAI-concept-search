{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import pickle\n",
    "import pythonbible as bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>v</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>In the beginning God{After \"God,\" the Hebrew h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Now the earth was formless and empty. Darkness...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>God said, \"Let there be light,\" and there was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>God saw the light, and saw that it was good. G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001005</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>God called the light Day, and the darkness he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31097</th>\n",
       "      <td>66022017</td>\n",
       "      <td>66</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>The Spirit and the bride say, \"Come!\" He who h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31098</th>\n",
       "      <td>66022018</td>\n",
       "      <td>66</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>I testify to everyone who hears the words of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31099</th>\n",
       "      <td>66022019</td>\n",
       "      <td>66</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>If anyone takes away from the words of the boo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31100</th>\n",
       "      <td>66022020</td>\n",
       "      <td>66</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>He who testifies these things says, \"Yes, I co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31101</th>\n",
       "      <td>66022021</td>\n",
       "      <td>66</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>The grace of the Lord Jesus Christ be with all...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31102 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id   b   c   v                                                  t\n",
       "0       1001001   1   1   1  In the beginning God{After \"God,\" the Hebrew h...\n",
       "1       1001002   1   1   2  Now the earth was formless and empty. Darkness...\n",
       "2       1001003   1   1   3  God said, \"Let there be light,\" and there was ...\n",
       "3       1001004   1   1   4  God saw the light, and saw that it was good. G...\n",
       "4       1001005   1   1   5  God called the light Day, and the darkness he ...\n",
       "...         ...  ..  ..  ..                                                ...\n",
       "31097  66022017  66  22  17  The Spirit and the bride say, \"Come!\" He who h...\n",
       "31098  66022018  66  22  18  I testify to everyone who hears the words of t...\n",
       "31099  66022019  66  22  19  If anyone takes away from the words of the boo...\n",
       "31100  66022020  66  22  20  He who testifies these things says, \"Yes, I co...\n",
       "31101  66022021  66  22  21  The grace of the Lord Jesus Christ be with all...\n",
       "\n",
       "[31102 rows x 5 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verses = pd.read_csv(\"t_web.csv\")\n",
    "# prompts should be a list of strings \"{citation}: {text}\"\n",
    "verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mathew_web_ids = bible.convert_reference_to_verse_ids(bible.NormalizedReference(book=bible.Book.MATTHEW, start_chapter=1, start_verse=1, end_chapter=28, end_verse=20, end_book=None),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The book of the generation of Jesus Christ{Christ (Greek) and Messiah (Hebrew) both mean \"Anointed One\"}, the son of David, the son of Abraham.',\n",
       "       'Abraham became the father of Isaac. Isaac became the father of Jacob. Jacob became the father of Judah and his brothers.',\n",
       "       'Judah became the father of Perez and Zerah by Tamar. Perez became the father of Hezron. Hezron became the father of Ram.',\n",
       "       ...,\n",
       "       'Jesus came to them and spoke to them, saying, \"All authority has been given to me in heaven and on earth.',\n",
       "       'Therefore go, and make disciples of all nations, baptizing them in the name of the Father and of the Son and of the Holy Spirit,',\n",
       "       'teaching them to observe all things that I commanded you. Behold, I am with you always, even to the end of the age.\" Amen.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the verses coresponding to the  ids\n",
    "prompts = verses.loc[verses[\"id\"].isin(mathew_web_ids), \"t\"].values\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    if '\"' in prompt:\n",
    "        print(\"prompt has double-quote character\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1071"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=mErc1WIkLkqo1tTGeE4pJqw0pxuxbY&access_type=offline&code_challenge=ZZBiEY_FM_l9JnGVMkc_jgFJjrE5hUieLBzzZAJ59Rc&code_challenge_method=S256\n",
      "\n",
      "\n",
      "You are now logged in as [du.wal1998@gmail.com].\n",
      "Your current project is [try-out-vertex-ai].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "GoogleAuthError",
     "evalue": "\nUnable to authenticate your request.\nDepending on your runtime environment, you can complete authentication by:\n- if in local JupyterLab instance: `!gcloud auth login` \n- if in Colab:\n    -`from google.colab import auth`\n    -`auth.authenticate_user()`\n- if in service account or other: please follow guidance in https://cloud.google.com/docs/authentication",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mC:\\git_repos\\local_LLM_experiment\\venv\\lib\\site-packages\\vertexai\\_model_garden\\_model_garden_models.py:335\u001b[0m, in \u001b[0;36m_ModelGardenModel.from_pretrained\u001b[1;34m(cls, model_name)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterface_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m auth_exceptions\u001b[38;5;241m.\u001b[39mGoogleAuthError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\git_repos\\local_LLM_experiment\\venv\\lib\\site-packages\\vertexai\\_model_garden\\_model_garden_models.py:223\u001b[0m, in \u001b[0;36m_from_pretrained\u001b[1;34m(interface_class, model_name, publisher_model, tuned_vertex_model)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    220\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minterface_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a correct model interface class since it does not have an instance schema URI.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    221\u001b[0m         )\n\u001b[1;32m--> 223\u001b[0m     model_info \u001b[38;5;241m=\u001b[39m \u001b[43m_get_model_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema_to_class_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43minterface_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_INSTANCE_SCHEMA_URI\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface_class\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\git_repos\\local_LLM_experiment\\venv\\lib\\site-packages\\vertexai\\_model_garden\\_model_garden_models.py:139\u001b[0m, in \u001b[0;36m_get_model_info\u001b[1;34m(model_id, schema_to_class_map, interface_class, publisher_model_res, tuned_vertex_model)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m publisher_model_res:\n\u001b[0;32m    138\u001b[0m     publisher_model_res \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 139\u001b[0m         \u001b[43m_publisher_models\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_PublisherModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresource_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_gca_resource\n\u001b[0;32m    142\u001b[0m     )\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m publisher_model_res\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublishers/google/models/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\git_repos\\local_LLM_experiment\\venv\\lib\\site-packages\\google\\cloud\\aiplatform\\_publisher_models.py:63\u001b[0m, in \u001b[0;36m_PublisherModel.__init__\u001b[1;34m(self, resource_name, project, location, credentials)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves an existing PublisherModel resource given a resource name or model garden id.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m        Overrides credentials set in aiplatform.init.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_resource_name(resource_name):\n",
      "File \u001b[1;32mC:\\git_repos\\local_LLM_experiment\\venv\\lib\\site-packages\\google\\cloud\\aiplatform\\base.py:512\u001b[0m, in \u001b[0;36mVertexAiResourceNoun.__init__\u001b[1;34m(self, project, location, credentials, resource_name)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocation \u001b[38;5;241m=\u001b[39m location \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mlocation\n\u001b[1;32m--> 512\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials \u001b[38;5;241m=\u001b[39m credentials \u001b[38;5;129;01mor\u001b[39;00m \u001b[43minitializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcredentials\u001b[49m\n\u001b[0;32m    514\u001b[0m appended_user_agent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\git_repos\\local_LLM_experiment\\venv\\lib\\site-packages\\google\\cloud\\aiplatform\\initializer.py:320\u001b[0m, in \u001b[0;36m_Config.credentials\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m logger\u001b[38;5;241m.\u001b[39maddFilter(logging_warning_filter)\n\u001b[1;32m--> 320\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_project_as_env_var_or_google_auth_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m credentials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credentials\n",
      "File \u001b[1;32mC:\\git_repos\\local_LLM_experiment\\venv\\lib\\site-packages\\google\\cloud\\aiplatform\\initializer.py:95\u001b[0m, in \u001b[0;36m_Config._set_project_as_env_var_or_google_auth_default\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credentials:\n\u001b[1;32m---> 95\u001b[0m     credentials, _ \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credentials \u001b[38;5;241m=\u001b[39m credentials\n",
      "File \u001b[1;32mC:\\git_repos\\local_LLM_experiment\\venv\\lib\\site-packages\\google\\auth\\_default.py:691\u001b[0m, in \u001b[0;36mdefault\u001b[1;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[1;32m--> 691\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGoogleAuthError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m vertexai\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtry-out-vertex-ai\u001b[39m\u001b[38;5;124m\"\u001b[39m, location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus-central1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_output_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m12\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      7\u001b[0m }\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTextGenerationModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-bison\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"You are an expert in the use of the English language. You will get single paragraphs of the bible as an input and output in a single number (from 0 to 100)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mhow much this paragraph says about the concept of seeking discomfort. It is not relevant if the verse suggests that seeking discomfort is good or bad, it is about if the bible verse says anything about \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparameters\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse from Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\git_repos\\local_LLM_experiment\\venv\\lib\\site-packages\\vertexai\\_model_garden\\_model_garden_models.py:337\u001b[0m, in \u001b[0;36m_ModelGardenModel.from_pretrained\u001b[1;34m(cls, model_name)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_pretrained(interface_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, model_name\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m auth_exceptions\u001b[38;5;241m.\u001b[39mGoogleAuthError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m auth_exceptions\u001b[38;5;241m.\u001b[39mGoogleAuthError(credential_exception_str) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mGoogleAuthError\u001b[0m: \nUnable to authenticate your request.\nDepending on your runtime environment, you can complete authentication by:\n- if in local JupyterLab instance: `!gcloud auth login` \n- if in Colab:\n    -`from google.colab import auth`\n    -`auth.authenticate_user()`\n- if in service account or other: please follow guidance in https://cloud.google.com/docs/authentication"
     ]
    }
   ],
   "source": [
    "vertexai.init(project=\"try-out-vertex-ai\", location=\"us-central1\")\n",
    "parameters = {\n",
    "    \"candidate_count\": 1,\n",
    "    \"max_output_tokens\": 12,\n",
    "    \"temperature\": 0,\n",
    "    \"top_k\": 1\n",
    "}\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison\")\n",
    "response = model.predict(\n",
    "    \"\"\"You are an expert in the use of the English language. You will get single paragraphs of the bible as an input and output in a single number (from 0 to 100)\n",
    "how much this paragraph says about the concept of seeking discomfort. It is not relevant if the verse suggests that seeking discomfort is good or bad, it is about if the bible verse says anything about \n",
    "seeking discomfort in general. \n",
    "As guidance:\n",
    "\\'Book 1:30: And the Lord said you shall seek discomfort in all aspects of your life\\' -> very high score\n",
    "\\'Book 1:30: And the Lord said you should make sure, that you never venture out of your comfort zone\\' -> very high score\n",
    "\\'Book 1:30: Marc felt uncomfortable\\' -> high score\n",
    "\\'Book 1:30: Marc built himself a cosy bed so that he can live comfortably\\' -> high score\n",
    "\\'Book 1:30: Jesus made a difficult ride of 2 days to the next city\\' -> medium score\n",
    "\\'Book 1:30: And so Jesus made water to wine\\' -> low score\n",
    "For the next prompt, you will output only one number. 0 represents the lowest correlation to the concept of comfort or \n",
    "discomfort and 100 represents the highest correlation to the concept of comfort and discomfort.\n",
    "Only output a single number. Do not output any explanation or context.\n",
    "\n",
    "input: Matthew 7:18,Matthew,7,18,\\\"A good tree cannot bring forth evil fruit, neither can a corrupt tree bring forth good fruit.\\\"\n",
    "\n",
    "\n",
    "\n",
    "output:\n",
    "\"\"\",\n",
    "    **parameters\n",
    ")\n",
    "print(f\"Response from Model: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calc how expensive this should be for different providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=\"try-out-vertex-ai\", location=\"us-central1\")\n",
    "parameters = {\n",
    "    \"candidate_count\": 1,\n",
    "    \"max_output_tokens\": 12,\n",
    "    \"temperature\": 0,\n",
    "    \"top_k\": 1\n",
    "}\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison\")\n",
    "response = model.predict(\n",
    "    \"\"\"You are an expert in the use of the English language. You will get single paragraphs of the bible as an input and output in a single number (from 0 to 100)\n",
    "how much this paragraph says about the concept of seeking discomfort. It is not relevant if the verse suggests that seeking discomfort is good or bad, it is about if the bible verse says anything about \n",
    "seeking discomfort in general. \n",
    "As guidance:\n",
    "\\'Book 1:30: And the Lord said you shall seek discomfort in all aspects of your life\\' -> very high score\n",
    "\\'Book 1:30: And the Lord said you should make sure, that you never venture out of your comfort zone\\' -> very high score\n",
    "\\'Book 1:30: Marc felt uncomfortable\\' -> high score\n",
    "\\'Book 1:30: Marc built himself a cosy bed so that he can live comfortably\\' -> high score\n",
    "\\'Book 1:30: Jesus made a difficult ride of 2 days to the next city\\' -> medium score\n",
    "\\'Book 1:30: And so Jesus made water to wine\\' -> low score\n",
    "For the next prompt, you will output only one number. 0 represents the lowest correlation to the concept of comfort or \n",
    "discomfort and 100 represents the highest correlation to the concept of comfort and discomfort.\n",
    "Only output a single number. Do not output any explanation or context.\n",
    "\n",
    "input: Matthew 7:18,Matthew,7,18,\\\"A good tree cannot bring forth evil fruit, neither can a corrupt tree bring forth good fruit.\\\"\n",
    "\n",
    "\n",
    "\n",
    "output:\n",
    "\"\"\",\n",
    "    **parameters\n",
    ")\n",
    "print(f\"Response from Model: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"You are an expert in the use of the English language. You will get single paragraphs of the bible as an input and output in a single number (from 0 to 100)\n",
    "how much this paragraph says about the concept of seeking discomfort. It is not relevant if the verse suggests that seeking discomfort is good or bad, it is about if the bible verse says anything about \n",
    "seeking discomfort in general. \n",
    "As guidance:\n",
    "\\'Book 1:30: And the Lord said you shall seek discomfort in all aspects of your life\\' -> very high score\n",
    "\\'Book 1:30: And the Lord said you should make sure, that you never venture out of your comfort zone\\' -> very high score\n",
    "\\'Book 1:30: Marc felt uncomfortable\\' -> high score\n",
    "\\'Book 1:30: Marc built himself a cosy bed so that he can live comfortably\\' -> high score\n",
    "\\'Book 1:30: Jesus made a difficult ride of 2 days to the next city\\' -> medium score\n",
    "\\'Book 1:30: And so Jesus made water to wine\\' -> low score\n",
    "For the next prompt, you will output only one number. 0 represents the lowest correlation to the concept of comfort or \n",
    "discomfort and 100 represents the highest correlation to the concept of comfort and discomfort.\n",
    "Only output a single number. Do not output any explanation or context.\"\"\"\n",
    "len(sys_prompt.split()) # number of tokens in system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_input_tokens = len(\"\".join(prompts).split()) + len(sys_prompt.split())*len(prompts)\n",
    "total_input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_output_tokens = len(prompts)*2\n",
    "total_output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now calculate the tokens with openAIs tokenizer tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding with gpt-3 tokeniser\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "tiktoken_tokenizer_len = len(enc.encode(\"\\n\".join(prompts)))\n",
    "tiktoken_tokenizer_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = len(\"\".join(prompts).split())\n",
    "total_chars = len(\"\".join(prompts))\n",
    "print(f\"openai tokenizer average words per token: {total_words/tiktoken_tokenizer_len}\")\n",
    "print(f\"openai tokenizer average chars per token: {total_chars/tiktoken_tokenizer_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"\\n\".join(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.encode(\"\\n\".join(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktoken.list_encoding_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding with gpt-3 tokeniser\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "total_input_tokens = len(enc.encode(\"\\n\".join(prompts))) + len(enc.encode(sys_prompt))*len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost for openAi GPT-3.5\n",
    "total_input_tokens * 1/1_000_000 + total_output_tokens * 2/1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost for openAi GPT-4\n",
    "total_input_tokens * 30/1_000_000 + total_output_tokens * 60/1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost for google vertex-ai gemini pro\n",
    "total_input_tokens * 0.25/1_000_000 + total_output_tokens * 0.5/1_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run it with openai and langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run ollama run seeker2 \"{prommpt}\" for all prompts ans save the return value\n",
    "# vers_ratings_dict = dict()\n",
    "# i = 0\n",
    "# N = len(prompts)\n",
    "# for prompt in prompts:\n",
    "#     i += 1\n",
    "#     if i % 100 == 0:\n",
    "#         print(f'Processing prompt {i} => {i/N:.2%}')\n",
    "#         # save dict with pickle \n",
    "#         pickle.dump(vers_ratings_dict, open( \"vers_ratings_dict.p\", \"wb\" ))\n",
    "#     try:\n",
    "#         command = f'ollama run seeker2 \"{prompt}\"'\n",
    "#         print(command)\n",
    "#         out = subprocess.check_output(command, shell=True).decode('utf-8')\n",
    "#         print(out)\n",
    "#         vers_ratings_dict[prompt] = out\n",
    "#     except:\n",
    "#         print(f'Execption while processing prompt: \"{prompt}\"')\n",
    "#         continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = 'ollama run seeker2 \"Matthew 7:13, Enter ye in at the strait gate: for wide is the gate, and broad is the way, that leadeth to destruction, and many there be which go in thereat:\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"ollama\"\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = subprocess.run(command, shell=True, check=True, capture_output=True)\n",
    "print(ret.stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "ollama = Ollama(base_url='http://localhost:11434',\n",
    "model=\"seeker2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.predict(\"Genesis 1:1: In the beginning God created the heaven and the earth.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ollama(\"Matthew 7:13, Enter ye in at the strait gate: for wide is the gate, and broad is the way, that leadeth to destruction, and many there be which go in thereat:\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
