,Unnamed: 0,vers_text,bible_vers_id,gpt_3_5_response_objects,gpt_3_5_ratings,gemini_pro_response_objects,gemini_pro_ratings,text_bison_response_objects,text_bison_ratings,vers_references
0,0,"The book of the generation of Jesus Christ{Christ (Greek) and Messiah (Hebrew) both mean ""Anointed One""}, the son of David, the son of Abraham.",40001001,"ChatCompletion(id='chatcmpl-8dJih4RJE7RLpLmqpBtpcX0Ea1iby', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381287, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:1
1,1,Abraham became the father of Isaac. Isaac became the father of Jacob. Jacob became the father of Judah and his brothers.,40001002,"ChatCompletion(id='chatcmpl-8dJihASa6FvfMt5Hp9yEWOm7w9oFO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381287, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:2
2,2,Judah became the father of Perez and Zerah by Tamar. Perez became the father of Hezron. Hezron became the father of Ram.,40001003,"ChatCompletion(id='chatcmpl-8dJiiz0eqtFTyX44Dj1SZ8K1hE6v2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381288, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:3
3,3,Ram became the father of Amminadab. Amminadab became the father of Nahshon. Nahshon became the father of Salmon.,40001004,"ChatCompletion(id='chatcmpl-8dJijPniBoPNg5LDY2t3CAvU86oQc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381289, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.3, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:4
4,4,Salmon became the father of Boaz by Rahab. Boaz became the father of Obed by Ruth. Obed became the father of Jesse.,40001005,"ChatCompletion(id='chatcmpl-8dJijlln8vxVkInb6DpePN5slVAlI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381289, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.5, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:5
5,5,Jesse became the father of David the king. David became the father of Solomon by her who had been the wife of Uriah.,40001006,"ChatCompletion(id='chatcmpl-8dJikw2XBSHGHuy8ZqWrYylDABAmw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381290, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.2, 0.1, 0.1, 1.0, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:6
6,6,Solomon became the father of Rehoboam. Rehoboam became the father of Abijah. Abijah became the father of Asa.,40001007,"ChatCompletion(id='chatcmpl-8dJilpgsQ00eZupW8CGXAGyfbcfeG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381291, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.3, 0.2, 0.3, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:7
7,7,Asa became the father of Jehoshaphat. Jehoshaphat became the father of Joram. Joram became the father of Uzziah.,40001008,"ChatCompletion(id='chatcmpl-8dJilKWXGoS0FtAMAedyvB5oPxZrl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381291, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:8
8,8,Uzziah became the father of Jotham. Jotham became the father of Ahaz. Ahaz became the father of Hezekiah.,40001009,"ChatCompletion(id='chatcmpl-8dJimfYjZ11hyNuYg115q6xhhPAdr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381292, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:9
9,9,Hezekiah became the father of Manasseh. Manasseh became the father of Amon. Amon became the father of Josiah.,40001010,"ChatCompletion(id='chatcmpl-8dJimxPqvw6zADMTxXA0O1kH12vX8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381292, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.5, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:10
10,10,"Josiah became the father of Jechoniah and his brothers, at the time of the exile to Babylon.",40001011,"ChatCompletion(id='chatcmpl-8dJinWzwR0myiZfNzGTBU9g1y6cf8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381293, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.3, 0.3, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.3, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:11
11,11,"After the exile to Babylon, Jechoniah became the father of Shealtiel. Shealtiel became the father of Zerubbabel.",40001012,"ChatCompletion(id='chatcmpl-8dJinRKaN8QV2VGZ7D7uZiJOGsBJy', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381293, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.3, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:12
12,12,Zerubbabel became the father of Abiud. Abiud became the father of Eliakim. Eliakim became the father of Azor.,40001013,"ChatCompletion(id='chatcmpl-8dJioz3CW71hYLtOzCY5zUSfCFT3K', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381294, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:13
13,13,Azor became the father of Sadoc. Sadoc became the father of Achim. Achim became the father of Eliud.,40001014,"ChatCompletion(id='chatcmpl-8dJioxtnnx9LdAaARQaFcGFi3dEeS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381294, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.4, 0.1, 0.2, 0.2, 0.1, 0.9, 0.2], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:14
14,14,Eliud became the father of Eleazar. Eleazar became the father of Matthan. Matthan became the father of Jacob.,40001015,"ChatCompletion(id='chatcmpl-8dJipwdyIuKi3yFkLL1ZeUYQr3N0H', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381295, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.5, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:15
15,15,"Jacob became the father of Joseph, the husband of Mary, from whom was born Jesus{""Jesus"" is a Greek variant of the Jewish name ""Yehoshua,"" which means ""Yah saves."" ""Jesus"" is also the masculine form of ""Yeshu'ah,"" which means ""Salvation.""}, who is called Christ.",40001016,"ChatCompletion(id='chatcmpl-8dJiphvBI0gGjNLUALbTqHmMpTUgg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381295, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=365, total_tokens=366))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 369
  candidates_token_count: 1
  total_token_count: 370
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:16
16,16,"So all the generations from Abraham to David are fourteen generations; from David to the exile to Babylon fourteen generations; and from the carrying away to Babylon to the Christ, fourteen generations.",40001017,"ChatCompletion(id='chatcmpl-8dJiqzBz38GwUEM9t6l1JgW7Jwphw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381296, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.4, 0.2, 0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:17
17,17,"Now the birth of Jesus Christ was like this; for after his mother, Mary, was engaged to Joseph, before they came together, she was found pregnant by the Holy Spirit.",40001018,"ChatCompletion(id='chatcmpl-8dJircH2Ki4emcFZUqs77GZCKOnJV', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381297, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.6, 0.2, 0.1, 1.0, 0.4, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.4, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:18
18,18,"Joseph, her husband, being a righteous man, and not willing to make her a public example, intended to put her away secretly.",40001019,"ChatCompletion(id='chatcmpl-8dJirEZsoBsxbhpNjplxpncOxadk4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381297, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.2, 0.1, 0.3, 0.2, 0.2, 0.1, 0.1, 0.9, 0.3, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:19
19,19,"But when he thought about these things, behold, an angel of the Lord appeared to him in a dream, saying, ""Joseph, son of David, don't be afraid to take to yourself Mary, your wife, for that which is conceived in her is of the Holy Spirit.",40001020,"ChatCompletion(id='chatcmpl-8dJisGA5BKoywWlxE9p128Qdnwbsp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381298, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=353, total_tokens=354))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 361
  candidates_token_count: 1
  total_token_count: 362
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.1, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:20
20,20,"She shall bring forth a son. You shall call his name Jesus, for it is he who shall save his people from their sins.""",40001021,"ChatCompletion(id='chatcmpl-8dJisAad1zz1RdVUkWHX9X0YBrWdd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381298, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:21
21,21,"Now all this has happened, that it might be fulfilled which was spoken by the Lord through the prophet, saying,",40001022,"ChatCompletion(id='chatcmpl-8dJitaYPvCuuZzrGPQ8GoEmn8Myd4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381299, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:22
22,22,"""Behold, the virgin shall be with child, And shall bring forth a son. They shall call his name Immanuel;"" Which is, being interpreted, ""God with us.""",40001023,"ChatCompletion(id='chatcmpl-8dJiuQxyeKe5IvZ0DjjWmjdFe5I5w', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381300, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:23
23,23,"Joseph arose from his sleep, and did as the angel of the Lord commanded him, and took his wife to himself;",40001024,"ChatCompletion(id='chatcmpl-8dJiuP9hWmMhvwSEeKWxyKJgntL5a', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381300, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.2, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:24
24,24,and didn't know her sexually until she had brought forth her firstborn son. He named him Jesus.,40001025,"ChatCompletion(id='chatcmpl-8dJivhNS1kL0zhdexwq02yme4s4OS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381301, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  total_token_count: 325
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.6, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.9, 0.2, 0.1, 1.0, 0.6, 0.3], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.9, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.6, 'Toxic': 0.3}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 1:25
25,25,"Now when Jesus was born in Bethlehem of Judea in the days of Herod the king, behold, wise men{The word for ""wise men"" (magoi) can also mean teachers, priests, physicians, astrologers, seers, interpreters of dreams, or sorcerers.} from the east came to Jerusalem, saying,",40002001,"ChatCompletion(id='chatcmpl-8dJiv6ok9L0U0oxhA3ANxKOGE2Nmn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381301, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=368, total_tokens=369))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 369
  candidates_token_count: 1
  total_token_count: 370
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 1.0, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:1
26,26,"""Where is he who is born King of the Jews? For we saw his star in the east, and have come to worship him.""",40002002,"ChatCompletion(id='chatcmpl-8dJiwFMzqgjrASyRN3Rok7bUYLjqY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381302, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.2, 0.3, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:2
27,27,"When Herod the king heard it, he was troubled, and all Jerusalem with him.",40002003,"ChatCompletion(id='chatcmpl-8dJixQJYD5i3iff61J9zQHmQ5BWw2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381303, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.3, 0.3, 0.6, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.3, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:3
28,28,"Gathering together all the chief priests and scribes of the people, he asked them where the Christ would be born.",40002004,"ChatCompletion(id='chatcmpl-8dJixZtFWti1GpwgomCNhauMKvJnm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381303, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:4
29,29,"They said to him, ""In Bethlehem of Judea, for thus it is written through the prophet,",40002005,"ChatCompletion(id='chatcmpl-8dJiyqvCg0o1S8coEyAMY1dKWel4G', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381304, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:5
30,30,"'You Bethlehem, land of Judah, Are in no way least among the princes of Judah: For out of you shall come forth a governor, Who shall shepherd my people, Israel.'""",40002006,"ChatCompletion(id='chatcmpl-8dJizhRqzL1rD8VQcIRfP7wIx3aqx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381305, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=335, total_tokens=336))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 340
  candidates_token_count: 1
  total_token_count: 341
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.2, 0.3, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:6
31,31,"Then Herod secretly called the wise men, and learned from them exactly what time the star appeared.",40002007,"ChatCompletion(id='chatcmpl-8dJizcAIl9mfecqqz3J0lyjQh35Yi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381305, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.2, 0.6, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:7
32,32,"He sent them to Bethlehem, and said, ""Go and search diligently for the young child. When you have found him, bring me word, so that I also may come and worship him.""",40002008,"ChatCompletion(id='chatcmpl-8dJj0UVxLvRZq9mYny3BUfSSV5TwY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381306, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=335, total_tokens=336))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 342
  candidates_token_count: 2
  total_token_count: 344
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:8
33,33,"They, having heard the king, went their way; and behold, the star, which they saw in the east, went before them, until it came and stood over where the young child was.",40002009,"ChatCompletion(id='chatcmpl-8dJj1l1b95GRO5pB3KdoyxX0lx91m', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381307, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=336, total_tokens=337))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 343
  candidates_token_count: 1
  total_token_count: 344
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:9
34,34,"When they saw the star, they rejoiced with exceedingly great joy.",40002010,"ChatCompletion(id='chatcmpl-8dJj1M3Rxu2N6khMNkZVEAJWPOrFl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381307, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:10
35,35,"They came into the house and saw the young child with Mary, his mother, and they fell down and worshiped him. Opening their treasures, they offered to him gifts: gold, frankincense, and myrrh.",40002011,"ChatCompletion(id='chatcmpl-8dJj2LBS8G4FWXWDSFOa7cFhWEYgn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381308, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=342, total_tokens=343))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 348
  candidates_token_count: 1
  total_token_count: 349
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.3, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:11
36,36,"Being warned in a dream that they shouldn't return to Herod, they went back to their own country another way.",40002012,"ChatCompletion(id='chatcmpl-8dJj2WUkgcrXCoCVW5YjabDUrWMsI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381308, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.5, 0.3, 0.2, 0.6, 0.1, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:12
37,37,"Now when they had departed, behold, an angel of the Lord appeared to Joseph in a dream, saying, ""Arise and take the young child and his mother, and flee into Egypt, and stay there until I tell you, for Herod will seek the young child to destroy him.""",40002013,"ChatCompletion(id='chatcmpl-8dJj3fUsh6GTgveSFy1bGJYneWZqI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381309, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=355, total_tokens=356))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 361
  candidates_token_count: 2
  total_token_count: 363
}
",20,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.3, 0.3, 0.2, 0.1, 0.1, 1.0, 0.3, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.2, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 2:13
38,38,"He arose and took the young child and his mother by night, and departed into Egypt,",40002014,"ChatCompletion(id='chatcmpl-8dJj4jmPaBliStDPYef4d175f3ZVV', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381310, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.3, 0.2, 0.1, 0.1, 0.1, 1.0, 0.3, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.2, 'Legal': 0.1, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:14
39,39,"and was there until the death of Herod; that it might be fulfilled which was spoken by the Lord through the prophet, saying, ""Out of Egypt I called my son.""",40002015,"ChatCompletion(id='chatcmpl-8dJj41kBdTpR1A4NGSIoGs2q8XWOb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381310, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.5, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:15
40,40,"Then Herod, when he saw that he was mocked by the wise men, was exceedingly angry, and sent out, and killed all the male children who were in Bethlehem and in all the surrounding countryside, from two years old and under, according to the exact time which he had learned from the wise men.",40002016,"ChatCompletion(id='chatcmpl-8dJj5bCBFtabDWvGzpJZovT5LI2zp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381311, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=358, total_tokens=359))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 364
  candidates_token_count: 1
  total_token_count: 365
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.4, 0.2, 0.4, 0.2, 0.3, 0.2, 0.1, 1.0, 0.2, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.4, 'Health': 0.2, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:16
41,41,"Then that which was spoken by Jeremiah the prophet was fulfilled, saying,",40002017,"ChatCompletion(id='chatcmpl-8dJj6ao01NYyssBkWsJNlgRsbr4dB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381312, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:17
42,42,"""A voice was heard in Ramah, Lamentation, weeping and great mourning, Rachel weeping for her children; She wouldn't be comforted, Because they are no more.""",40002018,"ChatCompletion(id='chatcmpl-8dJj6rfLuiojuwgLyd88jSFXb1bxk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381312, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=334, total_tokens=335))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.4, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:18
43,43,"But when Herod was dead, behold, an angel of the Lord appeared in a dream to Joseph in Egypt, saying,",40002019,"ChatCompletion(id='chatcmpl-8dJj7H3RrnX6l7GcX3Fjrbiaf0KcO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381313, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.4, 0.1, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.1, 'Health': 0.4, 'Insult': 0.1, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:19
44,44,"""Arise and take the young child and his mother, and go into the land of Israel, for those who sought the young child's life are dead.""",40002020,"ChatCompletion(id='chatcmpl-8dJj7lOJgoeFqxu8ZXsI6iWvCRR4f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381313, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 2
  total_token_count: 338
}
",10,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.5, 0.3, 0.3, 0.1, 0.1, 1.0, 0.2, 0.2, 0.2, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.5, 'Health': 0.3, 'Insult': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.2, 'War & Conflict': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 2:20
45,45,"He arose and took the young child and his mother, and came into the land of Israel.",40002021,"ChatCompletion(id='chatcmpl-8dJj8wc5fKQULC0qJvEjBqcCbryRK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381314, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.5, 0.3, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.5, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:21
46,46,"But when he heard that Archelaus was reigning over Judea in the place of his father, Herod, he was afraid to go there. Being warned in a dream, he withdrew into the region of Galilee,",40002022,"ChatCompletion(id='chatcmpl-8dJj8CDzHowBu63IjJmibymEGekTS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381314, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=341, total_tokens=342))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 2
  total_token_count: 346
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.2, 1.0, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 2:22
47,47,"and came and lived in a city called Nazareth; that it might be fulfilled which was spoken through the prophets: ""He will be called a Nazarene.""",40002023,"ChatCompletion(id='chatcmpl-8dJj9tB1m54dZG3Jo3270DM5TLSEC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381315, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 2:23
48,48,"In those days, John the Baptizer came, preaching in the wilderness of Judea, saying,",40003001,"ChatCompletion(id='chatcmpl-8dJj9WKClllZdO0zfcKustKYl52oW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381315, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.1, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 3:1
49,49,"""Repent, for the Kingdom of Heaven is at hand!""",40003002,"ChatCompletion(id='chatcmpl-8dJjA9hDidgJrqX2XJJtKU7pQPLtW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381316, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.3, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 3:2
50,50,"For this is he who was spoken of by Isaiah the prophet, saying, ""The voice of one crying in the wilderness, Make ready the way of the Lord, Make his paths straight.""",40003003,"ChatCompletion(id='chatcmpl-8dJjBcDIFSfTeYowIJE46Zrxf8SUP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381317, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=334, total_tokens=335))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 341
  candidates_token_count: 1
  total_token_count: 342
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 3:3
51,51,"Now John himself wore clothing made of camel's hair, with a leather belt around his waist. His food was locusts and wild honey.",40003004,"ChatCompletion(id='chatcmpl-8dJjBPKfXjluDwtNiCw2NenrcLaaF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381317, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 2
  total_token_count: 334
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.2, 0.1, 0.7, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 3:4
52,52,"Then people from Jerusalem, all of Judea, and all the region around the Jordan went out to him.",40003005,"ChatCompletion(id='chatcmpl-8dJjCyua9O2elgEJgPRu1J0D1G6R7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381318, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.3, 0.2, 0.3, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 3:5
53,53,"They were baptized by him in the Jordan, confessing their sins.",40003006,"ChatCompletion(id='chatcmpl-8dJjCSp1pmc5NiZ9VhfSBUhAYQuGa', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381318, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.3, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 3:6
54,54,"But when he saw many of the Pharisees and Sadducees coming for his baptism, he said to them, ""You offspring of vipers, who warned you to flee from the wrath to come?",40003007,"ChatCompletion(id='chatcmpl-8dJjDgLvG9WU0KV0AF1ezv2Ov5CX6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381319, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=338, total_tokens=339))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 343
  candidates_token_count: 2
  total_token_count: 345
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.2, 0.4, 0.3, 0.1, 0.1, 1.0, 0.2, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Health': 0.2, 'Insult': 0.4, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 3:7
55,55,Therefore bring forth fruit worthy of repentance!,40003008,"ChatCompletion(id='chatcmpl-8dJjDHcHir5qeIk8sNOjJQHTXXZUq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381319, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=305, total_tokens=306))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 311
  candidates_token_count: 1
  total_token_count: 312
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.3, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 3:8
56,56,"Don't think to yourselves, 'We have Abraham for our father,' for I tell you that God is able to raise up children to Abraham from these stones.",40003009,"ChatCompletion(id='chatcmpl-8dJjEDP0BltDoLlzJffsnsSgRARet', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381320, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.4, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 3:9
57,57,"""Even now the axe lies at the root of the trees. Therefore, every tree that doesn't bring forth good fruit is cut down, and cast into the fire.",40003010,"ChatCompletion(id='chatcmpl-8dJjE2DsfPy2KwROOFmvzRB1vZo92', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381320, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 2
  total_token_count: 340
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.5, 0.1, 0.3, 0.1, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 3:10
58,58,"I indeed baptize you in water for repentance, but he who comes after me is mightier than I, whose shoes I am not worthy to carry. He will baptize you in the Holy Spirit.{TR and NU add ""and with fire""}",40003011,"ChatCompletion(id='chatcmpl-8dJjFkIJzPYgSmLlewdS8p2IosrYT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381321, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=347, total_tokens=348))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 353
  candidates_token_count: 1
  total_token_count: 354
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.1, 0.2, 0.1, 1.0, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 3:11
59,59,"His winnowing fork is in his hand, and he will thoroughly cleanse his threshing floor. He will gather his wheat into the barn, but the chaff he will burn up with unquenchable fire.""",40003012,"ChatCompletion(id='chatcmpl-8dJjG3CYbhmGkTf7pVzY4zaNBZfil', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381322, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=340, total_tokens=341))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 345
  candidates_token_count: 1
  total_token_count: 346
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.1, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 3:12
60,60,"Then Jesus came from Galilee to the Jordan to John, to be baptized by him.",40003013,"ChatCompletion(id='chatcmpl-8dJjGqRjpxE0PauxQTGu2VkybI1rw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='20', role='assistant', function_call=None, tool_calls=None))], created=1704381322, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",20,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 3:13
61,61,"But John would have hindered him, saying, ""I need to be baptized by you, and you come to me?""",40003014,"ChatCompletion(id='chatcmpl-8dJjHrtWPjivW3lfmwWy0ZBIvLloJ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381323, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 3:14
62,62,"But Jesus, answering, said to him, ""Allow it now, for this is the fitting way for us to fulfill all righteousness."" Then he allowed him.",40003015,"ChatCompletion(id='chatcmpl-8dJjHEYh00RVPPcI6i5IK1a2sjOos', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381323, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 3:15
63,63,"Jesus, when he was baptized, went up directly from the water: and behold, the heavens were opened to him. He saw the Spirit of God descending as a dove, and coming on him.",40003016,"ChatCompletion(id='chatcmpl-8dJjIPgsAzun3t4hZpysS27ADrSzt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381324, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=336, total_tokens=337))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 343
  candidates_token_count: 1
  total_token_count: 344
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 3:16
64,64,"Behold, a voice out of the heavens said, ""This is my beloved Son, with whom I am well pleased.""",40003017,"ChatCompletion(id='chatcmpl-8dJjIYyK1bCZwYlUvFoHsbcn95RrO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381324, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.4, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 3:17
65,65,Then Jesus was led up by the Spirit into the wilderness to be tempted by the devil.,40004001,"ChatCompletion(id='chatcmpl-8dJjJBntQr5sn14xy7wZt5nbliCKU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='20', role='assistant', function_call=None, tool_calls=None))], created=1704381325, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",20,"candidates {
  content {
    role: ""model""
    parts {
      text: ""70""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 2
  total_token_count: 323
}
",70,"MultiCandidateTextGenerationResponse(text=' 80', _prediction_response=Prediction(predictions=[{'content': ' 80', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.1, 1.0, 0.2, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.1, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 80])",80,Matthew 4:1
66,66,"When he had fasted forty days and forty nights, he was hungry afterward.",40004002,"ChatCompletion(id='chatcmpl-8dJjKHeaWnS3nPt9VO3ET0EVJzv0V', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381326, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 2
  total_token_count: 321
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 1.0, 0.1, 0.1, 0.1, 0.7, 0.3, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 1.0, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.3, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 4:2
67,67,"The tempter came and said to him, ""If you are the Son of God, command that these stones become bread.""",40004003,"ChatCompletion(id='chatcmpl-8dJjKtrkbqc1nibJbOoOjR7f2Nv3P', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381326, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 2
  total_token_count: 330
}
",10,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 4:3
68,68,"But he answered, ""It is written, 'Man shall not live by bread alone, but by every word that proceeds out of the mouth of God.'""",40004004,"ChatCompletion(id='chatcmpl-8dJjL6MPwC4cfp3fIOhb3kw22NJiB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381327, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 2
  total_token_count: 336
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.1, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:4
69,69,"Then the devil took him into the holy city. He set him on the pinnacle of the temple,",40004005,"ChatCompletion(id='chatcmpl-8dJjLqCCJ0ZEtknkuy1oF0CBEA3VO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381327, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 2
  total_token_count: 325
}
",50,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.4, 0.2, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.1, 'Insult': 0.4, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 4:5
70,70,"and said to him, ""If you are the Son of God, throw yourself down, for it is written, 'He will give his angels charge concerning you.' and, 'On their hands they will bear you up, So that you don't dash your foot against a stone.'""",40004006,"ChatCompletion(id='chatcmpl-8dJjMJhUjMIEXBZaXBWvd6VyHVCdo', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381328, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=353, total_tokens=354))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 361
  candidates_token_count: 2
  total_token_count: 363
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.3, 0.1, 1.0, 0.1, 0.1, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'Violent': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:6
71,71,"Jesus said to him, ""Again, it is written, 'You shall not test the Lord, your God.'""",40004007,"ChatCompletion(id='chatcmpl-8dJjM8Mrq68YwKa8UyuYuOXXrct5f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381328, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:7
72,72,"Again, the devil took him to an exceedingly high mountain, and showed him all the kingdoms of the world, and their glory.",40004008,"ChatCompletion(id='chatcmpl-8dJjN3uTDwVIVQ6JCklI9N4N8Pkne', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381329, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.1, 0.3, 0.2, 0.2, 0.9, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.1, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:8
73,73,"He said to him, ""I will give you all of these things, if you will fall down and worship me.""",40004009,"ChatCompletion(id='chatcmpl-8dJjOolD0vXkZWNKysAFb1Ro0Tvpc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381330, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.1, 0.3, 0.2, 1.0, 0.2, 0.3, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.2, 'Health': 0.1, 'Insult': 0.3, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:9
74,74,"Then Jesus said to him, ""Get behind me,{TR and NU read ""Go away"" instead of ""Get behind me""} Satan! For it is written, 'You shall worship the Lord your God, and him only shall you serve.'""",40004010,"ChatCompletion(id='chatcmpl-8dJjOAxHbS3clp7UKPhbuRrgKRLVL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381330, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=345, total_tokens=346))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 352
  candidates_token_count: 1
  total_token_count: 353
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 0.1, 0.5, 0.2, 0.3, 0.1, 1.0, 0.2, 0.4, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Politics': 0.2, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.4, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:10
75,75,"Then the devil left him, and behold, angels came and ministered to him.",40004011,"ChatCompletion(id='chatcmpl-8dJjP4ZWXhc4xiOEtRFDckLpBIsPa', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381331, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.3, 0.2, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Insult': 0.3, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:11
76,76,"Now when Jesus heard that John was delivered up, he withdrew into Galilee.",40004012,"ChatCompletion(id='chatcmpl-8dJjQy7G3norIgtzI0SsE8OpdKM7n', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381332, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:12
77,77,"Leaving Nazareth, he came and lived in Capernaum, which is by the sea, in the region of Zebulun and Naphtali,",40004013,"ChatCompletion(id='chatcmpl-8dJjQm2AG4Rp1dTZ9FAiMOC7MSRbe', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381332, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:13
78,78,"that it might be fulfilled which was spoken through Isaiah the prophet, saying,",40004014,"ChatCompletion(id='chatcmpl-8dJjR84Ww7zBveJk85xg0mfASPfPt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381333, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:14
79,79,"""The land of Zebulun and the land of Naphtali, Toward the sea, beyond the Jordan, Galilee of the Gentiles,",40004015,"ChatCompletion(id='chatcmpl-8dJjSTgjqUbQn1gHydnalQ74BXUIL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381334, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:15
80,80,"The people who sat in darkness saw a great light, To those who sat in the region and shadow of death, To them light has dawned.""",40004016,"ChatCompletion(id='chatcmpl-8dJjTM3uo34DScIFBZsWvHxy0r2K2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381335, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.5, 0.1, 0.1, 0.1, 0.1, 0.9, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:16
81,81,"From that time, Jesus began to preach, and to say, ""Repent! For the Kingdom of Heaven is at hand.""",40004017,"ChatCompletion(id='chatcmpl-8dJjTpbize9AgtxzEyAlIwEcJ4zxV', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381335, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:17
82,82,"Walking by the sea of Galilee, he{TR reads ""Jesus"" instead of ""he""} saw two brothers: Simon, who is called Peter, and Andrew, his brother, casting a net into the sea; for they were fishermen.",40004018,"ChatCompletion(id='chatcmpl-8dJjU7tFWyhZQxyW4Qa7xVUZ53C4w', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381336, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=345, total_tokens=346))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 351
  candidates_token_count: 1
  total_token_count: 352
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:18
83,83,"He said to them, ""Come after me, and I will make you fishers for men.""",40004019,"ChatCompletion(id='chatcmpl-8dJjV6ZO2eiEfHKN5SFUDW2bZxTNt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381337, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.2, 0.4, 0.2, 0.3, 0.8, 0.5, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Health': 0.2, 'Insult': 0.4, 'Politics': 0.2, 'Profanity': 0.3, 'Religion & Belief': 0.8, 'Sexual': 0.5, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:19
84,84,They immediately left their nets and followed him.,40004020,"ChatCompletion(id='chatcmpl-8dJjVeg5xyUsFvub5OpkbVbcYy3QJ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381337, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=305, total_tokens=306))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 312
  candidates_token_count: 1
  total_token_count: 313
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.3, 0.2, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Finance': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Legal': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:20
85,85,"Going on from there, he saw two other brothers, James the son of Zebedee, and John his brother, in the boat with Zebedee their father, mending their nets. He called them.",40004021,"ChatCompletion(id='chatcmpl-8dJjWagvfN5geOTSVft2K69kfd9G1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381338, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=339, total_tokens=340))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 345
  candidates_token_count: 1
  total_token_count: 346
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:21
86,86,"They immediately left the boat and their father, and followed him.",40004022,"ChatCompletion(id='chatcmpl-8dJjW9M6eJ6KMxULi6pPW0B8GZraE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381338, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.4, 0.2, 0.2, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:22
87,87,"Jesus went about in all Galilee, teaching in their synagogues, preaching the Gospel of the Kingdom, and healing every disease and every sickness among the people.",40004023,"ChatCompletion(id='chatcmpl-8dJjXe2eyHprKxHFsbqNuSfknPYxr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381339, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 2
  total_token_count: 336
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:23
88,88,"The report about him went out into all Syria. They brought to him all who were sick, afflicted with various diseases and torments, possessed with demons, epileptics, and paralytics; and he healed them.",40004024,"ChatCompletion(id='chatcmpl-8dJjXGR72lBoV3pTa4o3FL3LhHUBq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381339, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=340, total_tokens=341))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 347
  candidates_token_count: 1
  total_token_count: 348
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.9, 0.1, 0.3, 0.1, 0.2, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.4, 'Health': 0.9, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.1, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:24
89,89,"Great multitudes from Galilee, Decapolis, Jerusalem, Judea and from beyond the Jordan followed him.",40004025,"ChatCompletion(id='chatcmpl-8dJjYGWleqUKL1zJHQFeFidplfvWw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381340, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 4:25
90,90,"Seeing the multitudes, he went up onto the mountain. When he had sat down, his disciples came to him.",40005001,"ChatCompletion(id='chatcmpl-8dJjYP0d9ZvSDqsy46qOJHGTcvwtd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381340, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:1
91,91,"He opened his mouth and taught them, saying,",40005002,"ChatCompletion(id='chatcmpl-8dJjZiyJgJapCcxNxZbgjV54MFLT1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381341, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=306, total_tokens=307))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 313
  candidates_token_count: 1
  total_token_count: 314
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.1, 0.2, 0.2, 0.9, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:2
92,92,"""Blessed are the poor in spirit, For theirs is the Kingdom of Heaven.",40005003,"ChatCompletion(id='chatcmpl-8dJjZmaeglKsArNOzO6LOzFQ5hOQk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381341, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.3, 0.3, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.2, 'Health': 0.3, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:3
93,93,"Blessed are those who mourn, For they shall be comforted.",40005004,"ChatCompletion(id='chatcmpl-8dJjaCxoPGt6cXDqMJEzFIR7W4aLQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='80', role='assistant', function_call=None, tool_calls=None))], created=1704381342, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",80,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 2
  total_token_count: 317
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.2, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:4
94,94,"Blessed are the gentle, For they shall inherit the earth.",40005005,"ChatCompletion(id='chatcmpl-8dJjbu5qRWHRJTb506MaRketuJuPV', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381343, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.4, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:5
95,95,"Blessed are those who hunger and thirst after righteousness, For they shall be filled.",40005006,"ChatCompletion(id='chatcmpl-8dJjb72qW71xc6BF7WDlWzVUuO9a4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381343, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 2
  total_token_count: 321
}
",50,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:6
96,96,"Blessed are the merciful, For they shall obtain mercy.",40005007,"ChatCompletion(id='chatcmpl-8dJjcriV1WmGrVCpcMzzjiObhRdQc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381344, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 314
  candidates_token_count: 1
  total_token_count: 315
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:7
97,97,"Blessed are the pure in heart, For they shall see God.",40005008,"ChatCompletion(id='chatcmpl-8dJjc6f9qs5HSJJnZ9q41aokwpo1c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381344, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:8
98,98,"Blessed are the peacemakers, For they shall be called children of God.",40005009,"ChatCompletion(id='chatcmpl-8dJjd3N9pUZELuyvwDAnxW8oXaOkR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381345, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:9
99,99,"Blessed are those who have been persecuted for righteousness' sake, For theirs is the Kingdom of Heaven.",40005010,"ChatCompletion(id='chatcmpl-8dJjeusxpO0b408w7aVNebL80ZSLp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381346, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 2
  total_token_count: 325
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.2, 0.3, 0.1, 0.1, 1.0, 0.2, 0.1, 0.4], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.1, 'Insult': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 5:10
100,100,"""Blessed are you when people reproach you, persecute you, and say all kinds of evil against you falsely, for my sake.",40005011,"ChatCompletion(id='chatcmpl-8dJjfQaz7kqCPBu8bPI1SbuSJMAzM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='50', role='assistant', function_call=None, tool_calls=None))], created=1704381347, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",50,"candidates {
  content {
    role: ""model""
    parts {
      text: ""80""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 2
  total_token_count: 332
}
",80,"MultiCandidateTextGenerationResponse(text=' 80', _prediction_response=Prediction(predictions=[{'content': ' 80', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.2, 0.1, 0.1, 1.0, 0.1, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.1, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 80])",80,Matthew 5:11
101,101,"Rejoice, and be exceedingly glad, for great is your reward in heaven. For that is how they persecuted the prophets who were before you.",40005012,"ChatCompletion(id='chatcmpl-8dJjfdJrIgAIwCBL7l8cHDHEEoL4I', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381347, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.3, 0.3, 0.2, 0.3, 0.2, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:12
102,102,"""You are the salt of the earth, but if the salt has lost its flavor, with what will it be salted? It is then good for nothing, but to be cast out and trodden under the feet of men.",40005013,"ChatCompletion(id='chatcmpl-8dJjg9piA3VEEWeVddci5kjUgzlA7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381348, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=342, total_tokens=343))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 349
  candidates_token_count: 2
  total_token_count: 351
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.5, 0.5, 0.2, 0.2, 0.3, 0.1, 0.9, 0.2, 0.3, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.5, 'Health': 0.5, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:13
103,103,You are the light of the world. A city located on a hill can't be hidden.,40005014,"ChatCompletion(id='chatcmpl-8dJjgKDLNkchgP62K5lr0a8tgce8H', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381348, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.2, 0.7, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:14
104,104,"Neither do you light a lamp, and put it under a measuring basket, but on a stand; and it shines to all who are in the house.",40005015,"ChatCompletion(id='chatcmpl-8dJji1gefj8wBIb8DHPrBbp3vtgby', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381350, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.2, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:15
105,105,"Even so, let your light shine before men; that they may see your good works, and glorify your Father who is in heaven.",40005016,"ChatCompletion(id='chatcmpl-8dJji9SSCzcJ1Iv5oQrOB4pfiw5N8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381350, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:16
106,106,"""Don't think that I came to destroy the law or the prophets. I didn't come to destroy, but to fulfill.",40005017,"ChatCompletion(id='chatcmpl-8dJjjfly0rJ5cl7n6ZfRnDbWnmgQx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381351, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.1, 0.2, 0.2, 0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:17
107,107,"For most assuredly, I tell you, until heaven and earth pass away, not even one smallest letter{literally, iota} or one tiny pen stroke{or, serif} shall in any way pass away from the law, until all things are accomplished.",40005018,"ChatCompletion(id='chatcmpl-8dJjjn4BjowV2W4ZrDdFPNWJ2MkZm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381351, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=349, total_tokens=350))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 354
  candidates_token_count: 1
  total_token_count: 355
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.5, 0.1, 0.2, 0.2, 0.1, 0.2, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.1, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:18
108,108,"Whoever, therefore, shall break one of these least commandments, and teach others to do so, shall be called least in the Kingdom of Heaven; but whoever shall do and teach them shall be called great in the Kingdom of Heaven.",40005019,"ChatCompletion(id='chatcmpl-8dJjkl3JXI3ffUpXtv8qzlCYa2ILu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381352, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=344, total_tokens=345))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 349
  candidates_token_count: 1
  total_token_count: 350
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:19
109,109,"For I tell you that unless your righteousness exceeds that of the scribes and Pharisees, there is no way you will enter into the Kingdom of Heaven.",40005020,"ChatCompletion(id='chatcmpl-8dJjlBURBveYIN02fkxGDX4bvg12Y', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381353, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 2
  total_token_count: 334
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.4, 0.4, 0.1, 0.3, 0.2, 0.1, 1.0, 0.1, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:20
110,110,"""You have heard that it was said to the ancient ones, 'You shall not murder;' and 'Whoever shall murder shall be in danger of the judgment.'",40005021,"ChatCompletion(id='chatcmpl-8dJjlRSxeOKW4YmzkJgkoZJfMhBvQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381353, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.3, 0.2, 0.1, 0.4, 0.2, 0.2, 0.1, 0.3, 0.9, 0.2, 0.3, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.3, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.3, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:21
111,111,"But I tell you, that everyone who is angry with his brother without a cause shall be in danger of the judgment; and whoever shall say to his brother, 'Raca{""Raca"" is an Aramaic insult, related to the word for ""empty"" and conveying the idea of empty-headedness.}!' shall be in danger of the council; and whoever shall say, 'You fool!' shall be in danger of the fire of Gehenna{Gehenna is another name for Hell that brings to mind an image of a burning garbage dump with dead bodies in it.}.",40005022,"ChatCompletion(id='chatcmpl-8dJjmHakQU4TwBfONu83bYxBtJnjV', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381354, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=415, total_tokens=416))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 421
  candidates_token_count: 2
  total_token_count: 423
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.4, 0.3, 0.1, 0.6, 0.4, 0.3, 0.1, 0.9, 0.1, 0.5, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.4, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.6, 'Politics': 0.4, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.5, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:22
112,112,"""If therefore you are offering your gift at the altar, and there remember that your brother has anything against you,",40005023,"ChatCompletion(id='chatcmpl-8dJjmg8Drtf8cIVKDtHD3366VYwWA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381354, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.5, 0.1, 0.4, 0.5, 0.4, 0.1, 1.0, 0.2, 0.2], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.5, 'Politics': 0.4, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:23
113,113,"leave your gift there before the altar, and go your way. First be reconciled to your brother, and then come and offer your gift.",40005024,"ChatCompletion(id='chatcmpl-8dJjnOZMIjO2F7gjBcuvISeJXTOv0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381355, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.3, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:24
114,114,"Agree with your adversary quickly, while you are with him in the way; lest perhaps the prosecutor deliver you to the judge, and the judge deliver you to the officer, and you be cast into prison.",40005025,"ChatCompletion(id='chatcmpl-8dJjnPA5eb946BaTB9fgSUkZ1aysp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381355, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=338, total_tokens=339))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 2
  total_token_count: 346
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.7, 'severity': 'HIGH', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.6, 0.5, 0.5, 0.2, 0.2, 0.9, 0.7, 0.2, 0.3, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Illicit Drugs': 0.6, 'Insult': 0.5, 'Legal': 0.5, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.9, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 5:25
115,115,"Most assuredly I tell you, you shall by no means get out of there, until you have paid the last penny.{Literally, kodrantes. A kodrantes was a small copper coin worth about 2 lepta (widow's mites)--not enough to buy very much of anything.}",40005026,"ChatCompletion(id='chatcmpl-8dJjoyvVUzUAb34ZXPMiEtyy7dqJ2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381356, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=360, total_tokens=361))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 362
  candidates_token_count: 1
  total_token_count: 363
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 1.0, 0.3, 0.2, 0.2, 0.2, 0.2, 0.1, 0.8, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Finance': 1.0, 'Health': 0.3, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:26
116,116,"""You have heard that it was said, {TR adds ""to the ancients,""} 'You shall not commit adultery;'",40005027,"ChatCompletion(id='chatcmpl-8dJjoFOs9rFaerYTKMyZZGj1Wzh02', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381356, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.5, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.6, 0.1, 0.2, 0.2, 0.2, 0.1, 0.9, 0.5, 0.3], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.5, 'Toxic': 0.3}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:27
117,117,but I tell you that everyone who gazes at a woman to lust after her has committed adultery with her already in his heart.,40005028,"ChatCompletion(id='chatcmpl-8dJjpS6IZXTYkR8kkrJXEE7LDG9O0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381357, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  total_token_count: 329
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.8, 'severityScore': 0.7, 'severity': 'MEDIUM', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.7, 0.1, 0.4, 0.1, 0.3, 0.6, 0.8, 0.5, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.7, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Politics': 0.1, 'Profanity': 0.3, 'Religion & Belief': 0.6, 'Sexual': 0.8, 'Toxic': 0.5, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:28
118,118,"If your right eye causes you to stumble, pluck it out and throw it away from you. For it is more profitable for you that one of your members should perish, than for your whole body to be cast into Gehenna.",40005029,"ChatCompletion(id='chatcmpl-8dJjpE2g4xfoU7PqDFZ5NNXitQUzZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381357, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=343, total_tokens=344))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 349
  candidates_token_count: 2
  total_token_count: 351
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.4, 0.3, 0.2, 1.0, 0.2, 0.3, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Health': 0.4, 'Insult': 0.3, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 5:29
119,119,"If your right hand causes you to stumble, cut it off, and throw it away from you: for it is profitable for you that one of your members should perish, and not your whole body be thrown into Gehenna.",40005030,"ChatCompletion(id='chatcmpl-8dJjq5Vn45hJFdrxG44CL6A83xWqP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381358, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=341, total_tokens=342))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 348
  candidates_token_count: 2
  total_token_count: 350
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.3, 0.3, 0.2, 1.0, 0.2, 0.3, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.3, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 5:30
120,120,"""It was also said, 'Whoever shall put away his wife, let him give her a writing of divorce,'",40005031,"ChatCompletion(id='chatcmpl-8dJjqi77LcZqzNF4D0Rj55xZarFuk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381358, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.8, 0.1, 0.6, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Legal': 0.8, 'Profanity': 0.1, 'Religion & Belief': 0.6, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:31
121,121,"but I tell you that whoever puts away his wife, except for the cause of sexual immorality, makes her an adulteress; and whoever marries her when she is put away commits adultery.",40005032,"ChatCompletion(id='chatcmpl-8dJjrzymSPTibOaPbl5NCNbiznYks', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381359, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=337, total_tokens=338))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 340
  candidates_token_count: 1
  total_token_count: 341
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.6, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.6, 0.1, 0.4, 0.5, 0.2, 0.2, 0.1, 0.9, 0.6, 0.4, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.5, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.6, 'Toxic': 0.4, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:32
122,122,"""Again you have heard that it was said to them of old time, 'You shall not make false vows, but shall perform to the Lord your vows,'",40005033,"ChatCompletion(id='chatcmpl-8dJjrlCFd6SY5b4j8PGDKYryzg5rV', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381359, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.1, 'Legal': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:33
123,123,"but I tell you, don't swear at all: neither by heaven, for it is the throne of God;",40005034,"ChatCompletion(id='chatcmpl-8dJjsFXRI1xUeW4AvFAwaEqUtywff', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381360, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:34
124,124,"nor by the earth, for it is the footstool of his feet; nor by Jerusalem, for it is the city of the great King.",40005035,"ChatCompletion(id='chatcmpl-8dJjsKPhfNPF0C8b97kcbb7vcM6nL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381360, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:35
125,125,"Neither shall you swear by your head, for you can't make one hair white or black.",40005036,"ChatCompletion(id='chatcmpl-8dJjsSMyiNroLVQn5kBFgeOycLUL6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381360, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.6, 0.3, 0.2, 0.2, 0.8, 0.2, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Health': 0.6, 'Insult': 0.3, 'Legal': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:36
126,126,But let your 'Yes' be 'Yes' and your 'No' be 'no.' Whatever is more than these is of the evil one.,40005037,"ChatCompletion(id='chatcmpl-8dJjtlNW6CoYJxThhsqyvdzD16veq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381361, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.4, 0.3, 0.2, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.3, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:37
127,127,"""You have heard that it was said, 'An eye for an eye, and a tooth for a tooth.'",40005038,"ChatCompletion(id='chatcmpl-8dJjuoJXPVRPhNDnn7gKLxwPksvhI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381362, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.9, 0.2, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.9, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:38
128,128,"But I tell you, don't resist him who is evil; but whoever strikes you on your right cheek, turn to him the other also.",40005039,"ChatCompletion(id='chatcmpl-8dJjuHkR0atPKfwRvYXdbrYyrqHly', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381362, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 2
  total_token_count: 335
}
",50,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.3, 0.2, 0.1, 0.1, 1.0, 0.1, 0.2, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.2, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 5:39
129,129,"If anyone sues you to take away your coat, let him have your cloak also.",40005040,"ChatCompletion(id='chatcmpl-8dJjv01puPNrCQUxRjuc7jBhlNlmH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381363, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 2
  total_token_count: 323
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.1, 0.3, 0.2, 0.2, 0.1, 0.1, 0.7, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 5:40
130,130,"Whoever compels you to go one mile, go with him two.",40005041,"ChatCompletion(id='chatcmpl-8dJjw9nNE0ioFvXWAewb2DWbWRCWd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='60', role='assistant', function_call=None, tool_calls=None))], created=1704381364, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",60,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 2
  total_token_count: 318
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.3, 0.1, 0.7, 0.2, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.1, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 5:41
131,131,"Give to him who asks you, and don't turn away him who desires to borrow from you.",40005042,"ChatCompletion(id='chatcmpl-8dJjw9JmQnoSfbxsHWYYQatZuodgP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381364, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.7, 0.3, 0.2, 0.2, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.7, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:42
132,132,"""You have heard that it was said, 'You shall love your neighbor, and hate your enemy.'",40005043,"ChatCompletion(id='chatcmpl-8dJjxkQmsayYMudxDZ1C7Vprtr0sY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381365, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  total_token_count: 324
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.5, 0.1, 0.6, 0.3, 0.2, 0.1, 0.8, 0.2, 0.4, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.5, 'Health': 0.1, 'Insult': 0.6, 'Politics': 0.3, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.4, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:43
133,133,"But I tell you, love your enemies, bless those who curse you, do good to those who hate you, and pray for those who mistreat you and persecute you,",40005044,"ChatCompletion(id='chatcmpl-8dJjxCbC3RuHFL1OhCmD8DgNNV1sf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381365, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""70""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 2
  total_token_count: 341
}
",70,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.5, 0.1, 0.2, 0.1, 0.9, 0.2, 0.4, 0.2, 0.1], 'categories': ['Derogatory', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Insult': 0.5, 'Politics': 0.1, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.4, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 5:44
134,134,"that you may be children of your Father who is in heaven. For he makes his sun to rise on the evil and the good, and sends rain on the just and the unjust.",40005045,"ChatCompletion(id='chatcmpl-8dJjywax6LwsUw1JgqDD0EemhJDNl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381366, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 340
  candidates_token_count: 1
  total_token_count: 341
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:45
135,135,"For if you love those who love you, what reward do you have? Don't even the tax collectors do the same?",40005046,"ChatCompletion(id='chatcmpl-8dJjzF9u7VQKQNTtNzlzB9uTevjat', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381367, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.9, 0.1, 0.1, 0.3, 0.2, 0.2, 0.1, 0.6, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.9, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.6, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:46
136,136,"If you only greet your friends, what more do you do than others? Don't even the tax collectors do the same?",40005047,"ChatCompletion(id='chatcmpl-8dJjznVhIiTGm8OlQ0gagh1c1ime2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381367, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.8, 0.2, 0.1, 0.3, 0.5, 0.6, 0.1, 0.1, 0.6, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.8, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.6, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:47
137,137,"Therefore you shall be perfect, just as your Father in heaven is perfect.",40005048,"ChatCompletion(id='chatcmpl-8dJjz7UkEbe6CxbZewfpjvIYVON9y', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381367, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.2, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 5:48
138,138,"""Be careful that you don't do your charitable giving before men, to be seen by them, or else you have no reward from your Father who is in heaven.",40006001,"ChatCompletion(id='chatcmpl-8dJk09Xv68nT50J4qtz6w0QZnYZCS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381368, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.7, 0.5, 0.1, 0.3, 0.1, 0.3, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.7, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:1
139,139,"Therefore when you do merciful deeds, don't sound a trumpet before yourself, as the hypocrites do in the synagogues and in the streets, that they may get glory from men. Most assuredly I tell you, they have received their reward.",40006002,"ChatCompletion(id='chatcmpl-8dJk04Fih3WZNjum1cgZpajq3EFiH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381368, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=348, total_tokens=349))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 353
  candidates_token_count: 1
  total_token_count: 354
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 0.5, 0.1, 0.4, 0.1, 0.6, 0.1, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:2
140,140,"But when you do merciful deeds, don't let your left hand know what your right hand does,",40006003,"ChatCompletion(id='chatcmpl-8dJk1VL2znMimxV3cPdwKZhcyeRQc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381369, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:3
141,141,"so that your merciful deeds may be in secret, then your Father who sees in secret will reward you openly.",40006004,"ChatCompletion(id='chatcmpl-8dJk2to8A7Gy0TyVAKC03AhFBgf7E', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381370, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:4
142,142,"""When you pray, you shall not be as the hypocrites, for they love to stand and pray in the synagogues and in the corners of the streets, that they may be seen by men. Most assuredly, I tell you, they have received their reward.",40006005,"ChatCompletion(id='chatcmpl-8dJk2BBI77oxlaEYgHOZjCgnFOszF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381370, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=351, total_tokens=352))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 357
  candidates_token_count: 1
  total_token_count: 358
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.1, 0.5, 0.1, 0.3, 0.3, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:5
143,143,"But you, when you pray, enter into your inner chamber, and having shut your door, pray to your Father who is in secret, and your Father who sees in secret will reward you openly.",40006006,"ChatCompletion(id='chatcmpl-8dJk3cmT66V3KCum8NiyGoIGd5y5C', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381371, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=336, total_tokens=337))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 343
  candidates_token_count: 2
  total_token_count: 345
}
",10,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:6
144,144,"In praying, don't use vain repetitions, as the Gentiles do; for they think that they will be heard for their much speaking.",40006007,"ChatCompletion(id='chatcmpl-8dJk3TqS2eJ13NuhBiUW3NB1Jw3Fd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381371, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:7
145,145,"Therefore don't be like them, for your Father knows what things you need, before you ask him.",40006008,"ChatCompletion(id='chatcmpl-8dJk4qfmkPd0wBwoaSwwCW5m6ZKCW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381372, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.5, 0.4, 0.1, 1.0, 0.1, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.5, 'Insult': 0.4, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:8
146,146,"Pray like this: 'Our Father in heaven, may your name be kept holy.",40006009,"ChatCompletion(id='chatcmpl-8dJk4IZPmnp2z06cwigpo4G8iEC19', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381372, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:9
147,147,"Let your Kingdom come. Let your will be done, as in heaven, so on earth.",40006010,"ChatCompletion(id='chatcmpl-8dJk5ZWeBXpmnfG7i1Pdpg9bYTw4q', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381373, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:10
148,148,Give us today our daily bread.,40006011,"ChatCompletion(id='chatcmpl-8dJk5iVolQprzFmjF1r2ucGu6nnHK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381373, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=303, total_tokens=304))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 310
  candidates_token_count: 1
  total_token_count: 311
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.9, 0.2, 0.1, 0.3, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.9, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.3, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:11
149,149,"Forgive us our debts, as we also forgive our debtors.",40006012,"ChatCompletion(id='chatcmpl-8dJk6mpwSkHp840V8nV8ULsZeCbhV', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381374, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 1.0, 0.2, 0.2, 0.2, 0.1, 0.2, 0.6, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Finance': 1.0, 'Health': 0.2, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.1, 'Profanity': 0.2, 'Religion & Belief': 0.6, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:12
150,150,"Bring us not into temptation, but deliver us from the evil one. For yours is the Kingdom, the power, and the glory forever. Amen.'",40006013,"ChatCompletion(id='chatcmpl-8dJk6vZVBvkFF8H6NzEYlERhpcmiS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381374, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:13
151,151,"""For if you forgive men their trespasses, your heavenly Father will also forgive you.",40006014,"ChatCompletion(id='chatcmpl-8dJk7kqO2u0eoWnjcyhPFgQBs6ust', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381375, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.3, 0.1, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:14
152,152,"But if you don't forgive men their trespasses, neither will your Father forgive your trespasses.",40006015,"ChatCompletion(id='chatcmpl-8dJk7kUgGvJc2Hynz5M6Qz2GLwfA4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381375, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.4, 0.1, 0.3, 0.2, 0.2, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:15
153,153,"""Moreover when you fast, don't be like the hypocrites, with sad faces. For they disfigure their faces, that they may be seen by men to be fasting. Most assuredly I tell you, they have received their reward.",40006016,"ChatCompletion(id='chatcmpl-8dJk8RHT70yGyZ6UqLsD6oTjfz70p', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381376, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=345, total_tokens=346))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 352
  candidates_token_count: 1
  total_token_count: 353
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 0.9, 0.1, 0.5, 0.1, 0.3, 0.2, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.9, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:16
154,154,"But you, when you fast, anoint your head, and wash your face;",40006017,"ChatCompletion(id='chatcmpl-8dJk8mO0Fs5wNmFU8oIzsDZGdSKw8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381376, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.9, 0.1, 0.3, 0.2, 1.0, 0.2, 0.3, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.9, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:17
155,155,"so that you are not seen by men to be fasting, but by your Father who is in secret, and your Father, who sees in secret, will reward you.",40006018,"ChatCompletion(id='chatcmpl-8dJk9IXtYN0ojg4lHFDw3JDhM0IkF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381377, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 1
  total_token_count: 338
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.6, 0.1, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:18
156,156,"""Don't lay up treasures for yourselves on the earth, where moth and rust consume, and where thieves break through and steal;",40006019,"ChatCompletion(id='chatcmpl-8dJk9t9E4kRn6hG1TON3YiPfLc8IQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381377, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 2
  total_token_count: 332
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.4, 0.2, 0.1, 0.1, 0.1, 0.8, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Legal': 0.1, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:19
157,157,"but lay up for yourselves treasures in heaven, where neither moth nor rust consume, and where thieves don't break through and steal;",40006020,"ChatCompletion(id='chatcmpl-8dJkAzstVyV70jGVcSldfXUNsK0Au', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381378, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.4, 0.2, 0.1, 0.2, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:20
158,158,"for where your treasure is, there your heart will be also.",40006021,"ChatCompletion(id='chatcmpl-8dJkAGixj0uECy50Q18TgU60uXqUR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381378, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.1, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.1, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:21
159,159,"""The lamp of the body is the eye. If therefore your eye is sound, your whole body will be full of light.",40006022,"ChatCompletion(id='chatcmpl-8dJkBKsilUGS90iCWaJ0fgklZQ634', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381379, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.7, 0.2, 0.2, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.7, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:22
160,160,"But if your eye is evil, your whole body will be full of darkness. If therefore the light that is in you is darkness, how great is the darkness!",40006023,"ChatCompletion(id='chatcmpl-8dJkBdKSXpIzY4bN6Chw9uuA22K6H', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381379, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:23
161,161,"""No one can serve two masters, for either he will hate the one and love the other; or else he will be devoted to one and despise the other. You can't serve both God and Mammon.",40006024,"ChatCompletion(id='chatcmpl-8dJkClo7Op1JL1SazBQ8qaRilonaL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381380, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=339, total_tokens=340))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 347
  candidates_token_count: 1
  total_token_count: 348
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.3, 0.1, 0.4, 0.1, 0.3, 0.1, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.2, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:24
162,162,"Therefore, I tell you, don't be anxious for your life: what you will eat, or what you will drink; nor yet for your body, what you will wear. Isn't life more than food, and the body more than clothing?",40006025,"ChatCompletion(id='chatcmpl-8dJkDrL4MjjEGymoIwNDcH104SVa7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381381, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=346, total_tokens=347))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 355
  candidates_token_count: 2
  total_token_count: 357
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.6, 0.2, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:25
163,163,"See the birds of the sky, that they don't sow, neither do they reap, nor gather into barns. Your heavenly Father feeds them. Aren't you of much more value than they?",40006026,"ChatCompletion(id='chatcmpl-8dJkDY2spEhZL8S4sJteyQknIyqOg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381381, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=336, total_tokens=337))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 1
  total_token_count: 345
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.5, 0.4, 0.2, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.4, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:26
164,164,"""Which of you, by being anxious, can add one cubit to the measure of his life?",40006027,"ChatCompletion(id='chatcmpl-8dJkEpTdAh3uVd0GUbFIgHEPPaBQy', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381382, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.6, 0.2, 0.1, 0.1, 0.1, 0.6, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.6, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:27
165,165,"Why are you anxious about clothing? Consider the lilies of the field, how they grow. They don't toil, neither do they spin,",40006028,"ChatCompletion(id='chatcmpl-8dJkENLgxYHVOlrzoexTC8HDlOU8x', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381382, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.1, 0.7, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:28
166,166,yet I tell you that even Solomon in all his glory was not dressed like one of these.,40006029,"ChatCompletion(id='chatcmpl-8dJkFqG1MJGlMpcjrddU5ZKQFRjL9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381383, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.3, 0.6, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.3, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:29
167,167,"But if God so clothes the grass of the field, which today exists, and tomorrow is thrown into the oven, won't he much more clothe you, you of little faith?",40006030,"ChatCompletion(id='chatcmpl-8dJkFAmBGWoankslMBSRDpQ9lBEW7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381383, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 340
  candidates_token_count: 1
  total_token_count: 341
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.4, 0.4, 0.1, 0.2, 0.1, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:30
168,168,"""Therefore don't be anxious, saying, 'What will we eat?', 'What will we drink?' or, 'With what will we be clothed?'",40006031,"ChatCompletion(id='chatcmpl-8dJkGyiIjd9GXIOqLrtcV1Z6n8DnA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381384, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.9, 0.1, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.9, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:31
169,169,"For the Gentiles seek after all these things, for your heavenly Father knows that you need all these things.",40006032,"ChatCompletion(id='chatcmpl-8dJkGaxTUV8PU24i4bu1OVC4L8egv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381384, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.5, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:32
170,170,"But seek first God's Kingdom, and his righteousness; and all these things will be given to you as well.",40006033,"ChatCompletion(id='chatcmpl-8dJkHlpmZDIbeuLk7dnwKsqIiOY2O', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381385, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 2
  total_token_count: 329
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.4, 'Insult': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 6:33
171,171,"Therefore don't be anxious for tomorrow, for tomorrow will be anxious for itself. Each day's own evil is sufficient.",40006034,"ChatCompletion(id='chatcmpl-8dJkHwD8bA8dRIK4kuyw3TMY1iBWD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381385, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.4, 0.3, 0.2, 0.1, 0.9, 0.1, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.3, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 6:34
172,172,"""Don't judge, so that you won't be judged.",40007001,"ChatCompletion(id='chatcmpl-8dJkIHOz1mMb1lzvErkde6uQIWysx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381386, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=308, total_tokens=309))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.2, 0.2, 0.7, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:1
173,173,"For with whatever judgment you judge, you will be judged; and with whatever measure you measure, it will be measured to you.",40007002,"ChatCompletion(id='chatcmpl-8dJkIouco3EaqNOSeOkiYuhdz4CbM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381386, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.1, 0.2, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:2
174,174,"Why do you see the speck that is in your brother's eye, but don't consider the beam that is in your own eye?",40007003,"ChatCompletion(id='chatcmpl-8dJkJR8XXGKulPzY75ghO4idKkHma', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381387, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.9, 0.2, 0.1, 0.9, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.9, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:3
175,175,"Or how will you tell your brother, 'Let me remove the speck from your eye;' and behold, the beam is in your own eye?",40007004,"ChatCompletion(id='chatcmpl-8dJkJfV9AYy37h5QOCHgvI7dKjg4B', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381387, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.9, 0.2, 0.2, 0.8, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.9, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:4
176,176,"You hypocrite! First remove the beam out of your own eye, and then you can see clearly to remove the speck out of your brother's eye.",40007005,"ChatCompletion(id='chatcmpl-8dJkKg3Hba4OVzgbC76UgqaWwnizW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381388, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  total_token_count: 334
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.8, 'severityScore': 0.7, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.4, 0.1, 0.7, 0.1, 0.8, 0.2, 0.6, 0.3, 0.1, 0.8, 0.1, 0.5, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.7, 'Illicit Drugs': 0.1, 'Insult': 0.8, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.5, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:5
177,177,"""Don't give that which is holy to the dogs, neither throw your pearls before the pigs, lest perhaps they trample them under their feet, and turn and tear you to pieces.",40007006,"ChatCompletion(id='chatcmpl-8dJkKSqVznYCNFYOsos4ucmtUrI9X', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381388, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 342
  candidates_token_count: 1
  total_token_count: 343
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.1, 0.4, 0.1, 0.4, 0.2, 0.2, 0.3, 0.1, 1.0, 0.2, 0.3, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:6
178,178,"""Ask, and it will be given you. Seek, and you will find. Knock, and it will be opened for you.",40007007,"ChatCompletion(id='chatcmpl-8dJkLFGelnv4gPmKB5ysGoxuaNAV6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='60', role='assistant', function_call=None, tool_calls=None))], created=1704381389, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",60,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 2
  total_token_count: 332
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.2, 0.2, 0.9, 0.2, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 7:7
179,179,For everyone who asks receives. He who seeks finds. To him who knocks it will be opened.,40007008,"ChatCompletion(id='chatcmpl-8dJkLz3OH7ttDCaitRiFj0otxVEkP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='50', role='assistant', function_call=None, tool_calls=None))], created=1704381389, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",50,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 2
  total_token_count: 325
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 7:8
180,180,"Or who is there among you, who, if his son asks him for bread, will give him a stone?",40007009,"ChatCompletion(id='chatcmpl-8dJkMA7pEqZmaASTPJKMuciejzLrn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381390, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.1, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:9
181,181,"Or if he asks for a fish, who will give him a serpent?",40007010,"ChatCompletion(id='chatcmpl-8dJkMitxWNKOulMro9cZCbqWHjXbB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381390, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.6, 0.2, 0.2, 0.8, 0.3, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:10
182,182,"If you then, being evil, know how to give good gifts to your children, how much more will your Father who is in heaven give good things to those who ask him!",40007011,"ChatCompletion(id='chatcmpl-8dJkNl7rBhiuN1GyYZNtLpu3q2AfM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381391, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.5, 0.1, 0.3, 0.2, 0.2, 1.0, 0.2, 0.2], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:11
183,183,"Therefore whatever you desire for men to do to you, you shall also do to them; for this is the law and the prophets.",40007012,"ChatCompletion(id='chatcmpl-8dJkNhxmdq3IicTrmuTn411YRW7u4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381391, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.5, 0.1, 0.2, 0.1, 0.2, 0.2, 1.0, 0.3, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:12
184,184,"""Enter in by the narrow gate; for wide is the gate and broad is the way that leads to destruction, and many are those who enter in by it.",40007013,"ChatCompletion(id='chatcmpl-8dJkO28g6NDzwhfilnPTCXwXP5Ibh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381392, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 2
  total_token_count: 338
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.5, 0.2, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 7:13
185,185,"How{TR reads ""Because"" instead of ""How""} narrow is the gate, and restricted is the way that leads to life! Few are those who find it.",40007014,"ChatCompletion(id='chatcmpl-8dJkOesVL9kCKT0BLFCByEgT8KTih', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381392, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 1
  total_token_count: 338
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 0.9, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:14
186,186,"""Beware of false prophets, who come to you in sheep's clothing, but inwardly are ravening wolves.",40007015,"ChatCompletion(id='chatcmpl-8dJkPmC7Dyl6XJ9rsfSqSg8AooTSs', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381393, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.2, 0.4, 0.3, 0.2, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Health': 0.2, 'Insult': 0.4, 'Politics': 0.3, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:15
187,187,"By their fruits you will know them. Do you gather grapes from thorns, or figs from thistles?",40007016,"ChatCompletion(id='chatcmpl-8dJkPuOzfy5cRAygjQ2pVLGRbR87a', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381393, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.2, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:16
188,188,"Even so, every good tree produces good fruit; but the corrupt tree produces evil fruit.",40007017,"ChatCompletion(id='chatcmpl-8dJkQN3pjknQCjgLEeuAE8o43iR5U', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381394, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.5, 0.3, 0.1, 0.2, 0.2, 0.8, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:17
189,189,"A good tree can't produce evil fruit, neither can a corrupt tree produce good fruit.",40007018,"ChatCompletion(id='chatcmpl-8dJkQHMhy7L4zLS8gNalTYwD44pC3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381394, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.2, 0.2, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:18
190,190,"Every tree that doesn't grow good fruit is cut down, and thrown into the fire.",40007019,"ChatCompletion(id='chatcmpl-8dJkRjZppyV3g8dJpumRvxMJoVSO2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381395, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.5, 0.4, 0.1, 0.2, 0.8, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Health': 0.5, 'Insult': 0.4, 'Legal': 0.1, 'Profanity': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:19
191,191,"Therefore, by their fruits you will know them.",40007020,"ChatCompletion(id='chatcmpl-8dJkRNqOANOV9VnSyvxkLZyJg39Fw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381395, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=306, total_tokens=307))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 313
  candidates_token_count: 1
  total_token_count: 314
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.6, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:20
192,192,"Not everyone who says to me, 'Lord, Lord,' will enter into the Kingdom of Heaven; but he who does the will of my Father who is in heaven.",40007021,"ChatCompletion(id='chatcmpl-8dJkS5Esuxbit4ijwrEcLxLl593tu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381396, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 1
  total_token_count: 338
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:21
193,193,"Many will tell me in that day, 'Lord, Lord, didn't we prophesy in your name, in your name cast out demons, and in your name do many mighty works?'",40007022,"ChatCompletion(id='chatcmpl-8dJkTUTffEgwsihGl80s6OFjsdIfz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381397, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=334, total_tokens=335))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 343
  candidates_token_count: 1
  total_token_count: 344
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:22
194,194,"Then I will tell them, 'I never knew you. Depart from me, you who work iniquity.'",40007023,"ChatCompletion(id='chatcmpl-8dJkTD5VRUVxMJMXcdJULA4kQeYfR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381397, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  total_token_count: 324
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.4, 0.1, 0.1, 0.6, 0.1, 0.2, 0.2, 0.3, 1.0, 0.1, 0.4, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.4, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.6, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.4, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:23
195,195,"""Everyone therefore who hears these words of mine, and does them, I will liken him to a wise man, who built his house on a rock.",40007024,"ChatCompletion(id='chatcmpl-8dJkTFXBWVyNhDW4XFGomkBng4uJN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381397, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.1, 0.9, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:24
196,196,"The rain came down, the floods came, and the winds blew, and beat on that house; and it didn't fall, for it was founded on the rock.",40007025,"ChatCompletion(id='chatcmpl-8dJkUFWYqqpFHlVahIemWfEaC5y8w', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381398, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.5, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:25
197,197,"Everyone who hears these words of mine, and doesn't do them will be like a foolish man, who built his house on the sand.",40007026,"ChatCompletion(id='chatcmpl-8dJkVG928FZGPTXpcJwHOZauTzwKB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381399, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  total_token_count: 332
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.3, 0.6, 0.3, 0.2, 0.8, 0.1, 0.3], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.6, 'Politics': 0.3, 'Profanity': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.3}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:26
198,198,"The rain came down, the floods came, and the winds blew, and beat on that house; and it fell--and great was its fall.""",40007027,"ChatCompletion(id='chatcmpl-8dJkV0TuM8zcD97rhiCIM4ckLwkB2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381399, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.1, 0.1, 0.4, 0.2, 0.1, 0.1, 0.9, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Legal': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:27
199,199,"It happened, when Jesus had finished saying these things, that the multitudes were astonished at his teaching,",40007028,"ChatCompletion(id='chatcmpl-8dJkW8CAu51nGZO7Tpwii6KB9hjFa', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381400, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:28
200,200,"for he taught them with authority, and not like the scribes.",40007029,"ChatCompletion(id='chatcmpl-8dJkW0gWxk7zf7lCXp9kwRDAOnm6z', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381400, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.4, 0.3, 0.2, 0.4, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.4, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 7:29
201,201,"When he came down from the mountain, great multitudes followed him.",40008001,"ChatCompletion(id='chatcmpl-8dJkXNOVZTUO9Uj1sj5JYFDxuAOcN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381401, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.3, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:1
202,202,"Behold, a leper came to him and worshiped him, saying, ""Lord, if you want to, you can make me clean.""",40008002,"ChatCompletion(id='chatcmpl-8dJkXUmDsKzw7zBqKCSPEcpfBRLnf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381401, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 2
  total_token_count: 334
}
",10,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.7, 0.3, 0.1, 1.0, 0.3, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.7, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:2
203,203,"Jesus stretched out his hand, and touched him, saying, ""I want to. Be made clean."" Immediately his leprosy was cleansed.",40008003,"ChatCompletion(id='chatcmpl-8dJkYihZBRAAGO3UZcapPtCacwu37', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381402, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 2
  total_token_count: 332
}
",10,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.9, 0.1, 0.2, 0.1, 1.0, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.9, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:3
204,204,"Jesus said to him, ""See that you tell nobody, but go, show yourself to the priest, and offer the gift that Moses commanded, as a testimony to them.""",40008004,"ChatCompletion(id='chatcmpl-8dJkYHBL1PqNRWqpMra5jQfcPhWyB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381402, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:4
205,205,"When he came into Capernaum, a centurion came to him, asking him,",40008005,"ChatCompletion(id='chatcmpl-8dJkZLnGTidsvQmvJRcXWHb7WTDXu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381403, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:5
206,206,"and saying, ""Lord, my servant lies in the house paralyzed, grievously tormented.""",40008006,"ChatCompletion(id='chatcmpl-8dJkZQ3GMV3gkVI3dYyj6CH04bTzx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381403, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.5, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 8:6
207,207,"Jesus said to him, ""I will come and heal him.""",40008007,"ChatCompletion(id='chatcmpl-8dJkaEMy9XcDQqPKNAuw8Rvkc0Sv9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381404, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:7
208,208,"The centurion answered, ""Lord, I'm not worthy for you to come under my roof. Just say the word, and my servant will be healed.",40008008,"ChatCompletion(id='chatcmpl-8dJkapBWBYgzK8ieKC75qzzAM9O7z', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381404, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 2
  total_token_count: 338
}
",10,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.3, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:8
209,209,"For I am also a man under authority, having under myself soldiers. I tell this one, 'Go,' and he goes; and tell another, 'Come,' and he comes; and tell my servant, 'Do this,' and he does it.""",40008009,"ChatCompletion(id='chatcmpl-8dJkbnHTUmZkRI4f4ToQ6Ya2oV9IM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381405, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=347, total_tokens=348))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 354
  candidates_token_count: 1
  total_token_count: 355
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:9
210,210,"When Jesus heard it, he marveled, and said to those who followed, ""Most assuredly I tell you, I haven't found so great a faith, not even in Israel.",40008010,"ChatCompletion(id='chatcmpl-8dJkbHw6uW9MhdlQieKVQQxAgCZ7S', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381405, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=334, total_tokens=335))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 341
  candidates_token_count: 1
  total_token_count: 342
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.3, 0.4, 0.6, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Health': 0.3, 'Insult': 0.4, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:10
211,211,"I tell you that many will come from the east and the west, and will sit down with Abraham, Isaac, and Jacob in the Kingdom of Heaven,",40008011,"ChatCompletion(id='chatcmpl-8dJkcnwYkLARwC1wsqjAW4NIdpE4E', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381406, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:11
212,212,"but the children of the Kingdom will be thrown out into the outer darkness. There will be weeping and the gnashing of teeth.""",40008012,"ChatCompletion(id='chatcmpl-8dJkcYtayulOmm3axNKIIyAMowrFn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381406, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.5, 0.1, 0.3, 0.2, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:12
213,213,"Jesus said to the centurion, ""Go your way. Let it be done for you as you have believed."" His servant was healed in that hour.",40008013,"ChatCompletion(id='chatcmpl-8dJkd973ZjgJ7Hwn9rv5R6fTvTks9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381407, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:13
214,214,"When Jesus came into Peter's house, he saw his wife's mother lying sick with a fever.",40008014,"ChatCompletion(id='chatcmpl-8dJkeiAko0aktmrZidbvMNjRk1Kew', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381408, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.9, 0.3, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Health': 0.9, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:14
215,215,"He touched her hand, and the fever left her. She got up and served him.{TR reads ""them"" instead of ""him""}",40008015,"ChatCompletion(id='chatcmpl-8dJkegTU2fGkshtyH1i81i5dOMv20', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381408, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.9, 0.1, 0.2, 0.1, 0.1, 0.9, 0.3, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.9, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:15
216,216,"When evening came, they brought to him many possessed with demons. He cast out the spirits with a word, and healed all who were sick;",40008016,"ChatCompletion(id='chatcmpl-8dJkfIXVYj2ne67vnaBbjwwMdVFGm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381409, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.5, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:16
217,217,"that it might be fulfilled which was spoken through Isaiah the prophet, saying: ""He took our infirmities, and bore our diseases.""",40008017,"ChatCompletion(id='chatcmpl-8dJkgtvsJeL1iH41yVbyPMlzJ3uxS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381410, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.9, 0.3, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.9, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:17
218,218,"Now when Jesus saw great multitudes around him, he gave the order to depart to the other side.",40008018,"ChatCompletion(id='chatcmpl-8dJkhlqZyyEuuKdQOq7ISpHvioDpY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381411, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:18
219,219,"A scribe came, and said to him, ""Teacher, I will follow you wherever you go.""",40008019,"ChatCompletion(id='chatcmpl-8dJkiBJVl8LjYgNUTLm40Svi5Cnq1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381412, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.1, 0.9, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:19
220,220,"Jesus said to him, ""The foxes have holes, and the birds of the sky have nests, but the Son of Man has nowhere to lay his head.""",40008020,"ChatCompletion(id='chatcmpl-8dJkjasJKCOgTL7a0TodzyRgbZPao', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381413, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 2
  total_token_count: 337
}
",50,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 8:20
221,221,"Another of his disciples said to him, ""Lord, allow me first to go and bury my father.""",40008021,"ChatCompletion(id='chatcmpl-8dJkjYbOJ1JAfG905z7z2uE0HM2GL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381413, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [1.0, 0.3, 0.2, 0.1, 0.4, 0.2, 0.2, 0.1, 1.0, 0.2, 0.3, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 1.0, 'Derogatory': 0.3, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:21
222,222,"But Jesus said to him, ""Follow me, and leave the dead to bury their own dead.""",40008022,"ChatCompletion(id='chatcmpl-8dJkkkmaM71JOxFSfghYwvENgwdXB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381414, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.4, 0.2, 0.1, 0.4, 0.1, 0.2, 0.1, 0.1, 1.0, 0.1, 0.3, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.4, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.3, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:22
223,223,"When he got into a boat, his disciples followed him.",40008023,"ChatCompletion(id='chatcmpl-8dJkkAin3wnhYbdeOeFjXiGDtip6r', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381414, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=308, total_tokens=309))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.2, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:23
224,224,"Behold, a great tempest arose in the sea, so much that the boat was covered with the waves, but he was asleep.",40008024,"ChatCompletion(id='chatcmpl-8dJklL1jlZm56ks8u8UMuJilJVoHw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381415, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 2
  total_token_count: 331
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.5, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 8:24
225,225,"They came to him, and woke him up, saying, ""Save us, Lord! We are dying!""",40008025,"ChatCompletion(id='chatcmpl-8dJklbp5CGd2Zlgg6SoZagwp21gYq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381415, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.2, 0.5, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:25
226,226,"He said to them, ""Why are you fearful, oh you of little faith?"" Then he got up, rebuked the wind and the sea, and there was a great calm.",40008026,"ChatCompletion(id='chatcmpl-8dJkmxHIZCxzZSybgzVqeDdtLt2cI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381416, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=334, total_tokens=335))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 340
  candidates_token_count: 2
  total_token_count: 342
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 8:26
227,227,"The men marveled, saying, ""What kind of man is this, that even the wind and the sea obey him?""",40008027,"ChatCompletion(id='chatcmpl-8dJkm2QC2AHzEt3nItSWmyDzCik8K', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381416, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:27
228,228,"When he came to the other side, into the country of the Gergesenes, two people possessed by demons met him there, coming out of the tombs, exceedingly fierce, so that nobody could pass by that way.",40008028,"ChatCompletion(id='chatcmpl-8dJknNa1VefuZ6Qf0pmDtauEqsAk2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381417, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=342, total_tokens=343))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 347
  candidates_token_count: 1
  total_token_count: 348
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.4, 0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Health': 0.4, 'Insult': 0.2, 'Legal': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:28
229,229,"Behold, they cried out, saying, ""What do we have to do with you, Jesus, Son of God? Have you come here to torment us before the time?""",40008029,"ChatCompletion(id='chatcmpl-8dJkoTHvRc2XDFc5pq1nW0pCHrajc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381418, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 2
  total_token_count: 340
}
",10,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.4, 0.2, 0.1, 1.0, 0.3, 0.3, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.1, 'Insult': 0.4, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.3, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 8:29
230,230,Now there was a herd of many pigs feeding far away from them.,40008030,"ChatCompletion(id='chatcmpl-8dJkoDgX6XYbn1BQPJgyWUnTtA1HG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381418, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.5, 0.4, 0.2, 0.1, 0.5, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.4, 'Legal': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.5, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:30
231,231,"The demons begged him, saying, ""If you cast us out, permit us to go away into the herd of pigs.""",40008031,"ChatCompletion(id='chatcmpl-8dJkpdsEAJWLy86NgRZwoLczX6ciy', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381419, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.1, 0.4, 0.5, 0.2, 0.2, 0.3, 0.1, 0.9, 0.2, 0.3, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:31
232,232,"He said to them, ""Go!"" They came out, and went into the herd of pigs: and behold, the whole herd of pigs rushed down the cliff into the sea, and died in the water.",40008032,"ChatCompletion(id='chatcmpl-8dJkp7a6EbbeIwtuGPGEdnQy418GG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381419, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=338, total_tokens=339))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 345
  candidates_token_count: 1
  total_token_count: 346
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.2, 0.5, 0.3, 0.1, 0.2, 0.9, 0.1, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.3, 'Legal': 0.1, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:32
233,233,"Those who fed them fled, and went away into the city, and told everything, including what happened to those who were possessed with demons.",40008033,"ChatCompletion(id='chatcmpl-8dJkqlig77LuPdRg3e0SLCJ1pRjGQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381420, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.5, 0.1, 0.3, 0.2, 0.2, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.3, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:33
234,234,"Behold, all the city came out to meet Jesus. When they saw him, they begged that he would depart from their borders.",40008034,"ChatCompletion(id='chatcmpl-8dJkreIUJi8BEFLDur2b6xSYrtkeM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381421, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.2, 0.3, 0.2, 0.3, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 8:34
235,235,"He entered into a boat, and crossed over, and came into his own city.",40009001,"ChatCompletion(id='chatcmpl-8dJkrXVDZHVZietLN6koxNTXTJpu5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381421, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.1, 0.3, 0.2, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:1
236,236,"Behold, they brought to him a man who was paralyzed, lying on a bed. Jesus, seeing their faith, said to the paralytic, ""Son, cheer up! Your sins are forgiven you.""",40009002,"ChatCompletion(id='chatcmpl-8dJksPp0XOE4jIXjWREs2kcLAwBSu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381422, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=339, total_tokens=340))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 2
  total_token_count: 346
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.4, 0.3, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 9:2
237,237,"Behold, some of the scribes said to themselves, ""This man blasphemes.""",40009003,"ChatCompletion(id='chatcmpl-8dJkszKi83BqeDlaCmMLhW9A8jEkn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381422, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.1, 0.5, 0.1, 0.3, 0.1, 1.0, 0.1, 0.3, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.5, 'Health': 0.1, 'Insult': 0.5, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.3, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:3
238,238,"Jesus, knowing their thoughts, said, ""Why do you think evil in your hearts?",40009004,"ChatCompletion(id='chatcmpl-8dJktrINIxlmBpMl3i3srgu3gcsoL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381423, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.2, 0.1, 0.4, 0.1, 0.1, 1.0, 0.1, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:4
239,239,"For which is easier, to say, 'Your sins are forgiven;' or to say, 'Get up, and walk?'",40009005,"ChatCompletion(id='chatcmpl-8dJkuP1xsIJ3pxfuCUP4kmhrJAI5C', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381424, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:5
240,240,"But that you may know that the Son of Man has authority on earth to forgive sins..."" (then he said to the paralytic), ""Get up, and take up your mat, and go up to your house.""",40009006,"ChatCompletion(id='chatcmpl-8dJkuzaudExfxaLy3sD2e7si4acmk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381424, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=341, total_tokens=342))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 347
  candidates_token_count: 2
  total_token_count: 349
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 9:6
241,241,He arose and departed to his house.,40009007,"ChatCompletion(id='chatcmpl-8dJkvbOoMRc2nDfuysth9Wud4vYbP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381425, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=304, total_tokens=305))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 311
  candidates_token_count: 1
  total_token_count: 312
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.5, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:7
242,242,"But when the multitudes saw it, they marveled and glorified God, who had given such authority to men.",40009008,"ChatCompletion(id='chatcmpl-8dJkvSEhnvZDelkv3lqrrwuKONKB5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381425, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.4, 0.2, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:8
243,243,"As Jesus passed by from there, he saw a man called Matthew sitting at the tax collection office. He said to him, ""Follow me."" He got up and followed him.",40009009,"ChatCompletion(id='chatcmpl-8dJkwCEMldTTIcA06xYNk9ipiHxq5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381426, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.7, 0.1, 0.1, 0.2, 0.2, 0.3, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.7, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:9
244,244,"It happened as he sat in the house, behold, many tax collectors and sinners came and sat down with Jesus and his disciples.",40009010,"ChatCompletion(id='chatcmpl-8dJkwevCJtIW8RGmerZeX5yLgkEck', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381426, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.2, 0.1, 0.4, 0.1, 0.3, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.3, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 9:10
245,245,"When the Pharisees saw it, they said to his disciples, ""Why does your teacher eat with tax collectors and sinners?""",40009011,"ChatCompletion(id='chatcmpl-8dJkxZhgFzpBhOYwrU8WFG7BhzQME', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381427, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.3, 0.5, 0.1, 0.4, 0.2, 0.6, 0.1, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Finance': 0.3, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:11
246,246,"When Jesus heard it, he said to them, ""Those who are healthy have no need for a physician, but those who are sick do.",40009012,"ChatCompletion(id='chatcmpl-8dJkx975IFkbzlRoA4ZsnabD1Nd8R', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381427, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.1, 1.0, 0.1, 0.3, 0.1, 0.2, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Finance': 0.1, 'Health': 1.0, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:12
247,247,"But you go and learn what this means: 'I desire mercy, and not sacrifice,' for I came not to call the righteous, but sinners to repentance.""",40009013,"ChatCompletion(id='chatcmpl-8dJkxinRq8CR2RphT2kHaL0zPJBkr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381427, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:13
248,248,"Then John's disciples came to him, saying, ""Why do we and the Pharisees fast often, but your disciples don't fast?""",40009014,"ChatCompletion(id='chatcmpl-8dJkygVL4uyRxOVviFQcGMqwVpM6X', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381428, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.6, 0.1, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:14
249,249,"Jesus said to them, ""Can the friends of the bridegroom mourn, as long as the bridegroom is with them? But the days will come when the bridegroom will be taken away from them, and then they will fast.",40009015,"ChatCompletion(id='chatcmpl-8dJkzuyyJXMsnJ9KyXRjmNw1kDrWr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381429, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=346, total_tokens=347))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 347
  candidates_token_count: 2
  total_token_count: 349
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.4, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:15
250,250,"No one puts a piece of unshrunk cloth on an old garment; for the patch would tear away from the garment, and a worse hole is made.",40009016,"ChatCompletion(id='chatcmpl-8dJkz40JDu3wTdKL6oqOpBYmdGSJl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381429, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.6, 0.2, 0.2, 0.1, 0.2, 0.6, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.1, 'Profanity': 0.2, 'Religion & Belief': 0.6, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:16
251,251,"Neither do people put new wine into old wineskins, or else the skins would burst, and the wine be spilled, and the skins ruined. No, they put new wine into fresh wineskins, and both are preserved.""",40009017,"ChatCompletion(id='chatcmpl-8dJkzMdz5Y9r8vJjid7NeW2pga99m', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381429, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=341, total_tokens=342))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 348
  candidates_token_count: 1
  total_token_count: 349
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.6, 0.1, 0.1, 0.2, 0.2, 0.8, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Legal': 0.2, 'Politics': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:17
252,252,"While he told these things to them, behold, a ruler came and worshiped him, saying, ""My daughter has just died, but come and lay your hand on her, and she will live.""",40009018,"ChatCompletion(id='chatcmpl-8dJl0nfz7oZKTX0pkA2ZEHCGsYzcN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381430, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=337, total_tokens=338))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 1
  total_token_count: 345
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.5, 0.1, 0.2, 0.1, 1.0, 0.3, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:18
253,253,"Jesus got up and followed him, as did his disciples.",40009019,"ChatCompletion(id='chatcmpl-8dJl15AqZNPU84YBV8SgXzRYyxa7x', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381431, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=308, total_tokens=309))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:19
254,254,"Behold, a woman who had an issue of blood for twelve years came behind him, and touched the tassels of his garment;",40009020,"ChatCompletion(id='chatcmpl-8dJl1rqwEL7DmPmDqsJT38VZjPqjW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381431, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 1.0, 0.1, 0.2, 0.2, 0.2, 0.2, 0.1, 0.7, 0.3, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Health': 1.0, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:20
255,255,"for she said within herself, ""If I just touch his garment, I will be made well.""",40009021,"ChatCompletion(id='chatcmpl-8dJl2XQj97jRjpObQQHQTimQjd2AB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381432, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  total_token_count: 323
}
",-1,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.8, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.9, 0.1, 0.2, 0.6, 0.8, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.9, 'Insult': 0.1, 'Profanity': 0.2, 'Religion & Belief': 0.6, 'Sexual': 0.8, 'Toxic': 0.3, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 9:21
256,256,"But Jesus, turning around and seeing her, said, ""Daughter, cheer up! Your faith has made you well."" And the woman was made well from that hour.",40009022,"ChatCompletion(id='chatcmpl-8dJl20c3ncuTHLavU26GzZAn49yCi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381432, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 1
  total_token_count: 338
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:22
257,257,"When Jesus came into the ruler's house, and saw the flute players, and the crowd in noisy disorder,",40009023,"ChatCompletion(id='chatcmpl-8dJl3BKuV87gaYDaywhGcrgqudLNS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381433, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 2
  total_token_count: 328
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.2, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:23
258,258,"he said to them, ""Make room, because the girl isn't dead, but sleeping."" They were ridiculing him.",40009024,"ChatCompletion(id='chatcmpl-8dJl3VlFgNXNHzuJP1sS1LkfY4blU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381433, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.4, 0.5, 0.5, 0.2, 0.2, 0.2, 0.1, 0.6, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.4, 'Health': 0.5, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.6, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:24
259,259,"But when the crowd was put out, he entered in, took her by the hand, and the girl arose.",40009025,"ChatCompletion(id='chatcmpl-8dJl4WspSHtUWnZbESpeqVjqS8pZh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381434, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.8, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:25
260,260,The report of this went out into all that land.,40009026,"ChatCompletion(id='chatcmpl-8dJl4w4BJr32RfYMecFyGENmDaoEH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381434, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=307, total_tokens=308))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 314
  candidates_token_count: 1
  total_token_count: 315
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.2, 0.2, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:26
261,261,"As Jesus passed by from there, two blind men followed him, calling out and saying, ""Have mercy on us, son of David!""",40009027,"ChatCompletion(id='chatcmpl-8dJl59c3wRJ8T1NVL5fNTM1UKPcmW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381435, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 2
  total_token_count: 333
}
",10,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 9:27
262,262,"When he had come into the house, the blind men came to him. Jesus said to them, ""Do you believe that I am able to do this?"" They told him, ""Yes, Lord.""",40009028,"ChatCompletion(id='chatcmpl-8dJl5iCXN7rY1nQV3EGlkYBaFL3YD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381435, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=337, total_tokens=338))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 2
  total_token_count: 346
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:28
263,263,"Then he touched their eyes, saying, ""According to your faith be it done to you.""",40009029,"ChatCompletion(id='chatcmpl-8dJl6Eu8Mk4hSf2rHNxKLzIHST8bh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381436, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:29
264,264,"Their eyes were opened. Jesus strictly charged them, saying, ""See that no one knows about this.""",40009030,"ChatCompletion(id='chatcmpl-8dJl7RsMbaOcQNO4X8RSYY5v19d2b', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381437, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 0.2, 0.1, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:30
265,265,But they went out and spread abroad his fame in all that land.,40009031,"ChatCompletion(id='chatcmpl-8dJl7T2qo5ysLAbVUbgwLIDDiohWb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381437, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.2, 0.2, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:31
266,266,"As they went out, behold, a mute man who was demon possessed was brought to him.",40009032,"ChatCompletion(id='chatcmpl-8dJl7Mb1VlrU3z3O9ANzWnKHyabn1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381437, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.5, 0.4, 0.2, 0.3, 0.1, 1.0, 0.3, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.4, 'Health': 0.5, 'Insult': 0.4, 'Legal': 0.2, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:32
267,267,"When the demon was cast out, the mute man spoke. The multitudes marveled, saying, ""Nothing like this has ever been seen in Israel!""",40009033,"ChatCompletion(id='chatcmpl-8dJl8EWFCMDdBidcvFkrqfFDFkjGN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381438, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.5, 0.3, 0.1, 0.2, 0.2, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.4, 'Health': 0.5, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:33
268,268,"But the Pharisees said, ""By the prince of the demons, he casts out demons.""",40009034,"ChatCompletion(id='chatcmpl-8dJl8IAj60dScxrpNrmfDVOAxpBww', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381438, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.3, 0.3, 0.3, 0.1, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Health': 0.3, 'Insult': 0.3, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:34
269,269,"Jesus went about all the cities and the villages, teaching in their synagogues, and preaching the Gospel of the Kingdom, and healing every disease and every sickness among the people.",40009035,"ChatCompletion(id='chatcmpl-8dJl95LqwxX9YFkOK4TPHA8zYgEJo', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381439, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 2
  total_token_count: 340
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:35
270,270,"But when he saw the multitudes, he was moved with compassion for them, because they were harassed{TR reads ""weary"" instead of ""harassed""} and scattered, like sheep without a shepherd.",40009036,"ChatCompletion(id='chatcmpl-8dJl9RR0RwbZdr751lBKCVJryIKDv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381439, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=338, total_tokens=339))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 2
  total_token_count: 346
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.4, 0.5, 0.6, 0.2, 0.2, 1.0, 0.3, 0.2, 0.2, 0.5], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.5, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.2, 'Violent': 0.2, 'War & Conflict': 0.5}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:36
271,271,"Then he said to his disciples, ""The harvest indeed is plentiful, but the laborers are few.",40009037,"ChatCompletion(id='chatcmpl-8dJlAhrAo7lJ1I4vG21WXeX9iBfug', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381440, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.5, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:37
272,272,"Pray therefore that the Lord of the harvest will send out laborers into his harvest.""",40009038,"ChatCompletion(id='chatcmpl-8dJlAuumYUlAWVlhEcaUWgdqAUqSe', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381440, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.1, 0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 9:38
273,273,"He called to himself his twelve disciples, and gave them authority over unclean spirits, to cast them out, and to heal every disease and every sickness.",40010001,"ChatCompletion(id='chatcmpl-8dJlBJEJuHwQ6Ls7wezyNEBrPbuH4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381441, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.7, 0.3, 0.1, 0.2, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Health': 0.7, 'Insult': 0.3, 'Legal': 0.1, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:1
274,274,"Now the names of the twelve apostles are these. The first, Simon, who is called Peter; Andrew, his brother; James the son of Zebedee; John, his brother;",40010002,"ChatCompletion(id='chatcmpl-8dJlEDGQqOHOB0X4uA9N8Z44a5ZQB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381444, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=335, total_tokens=336))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 341
  candidates_token_count: 1
  total_token_count: 342
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:2
275,275,"Philip; Bartholomew; Thomas; Matthew the tax collector; James the son of Alphaeus; and Lebbaeus, whose surname was Thaddaeus;",40010003,"ChatCompletion(id='chatcmpl-8dJlFDPiPeBJwqNiJ9P5NZzPEqkWN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381445, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.7, 0.3, 0.1, 0.2, 0.2, 0.2, 0.1, 0.8, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.7, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:3
276,276,"Simon the Canaanite; and Judas Iscariot, who also betrayed him.",40010004,"ChatCompletion(id='chatcmpl-8dJlFJ48ff1grsvOiFDvFniEj0K1d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381445, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 0.2, 0.4, 0.1, 0.4, 0.1, 0.3, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.4, 'Profanity': 0.1, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:4
277,277,"Jesus sent these twelve out, and charged them, saying, ""Don't go among the Gentiles, and don't enter into any city of the Samaritans.",40010005,"ChatCompletion(id='chatcmpl-8dJlG3UR7fPuEEIasuCxQKTMaPqGz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381446, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 1
  total_token_count: 338
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.5, 0.4, 0.4, 0.1, 0.1, 0.1, 1.0, 0.1, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.5, 'Health': 0.4, 'Insult': 0.4, 'Legal': 0.1, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.3, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:5
278,278,"Rather, go to the lost sheep of the house of Israel.",40010006,"ChatCompletion(id='chatcmpl-8dJlG2s0Iz6R0SzUET9dUbeCz3rYZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381446, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 0.3, 0.4, 0.3, 0.1, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.4, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:6
279,279,"As you go, preach, saying, 'The Kingdom of Heaven is at hand!'",40010007,"ChatCompletion(id='chatcmpl-8dJlHIT0Dwl3OLSaTvkSO9pUfYwAZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381447, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.1, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:7
280,280,"Heal the sick, cleanse the lepers{TR adds "", raise the dead""}, and cast out demons. Freely you received, so freely give.",40010008,"ChatCompletion(id='chatcmpl-8dJlHVCrFLRnn9jXal0yJWhUBBxxS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381447, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.6, 0.1, 0.2, 0.2, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:8
281,281,"Don't take any gold, nor silver, nor brass in your money belts.",40010009,"ChatCompletion(id='chatcmpl-8dJlIq1PoDLhaiv0g4ra4ZZtOcMde', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381448, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 1.0, 0.2, 0.3, 0.1, 0.1, 0.1, 0.7, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 1.0, 'Health': 0.2, 'Insult': 0.3, 'Legal': 0.1, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:9
282,282,"Take no bag for your journey, neither two coats, nor shoes, nor staff: for the laborer is worthy of his food.",40010010,"ChatCompletion(id='chatcmpl-8dJlIa3Y0PZfxiFCGYbSKHT9QaHxV', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381448, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 2
  total_token_count: 331
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.6, 0.2, 0.1, 0.7, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 10:10
283,283,"Into whatever city or village you enter, find out who in it is worthy; and stay there until you go on.",40010011,"ChatCompletion(id='chatcmpl-8dJlJV07WD3FynXc8Ai8Rg5q3AZQw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381449, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 2
  total_token_count: 329
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:11
284,284,"As you enter into the household, greet it.",40010012,"ChatCompletion(id='chatcmpl-8dJlJOkz6C62ljh2bGq3Aa0PrpDTR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381449, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=306, total_tokens=307))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 313
  candidates_token_count: 1
  total_token_count: 314
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.3, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:12
285,285,"If the household is worthy, let your peace come on it, but if it isn't worthy, let your peace return to you.",40010013,"ChatCompletion(id='chatcmpl-8dJlKQltXBUCscI97hjelrgKdRVRr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381450, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 0.2, 0.9, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:13
286,286,"Whoever doesn't receive you, nor hear your words, as you go out of that house or that city, shake off the dust from your feet.",40010014,"ChatCompletion(id='chatcmpl-8dJlKB1UKTHeKfZIHzyCAETkAcVMQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381450, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.3, 0.3, 0.2, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.3, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:14
287,287,"Most assuredly I tell you, it will be more tolerable for the land of Sodom and Gomorrah in the day of judgment than for that city.",40010015,"ChatCompletion(id='chatcmpl-8dJlLb03ko06bJgtFhpaUZDCYAUwH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381451, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.5, 0.1, 0.3, 0.1, 0.3, 0.2, 0.1, 1.0, 0.4, 0.3], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.4, 'Toxic': 0.3}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:15
288,288,"""Behold, I send you out as sheep in the midst of wolves. Therefore be wise as serpents, and harmless as doves.",40010016,"ChatCompletion(id='chatcmpl-8dJlLs3JpojAGhRqFd2EYTQu7SYJP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381451, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 2
  total_token_count: 331
}
",50,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 10:16
289,289,"But beware of men: for they will deliver you up to councils, and in their synagogues they will scourge you.",40010017,"ChatCompletion(id='chatcmpl-8dJlMT6Gule3bXMYkuziu9f0rtRXF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381452, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 327
  total_token_count: 327
}
",-1,"MultiCandidateTextGenerationResponse(text='', _prediction_response=Prediction(predictions=[{'content': '', 'citationMetadata': None, 'safetyAttributes': {'blocked': True, 'errors': [252.0]}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=True, errors=(252,), safety_attributes={}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[TextGenerationResponse(text='', is_blocked=True, errors=(252,), safety_attributes={}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]))])",-1,Matthew 10:17
290,290,"Yes, and you will be brought before governors and kings for my sake, for a testimony to them and to the Gentiles.",40010018,"ChatCompletion(id='chatcmpl-8dJlM5aJ4lNNzM6eOcITkQrAFHpw3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381452, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.1, 0.3, 0.1, 0.2, 0.2, 0.6, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:18
291,291,"But when they deliver you up, don't be anxious how or what you will say, for it will be given you in that hour what you will say.",40010019,"ChatCompletion(id='chatcmpl-8dJlN1Nebd8U4CqIXnGQc86eqKDWN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381453, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.2, 0.1, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:19
292,292,"For it is not you who speak, but the Spirit of your Father who speaks in you.",40010020,"ChatCompletion(id='chatcmpl-8dJlNyTiyt7oFdSVuXsHC9mYqXJkY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381453, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:20
293,293,"""Brother will deliver up brother to death, and the father his child. Children will rise up against parents, and cause them to be put to death.",40010021,"ChatCompletion(id='chatcmpl-8dJlOknzdEkOz29E3nT8TB9EUqzk8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381454, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.9, 0.4, 0.3, 0.6, 0.5, 0.1, 0.3, 0.1, 0.9, 0.2, 0.5, 0.5, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.9, 'Derogatory': 0.4, 'Health': 0.3, 'Insult': 0.6, 'Legal': 0.5, 'Politics': 0.1, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.5, 'Violent': 0.5, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:21
294,294,"You will be hated by all men for my name's sake, but he who endures to the end will be saved.",40010022,"ChatCompletion(id='chatcmpl-8dJlOBjkCWtdMgtGGsbfMUVgqSvhe', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381454, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 2
  total_token_count: 330
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.6, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.6, 0.3, 0.6, 0.1, 0.2, 0.3, 0.1, 0.9, 0.2, 0.5, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.6, 'Health': 0.3, 'Insult': 0.6, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.5, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:22
295,295,"But when they persecute you in this city, flee into the next, for most assuredly I tell you, you will not have gone through the cities of Israel, until the Son of Man has come.",40010023,"ChatCompletion(id='chatcmpl-8dJlOgU2xag9EN5vptMOLwqENNWkg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381454, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=338, total_tokens=339))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 2
  total_token_count: 346
}
",50,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.4, 0.1, 0.2, 0.1, 0.1, 1.0, 0.1, 0.2, 0.2, 0.5], 'categories': ['Derogatory', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.2, 'War & Conflict': 0.5}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 10:23
296,296,"""A disciple is not above his teacher, nor a servant above his lord.",40010024,"ChatCompletion(id='chatcmpl-8dJlPbPAA2YrDZyYVazj062WEeiXA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381455, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:24
297,297,"It is enough for the disciple that he be like his teacher, and the servant like his lord. If they have called the master of the house Beelzebul, how much more those of his household!",40010025,"ChatCompletion(id='chatcmpl-8dJlPuzPS0O2aVJP9sbhEFLUtQsWs', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381455, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=338, total_tokens=339))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 345
  candidates_token_count: 1
  total_token_count: 346
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:25
298,298,"Therefore don't be afraid of them, for there is nothing covered that will not be revealed; and hidden that will not be known.",40010026,"ChatCompletion(id='chatcmpl-8dJlQ5biUeY76Kv9D6IUhUIGlhtuZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381456, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.6, 0.2, 0.2, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:26
299,299,"What I tell you in the darkness, speak in the light; and what you hear whispered in the ear, proclaim on the housetops.",40010027,"ChatCompletion(id='chatcmpl-8dJlQyYVxRruLfRzUp5PGf33I1UNY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381456, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.2, 0.2, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:27
300,300,"Don't be afraid of those who kill the body, but are not able to kill the soul. Rather, fear him who is able to destroy both soul and body in Gehenna.",40010028,"ChatCompletion(id='chatcmpl-8dJlRxv3hJrC8zQsq7Aepb63jozGE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381457, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 341
  candidates_token_count: 1
  total_token_count: 342
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.4, 0.3, 0.1, 0.3, 0.2, 1.0, 0.1, 0.3, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.4, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.3, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:28
301,301,"""Aren't two sparrows sold for an assarion{An assarion is a small coin worth one tenth of a drachma or a sixteenth of a denarius (approximately the wages of one half hour of agricultural labor).}? Not one of them falls on the ground apart from your Father's will,",40010029,"ChatCompletion(id='chatcmpl-8dJlREMb1VdChZ2spZFizJXsNbo6B', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381457, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=361, total_tokens=362))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 366
  candidates_token_count: 1
  total_token_count: 367
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.3, 0.1, 0.3, 0.2, 0.3, 0.4, 0.1, 1.0, 0.3, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.3, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.4, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:29
302,302,but the very hairs of your head are all numbered.,40010030,"ChatCompletion(id='chatcmpl-8dJlScmZziHezrxLgups1xIQagziH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381458, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=307, total_tokens=308))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 314
  candidates_token_count: 1
  total_token_count: 315
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.2, 0.1, 0.7, 0.2, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:30
303,303,Therefore don't be afraid. You are of more value than many sparrows.,40010031,"ChatCompletion(id='chatcmpl-8dJlTN8YfBZr9CNrLh4hPsbn6kils', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381459, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.4, 0.3, 0.2, 0.9, 0.2, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.3, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:31
304,304,"Everyone therefore who confesses me before men, him I will also confess before my Father who is in heaven.",40010032,"ChatCompletion(id='chatcmpl-8dJlTjr2wAtKsEN6LoV0gfSvwRVVx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381459, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:32
305,305,"But whoever denies me before men, him I will also deny before my Father who is in heaven.",40010033,"ChatCompletion(id='chatcmpl-8dJlUzpHQ0U7IlNMMpWp0e5yojJG6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381460, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.4, 0.3, 0.5, 0.4, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.4, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.4, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:33
306,306,"""Don't think that I came to send peace on the earth. I didn't come to send peace, but a sword.",40010034,"ChatCompletion(id='chatcmpl-8dJlUn9UPWBxLhhgOSX4pPH42Nl2M', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381460, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 2
  total_token_count: 333
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.3, 0.1, 0.1, 0.7, 0.1, 0.1, 0.1, 0.4], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.1, 'Toxic': 0.1, 'Violent': 0.1, 'War & Conflict': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 10:34
307,307,"For I came to set a man at odds against his father, and a daughter against her mother, and a daughter-in-law against her mother-in-law.",40010035,"ChatCompletion(id='chatcmpl-8dJlV05sxUMeLJohPZXBg6KzkUTvO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381461, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.3, 0.4, 0.9, 0.6, 0.2, 0.1, 0.6, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.4, 'Legal': 0.9, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.6, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:35
308,308,A man's foes will be those of his own household.,40010036,"ChatCompletion(id='chatcmpl-8dJlV0TVfO3LN7qPZpCbL9OqrhW40', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381461, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=308, total_tokens=309))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.5, 0.1, 0.2, 0.5, 0.2, 0.6, 0.2, 0.1, 0.8, 0.1, 0.2, 0.1, 0.4], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.5, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.1, 'War & Conflict': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:36
309,309,He who loves father or mother more than me is not worthy of me; and he who loves son or daughter more than me isn't worthy of me.,40010037,"ChatCompletion(id='chatcmpl-8dJlWisOi8TAzKusMSKhwY0RsLf2c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381462, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.5, 0.2, 0.2, 0.6, 0.3, 0.3], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.5, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.6, 'Sexual': 0.3, 'Toxic': 0.3}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:37
310,310,"He who doesn't take his cross and follow after me, isn't worthy of me.",40010038,"ChatCompletion(id='chatcmpl-8dJlaZf17c6Vges3o4JTKEa3eGpoa', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381466, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""70""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 2
  total_token_count: 325
}
",70,"MultiCandidateTextGenerationResponse(text=' 80', _prediction_response=Prediction(predictions=[{'content': ' 80', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.4, 0.3, 0.1, 0.9, 0.1, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.4, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 80])",80,Matthew 10:38
311,311,He who finds his life will lose it; and he who loses his life for my sake will find it.,40010039,"ChatCompletion(id='chatcmpl-8dJlaxaAqghJCmtmC3CurwssmTRF8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381466, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 2
  total_token_count: 327
}
",50,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.2, 0.1, 0.3, 0.3, 0.2, 0.1, 0.9, 0.1, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.3, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:39
312,312,"He who receives you receives me, and he who receives me receives him who sent me.",40010040,"ChatCompletion(id='chatcmpl-8dJlbOIzH1kjbR8mHorhifY45vgSG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381467, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.2, 0.1, 0.2, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:40
313,313,He who receives a prophet in the name of a prophet will receive a prophet's reward: and he who receives a righteous man in the name of a righteous man will receive a righteous man's reward.,40010041,"ChatCompletion(id='chatcmpl-8dJlb5jQjuVy80cwp6oTaxcJQKK0R', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381467, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=336, total_tokens=337))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 345
  candidates_token_count: 1
  total_token_count: 346
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.4, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:41
314,314,"Whoever gives one of these little ones just a cup of cold water to drink in the name of a disciple, most assuredly I tell you he will in no way lose his reward.""",40010042,"ChatCompletion(id='chatcmpl-8dJlc4Xgs8kKiemSMQO5w5BdyU9av', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381468, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=334, total_tokens=335))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.6, 0.1, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 10:42
315,315,"It happened that when Jesus had finished directing his twelve disciples, he departed from there to teach and preach in their cities.",40011001,"ChatCompletion(id='chatcmpl-8dJlczgcehKzBQyBN5hnctvB3cJzF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381468, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:1
316,316,"Now when John heard in the prison the works of Christ, he sent two of his disciples",40011002,"ChatCompletion(id='chatcmpl-8dJldUNvA4kgm1TxUobKIVPzQD1VY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381469, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.1, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:2
317,317,"and said to him, ""Are you he who comes, or should we look for another?""",40011003,"ChatCompletion(id='chatcmpl-8dJlddf4YlsX21iaxEMd6ZbtyT3lc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381469, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.1, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:3
318,318,"Jesus answered them, ""Go and tell John the things which you hear and see:",40011004,"ChatCompletion(id='chatcmpl-8dJldIH4RdFfnvkzqaWfMHpjMPgvv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381469, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:4
319,319,"the blind receive their sight, the lame walk, the lepers are cleansed, the deaf hear, the dead are raised up, and the poor have good news preached to them.",40011005,"ChatCompletion(id='chatcmpl-8dJleRr2DNM0Q5uZILCCMOYxX8Ez8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381470, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.6, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:5
320,320,"Blessed is he who finds no occasion for stumbling in me.""",40011006,"ChatCompletion(id='chatcmpl-8dJleQmvmWVMfigrHp3AMfMteUA5Q', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381470, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:6
321,321,"As these went their way, Jesus began to say to the multitudes concerning John, ""What did you go out into the wilderness to see? A reed shaken by the wind?",40011007,"ChatCompletion(id='chatcmpl-8dJlfHwzDdqnMkmmWOJgAMhX1nvbc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381471, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:7
322,322,"But what did you go out to see? A man in soft clothing? Behold, those who wear soft clothing are in king's houses.",40011008,"ChatCompletion(id='chatcmpl-8dJlgcFOt9w1PnNPDDlgC6g8YNPaW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381472, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.2, 0.6, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 0.6, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 11:8
323,323,"But why did you go out? To see a prophet? Yes, I tell you, and much more than a prophet.",40011009,"ChatCompletion(id='chatcmpl-8dJlgnUXGskeqQTTex9WmS6TWQMFf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381472, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.4, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.4, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:9
324,324,"For this is he, of whom it is written, 'Behold, I send my messenger before your face, who will prepare your way before you.'",40011010,"ChatCompletion(id='chatcmpl-8dJlhguG6XcA0wIo0k6E0xxIjZ1d5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381473, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:10
325,325,"Most assuredly I tell you, among those who are born of women there has not arisen anyone greater than John the Baptizer; yet he who is least in the Kingdom of Heaven is greater than he.",40011011,"ChatCompletion(id='chatcmpl-8dJlh4SabylOl4QxiAviUjCq2AvX5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381473, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=337, total_tokens=338))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 1
  total_token_count: 345
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:11
326,326,"From the days of John the Baptizer until now, the Kingdom of Heaven suffers violence, and the violent take it by force.",40011012,"ChatCompletion(id='chatcmpl-8dJliljF7gcyCclUcUJj349YlnpPM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381474, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 2
  total_token_count: 332
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.1, 0.4, 0.2, 0.4, 0.1, 0.3, 1.0, 0.3, 0.3, 0.8, 0.5], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.4, 'Profanity': 0.1, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.3, 'Violent': 0.8, 'War & Conflict': 0.5}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 11:12
327,327,For all the prophets and the law prophesied until John.,40011013,"ChatCompletion(id='chatcmpl-8dJliOwrwC28Thaxhe0RHM75jCtDq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381474, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:13
328,328,"If you are willing to receive it, this is Elijah, who is to come.",40011014,"ChatCompletion(id='chatcmpl-8dJliAizFt0jFS6AQBikR4pb9zJAI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381474, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:14
329,329,"He who has ears to hear, let him hear.",40011015,"ChatCompletion(id='chatcmpl-8dJljgwDad791wF6KA6g1FBRGo6kl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381475, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=307, total_tokens=308))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 314
  candidates_token_count: 1
  total_token_count: 315
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.6, 0.2, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.6, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:15
330,330,"""But to what shall I compare this generation? It is like children sitting in the marketplaces, who call to their companions",40011016,"ChatCompletion(id='chatcmpl-8dJlkHuGLKenMUaz00DsvYafe2oqP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381476, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.1, 0.2, 0.2, 0.1, 0.8, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:16
331,331,"and say, 'We played the flute for you, and you didn't dance. We mourned for you, and you didn't lament.'",40011017,"ChatCompletion(id='chatcmpl-8dJlkeY2fq0AoxIMoGGpZOE5yybd5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381476, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 0.2, 0.1, 0.7, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:17
332,332,"For John came neither eating nor drinking, and they say, 'He has a demon.'",40011018,"ChatCompletion(id='chatcmpl-8dJlloFmADlWLZcnTMgTUETgLWO76', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381477, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.4, 0.5, 0.4, 1.0, 0.3, 0.3], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Health': 0.4, 'Insult': 0.5, 'Profanity': 0.4, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.3}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 11:18
333,333,"The Son of Man came eating and drinking, and they say, 'Behold, a gluttonous man and a drunkard, a friend of tax collectors and sinners!' But wisdom is justified by her children.""",40011019,"ChatCompletion(id='chatcmpl-8dJllQMiuym9NmgKk3dzAoqhnJmfc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381477, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=340, total_tokens=341))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 1
  total_token_count: 345
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.6, 0.1, 0.4, 0.3, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:19
334,334,"Then he began to denounce the cities in which most of his mighty works had been done, because they didn't repent.",40011020,"ChatCompletion(id='chatcmpl-8dJllP7NwU2eGrDtkvSWgAj7G9HlC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381477, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.4, 0.1, 0.2, 0.1, 0.4, 0.2, 0.6, 0.1, 0.1, 1.0, 0.1, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:20
335,335,"""Woe to you, Chorazin! Woe to you, Bethsaida! For if the mighty works had been done in Tyre and Sidon which were done in you, they would have repented long ago in sackcloth and ashes.",40011021,"ChatCompletion(id='chatcmpl-8dJlm6UPZIBweLYl3ZK1P1unpkfkZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381478, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=347, total_tokens=348))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 352
  candidates_token_count: 1
  total_token_count: 353
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.5, 0.4, 0.4, 0.1, 1.0, 0.1, 0.3, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Politics': 0.4, 'Profanity': 0.4, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.3, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:21
336,336,"But I tell you, it will be more tolerable for Tyre and Sidon on the day of judgment than for you.",40011022,"ChatCompletion(id='chatcmpl-8dJln8xMqlEKcWaFyuqDgOhTBo2A3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381479, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.3, 0.4, 0.5, 0.6, 0.1, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.4, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:22
337,337,"You, Capernaum, who are exalted to Heaven, you will go down to Hades. For if the mighty works had been done in Sodom which were done in you, it would have remained until this day.",40011023,"ChatCompletion(id='chatcmpl-8dJln4er0IPl0Mik2xbAnflGr4XNY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381479, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=342, total_tokens=343))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 347
  candidates_token_count: 1
  total_token_count: 348
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.5, 0.1, 0.3, 0.3, 1.0, 0.5, 0.3], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Profanity': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.5, 'Toxic': 0.3}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:23
338,338,"But I tell you that it will be more tolerable for the land of Sodom, on the day of judgment, than for you.""",40011024,"ChatCompletion(id='chatcmpl-8dJln9ULOLXdc2INjsaoaF3eVoqmf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381479, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.5, 0.1, 0.3, 0.2, 0.2, 0.1, 1.0, 0.4, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.4, 'Toxic': 0.3, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:24
339,339,"At that time, Jesus answered, ""I thank you, Father, Lord of heaven and earth, that you hid these things from the wise and understanding, and revealed them to infants.",40011025,"ChatCompletion(id='chatcmpl-8dJlotDtvgF4crdqLKn8GTuHpjTyK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381480, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 340
  candidates_token_count: 1
  total_token_count: 341
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.2, 0.1, 0.1, 1.0, 0.3, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:25
340,340,"Yes, Father, for so it was well-pleasing in your sight.",40011026,"ChatCompletion(id='chatcmpl-8dJloDYBKHbIZ8wf8mGuzz6PHly3p', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381480, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:26
341,341,"All things have been delivered to me by my Father. No one knows the Son, except the Father; neither does anyone know the Father, except the Son, and he to whom the Son desires to reveal him.",40011027,"ChatCompletion(id='chatcmpl-8dJlp1khpNSngV8rmeW59QI5DYFLw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381481, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=339, total_tokens=340))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 346
  candidates_token_count: 1
  total_token_count: 347
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:27
342,342,"""Come to me, all you who labor and are heavily burdened, and I will give you rest.",40011028,"ChatCompletion(id='chatcmpl-8dJlptjtxPbGVpYWHpuiHfcLtCuMU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381481, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 2
  total_token_count: 326
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 11:28
343,343,"Take my yoke upon you, and learn from me, for I am gentle and lowly in heart; and you will find rest for your souls.",40011029,"ChatCompletion(id='chatcmpl-8dJlqWAqwqCykLSa1tHKl0G2lzg9x', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381482, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 2
  total_token_count: 334
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.2, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:29
344,344,"For my yoke is easy, and my burden is light.""",40011030,"ChatCompletion(id='chatcmpl-8dJlqalBfLyNSDEv7j8xw6Ah2uVxI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381482, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 11:30
345,345,"At that time, Jesus went on the Sabbath day through the grain fields. His disciples were hungry and began to pluck heads of grain and to eat.",40012001,"ChatCompletion(id='chatcmpl-8dJlrTT7i5Lo07V0aYZhM18DaTBCQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381483, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 2
  total_token_count: 335
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.9, 0.3, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.9, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 12:1
346,346,"But the Pharisees, when they saw it, said to him, ""Behold, your disciples do what is not lawful to do on the Sabbath.""",40012002,"ChatCompletion(id='chatcmpl-8dJlrempdAfXIUONcNgtFH54Iins5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381483, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.5, 0.5, 0.1, 0.3, 0.5, 0.3, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.5, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:2
347,347,"But he said to them, ""Haven't you read what David did, when he was hungry, and those who were with him;",40012003,"ChatCompletion(id='chatcmpl-8dJlsZSs8fAcz7hOiVxOqCP2uGSwu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381484, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 2
  total_token_count: 333
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.9, 0.2, 0.8, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.9, 'Insult': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 12:3
348,348,"how he entered into the house of God, and ate the show bread, which was not lawful for him to eat, neither for those who were with him, but only for the priests?",40012004,"ChatCompletion(id='chatcmpl-8dJlskRKaR92g0N8AKf4v17zaPMo9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381484, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=334, total_tokens=335))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 341
  candidates_token_count: 1
  total_token_count: 342
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.7, 0.1, 0.2, 0.5, 0.3, 0.1, 1.0, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.7, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.5, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:4
349,349,"Or have you not read in the law, that on the Sabbath day, the priests in the temple profane the Sabbath, and are guiltless?",40012005,"ChatCompletion(id='chatcmpl-8dJlthYyOPZJexCOKwUHlt5esuopB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381485, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.5, 0.1, 0.3, 0.5, 0.3, 0.1, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:5
350,350,But I tell you that one greater than the temple is here.,40012006,"ChatCompletion(id='chatcmpl-8dJltJf6clqYH7tj6bZQQuAa8cmZS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381485, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:6
351,351,"But if you had known what this means, 'I desire mercy, and not sacrifice,' you would not have condemned the guiltless.",40012007,"ChatCompletion(id='chatcmpl-8dJluy1t8PsSAqLMhiQeupRfXlEJU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381486, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.3, 0.1, 0.2, 0.1, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:7
352,352,"For the Son of Man is Lord of the Sabbath.""",40012008,"ChatCompletion(id='chatcmpl-8dJlvY24h9Inki8dJnBKQaqBaqozP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381487, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=307, total_tokens=308))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 314
  candidates_token_count: 1
  total_token_count: 315
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:8
353,353,"He departed there, and went into their synagogue.",40012009,"ChatCompletion(id='chatcmpl-8dJlvQ6nPVyj3Vci84cdRb6k6ka32', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381487, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=306, total_tokens=307))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 313
  candidates_token_count: 1
  total_token_count: 314
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.5, 0.1, 0.3, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:9
354,354,"And behold there was a man with a withered hand. They asked him, ""Is it lawful to heal on the Sabbath day?"" that they might accuse him.",40012010,"ChatCompletion(id='chatcmpl-8dJlwifI0KOLAHLW8itT9WZI02NzH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381488, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 2
  total_token_count: 337
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.1, 0.9, 0.1, 0.3, 0.5, 0.3, 1.0, 0.2, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.9, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:10
355,355,"He said to them, ""What man is there among you, who has one sheep, and if this one falls into a pit on the Sabbath day, won't he grab on to it, and lift it out?",40012011,"ChatCompletion(id='chatcmpl-8dJlw8Fm4GiQNzq6oGQ0q90uvdKLq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381488, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=340, total_tokens=341))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 348
  candidates_token_count: 2
  total_token_count: 350
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.5, 0.3, 0.2, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.3, 'Legal': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:11
356,356,"Of how much more value then is a man than a sheep! Therefore it is lawful to do good on the Sabbath day.""",40012012,"ChatCompletion(id='chatcmpl-8dJlx4JEl8BL1Uv40NIHq8nOnQwGE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381489, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.6, 0.1, 0.2, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:12
357,357,"Then he told the man, ""Stretch out your hand."" He stretched it out; and it was restored whole, just like the other.",40012013,"ChatCompletion(id='chatcmpl-8dJlxUOuH5KPnc37hBNBYWRoSCb3Z', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381489, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:13
358,358,"But the Pharisees went out, and conspired against him, how they might destroy him.",40012014,"ChatCompletion(id='chatcmpl-8dJlxzaWe2DP3LWOSYkltpHbT4ywC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381489, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.3, 0.5, 0.1, 0.6, 0.1, 0.1, 1.0, 0.1, 0.3, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.5, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.5, 'Legal': 0.1, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.3, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:14
359,359,"Jesus, perceiving that, withdrew from there. Great multitudes followed him; and he healed them all,",40012015,"ChatCompletion(id='chatcmpl-8dJlyIILmoCHUzwlY5hyGMGHdqZib', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381490, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.5, 0.1, 0.2, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:15
360,360,and charged them that they should not make him known:,40012016,"ChatCompletion(id='chatcmpl-8dJly6W6OlnHYFzLdMmumnFuzUz8F', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381490, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=307, total_tokens=308))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 314
  candidates_token_count: 1
  total_token_count: 315
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.3, 0.5, 0.1, 0.3, 0.5, 0.3, 0.1, 0.3, 0.9, 0.2, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.3, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.3, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:16
361,361,"that it might be fulfilled which was spoken through Isaiah the prophet, saying,",40012017,"ChatCompletion(id='chatcmpl-8dJlzjDZzxsdtC9vPp9MFRiNs8KQh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381491, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:17
362,362,"""Behold, my servant whom I have chosen; My beloved in whom my soul is well pleased: I will put my Spirit on him. He will proclaim justice to the Gentiles.",40012018,"ChatCompletion(id='chatcmpl-8dJlzASvUkCWLnZ7sC1K40PNtsyHQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381491, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=334, total_tokens=335))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:18
363,363,"He will not strive, nor shout; Neither will anyone hear his voice in the streets.",40012019,"ChatCompletion(id='chatcmpl-8dJm0yyrYgTuAbtWWJRXK0DVxCS9f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381492, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:19
364,364,"He won't break a bruised reed. He won't quench a smoking flax, Until he leads justice to victory.",40012020,"ChatCompletion(id='chatcmpl-8dJm0HugKmsn0zDLetyFoxZG2OktT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381492, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.3, 0.5, 0.1, 0.3, 0.5, 0.6, 0.1, 0.1, 0.9, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.3, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:20
365,365,"In his name, the Gentiles will hope.""",40012021,"ChatCompletion(id='chatcmpl-8dJm1lszVap85ycbgkqJ85JIqukAf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381493, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=306, total_tokens=307))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 312
  candidates_token_count: 1
  total_token_count: 313
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.4, 0.1, 0.2, 0.3, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:21
366,366,"Then one possessed by a demon, blind and mute, was brought to him and he healed him, so that the blind and mute man both spoke and saw.",40012022,"ChatCompletion(id='chatcmpl-8dJm2j7JaW2Jy2TwOmnu5h3wwjGY8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381494, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.6, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Legal': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:22
367,367,"All the multitudes were amazed, and said, ""Can this be the son of David?""",40012023,"ChatCompletion(id='chatcmpl-8dJm2itC7vTX3tMQvMCzcMHf3v9yJ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381494, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 0.1, 1.0, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:23
368,368,"But when the Pharisees heard it, they said, ""This man does not cast out demons, except by Beelzebul, the prince of the demons.""",40012024,"ChatCompletion(id='chatcmpl-8dJm3pnvjUxREvRCVZBZt2p2mHFum', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381495, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.2, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:24
369,369,"Knowing their thoughts, Jesus said to them, ""Every kingdom divided against itself is brought to desolation, and every city or house divided against itself will not stand.",40012025,"ChatCompletion(id='chatcmpl-8dJm3q2oL1DZxzmGm1o3bKtCHfDH6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381495, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:25
370,370,"If Satan casts out Satan, he is divided against himself. How then will his kingdom stand?",40012026,"ChatCompletion(id='chatcmpl-8dJm4DxoKihLP1M4jxc3BjQxANx9i', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381496, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.1, 0.3, 0.6, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.1, 'Insult': 0.3, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:26
371,371,"If I by Beelzebul cast out demons, by whom do your children cast them out? Therefore they will be your judges.",40012027,"ChatCompletion(id='chatcmpl-8dJm4soE30jWCy4CWP8E6CFq5rfy6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381496, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.5, 0.3, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.4, 'Health': 0.5, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:27
372,372,"But if I by the Spirit of God cast out demons, then the Kingdom of God has come upon you.",40012028,"ChatCompletion(id='chatcmpl-8dJm5PB9X4WgkOoQKsAReEtmjHBMB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381497, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.5, 0.1, 0.3, 0.2, 0.2, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:28
373,373,"Or how can one enter into the house of the strong man, and plunder his goods, unless he first bind the strong man? Then he will plunder his house.",40012029,"ChatCompletion(id='chatcmpl-8dJm6Re47GY8R6bDp8ieNzWYdkrKE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381498, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 2
  total_token_count: 338
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.8, 'severity': 'HIGH', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.2, 0.5, 0.2, 0.2, 0.2, 0.7, 0.7, 0.4, 0.3, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Finance': 0.2, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.7, 'Religion & Belief': 0.7, 'Sexual': 0.4, 'Toxic': 0.3, 'Violent': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:29
374,374,"""He who is not with me is against me, and he who doesn't gather with me, scatters.",40012030,"ChatCompletion(id='chatcmpl-8dJm6Ihy0yMHN8WJmDKLtBwUFUpx7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381498, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.3, 0.6, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:30
375,375,"Therefore I tell you, every sin and blasphemy will be forgiven men, but the blasphemy against the Spirit will not be forgiven men.",40012031,"ChatCompletion(id='chatcmpl-8dJm7k4MeqQxQZx4qMsE618NYXS71', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381499, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.4, 0.1, 0.4, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.3], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.3}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:31
376,376,"Whoever speaks a word against the Son of Man, it will be forgiven him; but whoever speaks against the Holy Spirit, it will not be forgiven him, neither in this age, nor in that which is to come.",40012032,"ChatCompletion(id='chatcmpl-8dJm7scoVXPg1xjoZtd3InnEGHn72', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381499, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=341, total_tokens=342))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 347
  candidates_token_count: 1
  total_token_count: 348
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.4, 0.1, 0.3, 0.2, 0.2, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:32
377,377,"""Either make the tree good, and its fruit good, or make the tree corrupt, and its fruit corrupt; for the tree is known by its fruit.",40012033,"ChatCompletion(id='chatcmpl-8dJm8Dmc8FqqgSGXpXWIB8F9WKXZM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381500, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.6, 0.3, 0.1, 0.2, 0.9, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Insult': 0.3, 'Politics': 0.1, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:33
378,378,"You offspring of vipers, how can you, being evil, speak good things? For out of the abundance of the heart, the mouth speaks.",40012034,"ChatCompletion(id='chatcmpl-8dJm8Mi61j5gii7r4loDQhGwpIL6M', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381500, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.3, 0.4, 0.2, 0.3, 0.1, 1.0, 0.2, 0.3], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.4, 'Politics': 0.2, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:34
379,379,"The good man out of his good treasure brings out good things, and the evil man out of his evil treasure{TR adds ""of the heart""} brings out evil things.",40012035,"ChatCompletion(id='chatcmpl-8dJm9rpvlepiKq22KmNfEKoGcRSu3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381501, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.4, 0.3, 0.2, 0.2, 0.1, 0.9, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:35
380,380,"I tell you that every idle word that men speak, they will give account of it in the day of judgment.",40012036,"ChatCompletion(id='chatcmpl-8dJmAwhmYiXXmf8kgtM3WarEg4xng', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381502, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.3, 0.4, 0.2, 0.3, 0.2, 0.8, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Health': 0.3, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:36
381,381,"For by your words you will be justified, and by your words you will be condemned.""",40012037,"ChatCompletion(id='chatcmpl-8dJmANpF76jS9mBvkSzY5HOPUgV9d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381502, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.1, 0.3, 0.2, 0.2, 0.1, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:37
382,382,"Then certain of the scribes and Pharisees answered, ""Teacher, we want to see a sign from you.""",40012038,"ChatCompletion(id='chatcmpl-8dJmB5ReInK7yQ2ZaI5gudHr3N1QU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381503, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:38
383,383,"But he answered them, ""An evil and adulterous generation seeks after a sign, but no sign will be given it but the sign of Jonah the prophet.",40012039,"ChatCompletion(id='chatcmpl-8dJmBrobQ1YnWOYF9xPoZxM6IC9dp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381503, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 2
  total_token_count: 337
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.4, 0.3, 0.1, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.4, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 12:39
384,384,"For as Jonah was three days and three nights in the belly of the whale, so will the Son of Man be three days and three nights in the heart of the earth.",40012040,"ChatCompletion(id='chatcmpl-8dJmCl3ZjaRYW2ZnrHzdONolQtnM1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381504, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 2
  total_token_count: 340
}
",50,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.4, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 12:40
385,385,"The men of Nineveh will stand up in the judgment with this generation, and will condemn it, for they repented at the preaching of Jonah; and behold, someone greater than Jonah is here.",40012041,"ChatCompletion(id='chatcmpl-8dJmCsDxnPc8ZYNrVgVl2SsO91eXF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381504, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=336, total_tokens=337))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 343
  candidates_token_count: 1
  total_token_count: 344
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.2, 0.3, 0.1, 0.6, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:41
386,386,"The queen of the south will rise up in the judgment with this generation, and will condemn it, for she came from the ends of the earth to hear the wisdom of Solomon; and behold, someone greater than Solomon is here.",40012042,"ChatCompletion(id='chatcmpl-8dJmDkjxaKBjVJb3tstvKL3AiK3gD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381505, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=342, total_tokens=343))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 349
  candidates_token_count: 1
  total_token_count: 350
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.3, 0.3, 0.6, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.3, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:42
387,387,"But the unclean spirit, when he is gone out of the man, passes through waterless places, seeking rest, and doesn't find it.",40012043,"ChatCompletion(id='chatcmpl-8dJmDi35IOOte2YAKVjmmm0bEbfXb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381505, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 2
  total_token_count: 335
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 12:43
388,388,"Then he says, 'I will return into my house from which I came out,' and when he has come back, he finds it empty, swept, and put in order.",40012044,"ChatCompletion(id='chatcmpl-8dJmEeMvboimQFIiWb5wnFjPGbHAt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381506, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 2
  total_token_count: 341
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:44
389,389,"Then he goes, and takes with himself seven other spirits more evil than he is, and they enter in and dwell there. The last state of that man becomes worse than the first. Even so will it be also to this evil generation.""",40012045,"ChatCompletion(id='chatcmpl-8dJmEoX05ImC3vG3sFq4b44QgVt8c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381506, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=344, total_tokens=345))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 351
  candidates_token_count: 1
  total_token_count: 352
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.1, 0.3, 0.1, 0.1, 1.0, 0.1, 0.2], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:45
390,390,"While he was yet speaking to the multitudes, behold, his mother and his brothers stood outside, seeking to speak to him.",40012046,"ChatCompletion(id='chatcmpl-8dJmFIXHSWuTSy3gm01aoKn6ppels', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381507, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 2
  total_token_count: 330
}
",10,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 12:46
391,391,"One said to him, ""Behold, your mother and your brothers stand outside, seeking to speak to you.""",40012047,"ChatCompletion(id='chatcmpl-8dJmG29Mv1PORo4IX5PVaGitiOVxx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381508, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 12:47
392,392,"But he answered him who spoke to him, ""Who is my mother? Who are my brothers?""",40012048,"ChatCompletion(id='chatcmpl-8dJmHpBbdeQMcnIuo9pVLClfs4MN5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381509, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.7, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:48
393,393,"He stretched out his hand towards his disciples, and said, ""Behold, my mother and my brothers!",40012049,"ChatCompletion(id='chatcmpl-8dJmHBluCmMQz36C8phh5zR0oGHFY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381509, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:49
394,394,"For whoever does the will of my Father who is in heaven, he is my brother, and sister, and mother.""",40012050,"ChatCompletion(id='chatcmpl-8dJmIGpKaHgWALKpM9a6jHP6xERza', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381510, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.1, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 12:50
395,395,"On that day Jesus went out of the house, and sat by the seaside.",40013001,"ChatCompletion(id='chatcmpl-8dJmIqWU6G41cmZyt0XBQ1JM7BlxQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381510, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:1
396,396,"Great multitudes gathered to him, so that he entered into a boat, and sat, and all the multitude stood on the beach.",40013002,"ChatCompletion(id='chatcmpl-8dJmJIp04Icr5T098JIxrNkFfzJui', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381511, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:2
397,397,"He spoke to them many things in parables, saying, ""Behold, a farmer went out to sow.",40013003,"ChatCompletion(id='chatcmpl-8dJmJQxgjWjbRlkfGPggygMc0d8kg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381511, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:3
398,398,"As he sowed, some seeds fell by the roadside, and the birds came and devoured them.",40013004,"ChatCompletion(id='chatcmpl-8dJmKj5ejFS9jdzkbQ41KuOqgF2Jg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381512, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.1, 0.5, 0.2, 0.1, 0.2, 0.9, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Legal': 0.1, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:4
399,399,"Others fell on rocky ground, where they didn't have much soil, and immediately they sprang up, because they had no depth of earth.",40013005,"ChatCompletion(id='chatcmpl-8dJmKMYEGkHOiK4i5sELPJBdTqO9p', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381512, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.7, 0.2, 0.1, 0.3, 0.2, 0.2, 0.1, 0.1, 0.8, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.7, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Legal': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:5
400,400,"When the sun had risen, they were scorched. Because they had no root, they withered away.",40013006,"ChatCompletion(id='chatcmpl-8dJmLMbOizuFMzgiRJ0x6ekl1hawQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381513, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 2
  total_token_count: 325
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.6, 0.2, 0.1, 0.8, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 13:6
401,401,Others fell among thorns. The thorns grew up and choked them:,40013007,"ChatCompletion(id='chatcmpl-8dJmM3FzwbCTYoU6gcSwvfpLUCZyn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381514, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.2, 0.1, 0.2, 0.3, 0.1, 0.1, 0.9, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 13:7
402,402,"and others fell on good soil, and yielded fruit: some one hundred times as much, some sixty, and some thirty.",40013008,"ChatCompletion(id='chatcmpl-8dJmMN3VWRzPQ94AJFcerW4w0C73C', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381514, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 0.5, 0.2, 0.1, 0.1, 0.7, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.1, 'Finance': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Legal': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:8
403,403,"He who has ears to hear, let him hear.""",40013009,"ChatCompletion(id='chatcmpl-8dJmN8yYqR7YBqMnMz5cOUUgYNpQL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381515, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=307, total_tokens=308))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 314
  candidates_token_count: 1
  total_token_count: 315
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.2, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:9
404,404,"The disciples came, and said to him, ""Why do you speak to them in parables?""",40013010,"ChatCompletion(id='chatcmpl-8dJmOiJ0Za5pAEyU5u0jzSjaeaCLu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381516, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:10
405,405,"He answered them, ""To you it is given to know the mysteries of the Kingdom of Heaven, but it is not given to them.",40013011,"ChatCompletion(id='chatcmpl-8dJmOGLR3tO7ZlmTVizzRadwaGfdD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381516, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:11
406,406,"For whoever has, to him will be given, and he will have abundance, but whoever doesn't have, from him will be taken away even that which he has.",40013012,"ChatCompletion(id='chatcmpl-8dJmPwZDvvRivvsL5HGEIr23A5GQR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381517, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:12
407,407,"Therefore I speak to them in parables, because seeing they don't see, and hearing, they don't hear, neither do they understand.",40013013,"ChatCompletion(id='chatcmpl-8dJmPateewNvwN2ih7eqWmlSN0q5c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381517, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.5, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:13
408,408,"In them the prophecy of Isaiah is fulfilled, which says,  'By hearing you will hear, And will in no way understand; Seeing you will see, And will in no way perceive:",40013014,"ChatCompletion(id='chatcmpl-8dJmQuCEqbdNLsiKgFFGZllpqM9ap', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381518, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=335, total_tokens=336))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 342
  candidates_token_count: 1
  total_token_count: 343
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:14
409,409,"For this people's heart has grown callous, Their ears are dull of hearing, They have closed their eyes; Or else perhaps they might perceive with their eyes, Hear with their ears, Understand with their heart, And should turn again; And I would heal them.'",40013015,"ChatCompletion(id='chatcmpl-8dJmQST6Z8Na59pmZI5qxDlBi6dl8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381518, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=350, total_tokens=351))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 357
  candidates_token_count: 1
  total_token_count: 358
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.6, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:15
410,410,"""But blessed are your eyes, for they see; and your ears, for they hear.",40013016,"ChatCompletion(id='chatcmpl-8dJmR27hO0IXcrwrHB2du4efTynff', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381519, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:16
411,411,"For most assuredly I tell you that many prophets and righteous men desired to see the things which you see, and didn't see them; and to hear the things which you hear, and didn't hear them.",40013017,"ChatCompletion(id='chatcmpl-8dJmSFXiK1cFmEhUZcKQr4hu0bzUo', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381520, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=339, total_tokens=340))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 347
  candidates_token_count: 1
  total_token_count: 348
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:17
412,412,"""Hear, then, the parable of the farmer.",40013018,"ChatCompletion(id='chatcmpl-8dJmSo8FtnHwe06gN6CSQtfgNDMQi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381520, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=308, total_tokens=309))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 314
  candidates_token_count: 1
  total_token_count: 315
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 0.2, 0.1, 0.9, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:18
413,413,"When anyone hears the word of the Kingdom, and doesn't understand it, the evil one comes, and snatches away that which has been sown in his heart. This is what was sown by the roadside.",40013019,"ChatCompletion(id='chatcmpl-8dJmTf4kQuD7pyL0SXJ3L771FkdIw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381521, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=340, total_tokens=341))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 346
  candidates_token_count: 1
  total_token_count: 347
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.3, 0.3, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:19
414,414,"What was sown on the rocky places, this is he who hears the word, and immediately with joy receives it;",40013020,"ChatCompletion(id='chatcmpl-8dJmTLuI7WvodLoL03RNkEL6mC4Li', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381521, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:20
415,415,"yet he has no root in himself, but endures for a while. When oppression or persecution arises because of the word, immediately he stumbles.",40013021,"ChatCompletion(id='chatcmpl-8dJmUe6qvkiL7Uyggkrp19Q5F1gZh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381522, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 2
  total_token_count: 334
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.2, 0.1, 0.3, 0.5, 0.3, 0.3, 0.8, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.3, 'Public Safety': 0.3, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 13:21
416,416,"What was sown among the thorns, this is he who hears the word, but the cares of this age and the deceitfulness of riches choke the word, and he becomes unfruitful.",40013022,"ChatCompletion(id='chatcmpl-8dJmUeuoH0cahIXP7d3CdGb009rnv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381522, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=336, total_tokens=337))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 341
  candidates_token_count: 1
  total_token_count: 342
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.5, 0.3, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:22
417,417,"What was sown on the good ground, this is he who hears the word, and understands it, who most assuredly bears fruit, and brings forth, some one hundred times as much, some sixty, and some thirty.""",40013023,"ChatCompletion(id='chatcmpl-8dJmV3kxFxu0biBvx8og4VMWXhShd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381523, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=342, total_tokens=343))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 347
  candidates_token_count: 1
  total_token_count: 348
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.4, 0.1, 0.1, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:23
418,418,"He set another parable before them, saying, ""The Kingdom of Heaven is like a man who sowed good seed in his field,",40013024,"ChatCompletion(id='chatcmpl-8dJmWtXNyJSQQ7E1EHEevCfM5E2AC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381524, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:24
419,419,"but while people slept, his enemy came and sowed darnel{darnel is a weed grass (probably bearded darnel or lolium temulentum) that looks very much like wheat until it is mature, when the difference becomes very apparent.} also among the wheat, and went away.",40013025,"ChatCompletion(id='chatcmpl-8dJmWJ9oyV5rZztYJpCJhaDFjRmO8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381524, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=358, total_tokens=359))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 362
  candidates_token_count: 1
  total_token_count: 363
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.2, 0.1, 0.5, 0.1, 0.3, 0.2, 0.2, 0.1, 0.1, 0.9, 0.1, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:25
420,420,"But when the blade sprang up and brought forth fruit, then the darnel appeared also.",40013026,"ChatCompletion(id='chatcmpl-8dJmXa2SWNaBkBvitjc1KtmjOjR1F', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381525, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.3, 0.5, 0.1, 0.4, 0.1, 0.2, 0.5, 0.1, 0.6, 0.2, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.3, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.5, 'Public Safety': 0.1, 'Religion & Belief': 0.6, 'Sexual': 0.2, 'Toxic': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:26
421,421,"The servants of the householder came and said to him, 'Sir, didn't you sow good seed in your field? Where did this darnel come from?'",40013027,"ChatCompletion(id='chatcmpl-8dJmXcs9VRtu9liOsm7KEjKSgBGLo', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381525, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.6, 0.3, 0.1, 0.6, 0.1, 0.3, 0.5, 0.1, 0.8, 0.3, 0.4, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Finance': 0.6, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.6, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.5, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.3, 'Toxic': 0.4, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:27
422,422,"""He said to them, 'An enemy has done this.' ""The servants asked him, 'Do you want us to go and gather them up?'",40013028,"ChatCompletion(id='chatcmpl-8dJmYFhvyI0vaQ29uwcB5IXSUdHxj', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381526, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 0.3, 0.3, 0.3, 0.1, 0.1, 0.9, 0.1, 0.1, 0.1, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.3, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1, 'Violent': 0.1, 'War & Conflict': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:28
423,423,"""But he said, 'No, lest perhaps while you gather up the darnel, you root up the wheat with them.",40013029,"ChatCompletion(id='chatcmpl-8dJmYBYEZgWDiuTqj1UFmDTkVa0cB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381526, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.3, 0.4, 0.1, 0.5, 0.2, 0.3, 0.6, 0.1, 0.8, 0.2, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Finance': 0.3, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.6, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.4, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:29
424,424,"Let both grow together until the harvest, and in the harvest time I will tell the reapers, ""First, gather up the darnel, and bind them in bundles to burn them; but gather the wheat into my barn.""'""",40013030,"ChatCompletion(id='chatcmpl-8dJmZU22OSx0K3VfkDV8Ll3L0OHxj', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381527, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=343, total_tokens=344))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 350
  candidates_token_count: 1
  total_token_count: 351
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.7, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.1, 0.2, 0.1, 0.5, 0.2, 0.2, 0.4, 0.1, 0.9, 0.2, 0.3, 0.5], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.4, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.5}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:30
425,425,"He set another parable before them, saying, ""The Kingdom of Heaven is like a grain of mustard seed, which a man took, and sowed in his field;",40013031,"ChatCompletion(id='chatcmpl-8dJmZzDjufimzFPJ9i6fxDgM0QolC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381527, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 1
  total_token_count: 338
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:31
426,426,"which indeed is smaller than all seeds. But when it is grown, it is greater than the herbs, and becomes a tree, so that the birds of the air come and lodge in its branches.""",40013032,"ChatCompletion(id='chatcmpl-8dJmaHHRSJds2UeE5apqQEHl8Eb2Z', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381528, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=336, total_tokens=337))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 343
  candidates_token_count: 1
  total_token_count: 344
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.6, 0.1, 0.1, 0.2, 0.1, 0.8, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:32
427,427,"He spoke another parable to them. ""The Kingdom of Heaven is like yeast, which a woman took, and hid in three measures{Literally, satas. 3 satas = about 0.5 bushel or 22 litres} of meal, until it was all leavened.""",40013033,"ChatCompletion(id='chatcmpl-8dJma29mbP4jwjvWclX4aSWxTONmE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381528, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=358, total_tokens=359))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 363
  candidates_token_count: 2
  total_token_count: 365
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.6, 0.1, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:33
428,428,"Jesus spoke all these things in parables to the multitudes; and without a parable, he didn't speak to them,",40013034,"ChatCompletion(id='chatcmpl-8dJma2EAFGvKAKQSSAwG3l7YxghNz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381528, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:34
429,429,"that it might be fulfilled which was spoken through the prophet, saying,  ""I will open my mouth in parables; I will utter things hidden from the foundation of the world.""",40013035,"ChatCompletion(id='chatcmpl-8dJmbz2tB3D7qtJiGwYfAB2gL7kAs', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381529, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 340
  candidates_token_count: 1
  total_token_count: 341
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.4, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:35
430,430,"Then Jesus sent the multitudes away, and went into the house. His disciples came to him, saying, ""Explain to us the parable of the darnel of the field.""",40013036,"ChatCompletion(id='chatcmpl-8dJmbJbsoB63IuBT7wXNzV0S2zeYl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381529, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=334, total_tokens=335))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.2, 0.2, 0.5, 0.2, 0.5, 0.1, 0.9, 0.3, 0.4], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Finance': 0.2, 'Health': 0.2, 'Insult': 0.5, 'Politics': 0.2, 'Profanity': 0.5, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.3, 'Toxic': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:36
431,431,"He answered them, ""He who sows the good seed is the Son of Man,",40013037,"ChatCompletion(id='chatcmpl-8dJmc7TWXe4ffgZILLUcsN9ELboh3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381530, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:37
432,432,"the field is the world; and the good seed, these are the children of the Kingdom; and the darnel are the children of the evil one.",40013038,"ChatCompletion(id='chatcmpl-8dJmchHfS3twIfmRWEQpfw4T3XlFF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381530, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.5, 0.1, 0.4, 0.1, 0.3, 0.5, 0.1, 0.8, 0.2, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.5, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:38
433,433,"The enemy who sowed them is the devil. The harvest is the end of the age, and the reapers are angels.",40013039,"ChatCompletion(id='chatcmpl-8dJmdtAELvhFAdA1r9rFuWzhRBrig', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381531, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.4, 0.3, 0.2, 0.9, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.3, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:39
434,434,As therefore the darnel is gathered up and burned with fire; so will it be at the end of this age.,40013040,"ChatCompletion(id='chatcmpl-8dJmdqMMGl9noVdkzkhKr5uz7OclY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381531, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.6, 0.1, 0.5, 0.2, 0.2, 0.4, 0.1, 0.8, 0.2, 0.4, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.4, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.4, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:40
435,435,"The Son of Man will send out his angels, and they will gather out of his Kingdom all things that cause stumbling, and those who do iniquity,",40013041,"ChatCompletion(id='chatcmpl-8dJmeg3h0FeMqoM2mJSF4Z8GQYtXc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381532, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:41
436,436,and will cast them into the furnace of fire. There will be weeping and the gnashing of teeth.,40013042,"ChatCompletion(id='chatcmpl-8dJmefrHh1BSCRlJw6Kg34irGn59G', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381532, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.7, 'severity': 'HIGH', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.4, 0.5, 0.1, 0.4, 0.1, 0.2, 0.1, 1.0, 0.2, 0.3, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.4, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Politics': 0.1, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:42
437,437,"Then the righteous will shine forth like the sun in the Kingdom of their Father. He who has ears to hear, let him hear.",40013043,"ChatCompletion(id='chatcmpl-8dJmftKWYpF3u4tOEKkXnunReHoYY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381533, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:43
438,438,"""Again, the Kingdom of Heaven is like a treasure hidden in the field, which a man found, and hid. In his joy, he goes and sells all that he has, and buys that field.",40013044,"ChatCompletion(id='chatcmpl-8dJmgTA2sW0npdPTbXCClvTl9SBK3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381534, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=338, total_tokens=339))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 345
  candidates_token_count: 2
  total_token_count: 347
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.3, 0.1, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.2, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:44
439,439,"""Again, the Kingdom of Heaven is like a man who is a merchant seeking fine pearls,",40013045,"ChatCompletion(id='chatcmpl-8dJmgfKp8TSiccQUeUgWBBPKbSBGR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381534, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 2
  total_token_count: 324
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.1, 0.1, 0.1, 0.9, 0.3, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.2, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:45
440,440,"who having found one pearl of great price, he went and sold all that he had, and bought it.",40013046,"ChatCompletion(id='chatcmpl-8dJmh86LMD8X8hZFPBnYmO0qYrgU4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381535, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 2
  total_token_count: 327
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.1, 0.1, 0.6, 0.3, 0.1], 'categories': ['Derogatory', 'Finance', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.3, 'Insult': 0.1, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.6, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 13:46
441,441,"""Again, the Kingdom of Heaven is like a dragnet, that was cast into the sea, and gathered some fish of every kind,",40013047,"ChatCompletion(id='chatcmpl-8dJmhXIVaoinzFhxslu97tUUPSE20', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381535, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.3, 0.2, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:47
442,442,"which, when it was filled, they drew up on the beach. They sat down, and gathered the good into containers, but the bad they threw away.",40013048,"ChatCompletion(id='chatcmpl-8dJmiHGK2oM1ZhEPWTT26mRQgslkF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381536, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.2, 0.1, 0.8, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Finance': 0.2, 'Health': 0.4, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:48
443,443,"So will it be in the end of the world. The angels will come forth, and separate the wicked from among the righteous,",40013049,"ChatCompletion(id='chatcmpl-8dJmiV2Ru9WbwTdM3ZnaDQ2Ddq1AX', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381536, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:49
444,444,"and will cast them into the furnace of fire. There will be the weeping and the gnashing of teeth.""",40013050,"ChatCompletion(id='chatcmpl-8dJmjxoSrX8Fh0cf6nZj3fPXB1CzM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381537, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.5, 0.1, 0.2, 0.1, 0.2, 0.1, 0.1, 1.0, 0.2, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:50
445,445,"Jesus said to them, ""Have you understood all these things?"" They answered him, ""Yes, Lord.""",40013051,"ChatCompletion(id='chatcmpl-8dJmkicmAo8CLxMC2im9P3vTmylrF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381538, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:51
446,446,"He said to them, ""Therefore, every scribe who has been made a disciple in the Kingdom of Heaven is like a man who is a householder, who brings out of his treasure new and old things.""",40013052,"ChatCompletion(id='chatcmpl-8dJmkenei29hmdezqrux8SCQvgl74', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381538, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=339, total_tokens=340))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 1
  total_token_count: 345
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:52
447,447,"It happened that when Jesus had finished these parables, he departed from there.",40013053,"ChatCompletion(id='chatcmpl-8dJmlnUTKmfwjMphiT5IHjqeGMOGC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381539, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:53
448,448,"Coming into his own country, he taught them in their synagogue, so that they were astonished, and said, ""Where did this man get this wisdom, and these mighty works?",40013054,"ChatCompletion(id='chatcmpl-8dJmlGpLuM6q2eBXAyMh0Ug1gdnQS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381539, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:54
449,449,"Isn't this the carpenter's son? Isn't his mother called Mary, and his brothers, James, Joses, Simon, and Judas?",40013055,"ChatCompletion(id='chatcmpl-8dJmlpZwLa2kMNg6YoZOR6Pitgwnn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381539, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.1, 0.2, 0.3, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:55
450,450,"Aren't all of his sisters with us? Where then did this man get all of these things?""",40013056,"ChatCompletion(id='chatcmpl-8dJmmeQAqDbbWsm8J3rNCCxp0x1j2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381540, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.2, 0.2, 0.1, 0.7, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:56
451,451,"They were offended by him. But Jesus said to them, ""A prophet is not without honor, except in his own country, and in his own house.""",40013057,"ChatCompletion(id='chatcmpl-8dJmm13W00LtMBIysRBWfEQ7EMh9i', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381540, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.1, 0.2, 0.4, 0.2, 0.6, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:57
452,452,He didn't do many mighty works there because of their unbelief.,40013058,"ChatCompletion(id='chatcmpl-8dJmnAYo4T3m0NKcvADKUGlx10L89', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381541, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 0.4, 0.5, 0.2, 0.3, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 13:58
453,453,"At that time, Herod the tetrarch heard the report concerning Jesus,",40014001,"ChatCompletion(id='chatcmpl-8dJmo3vOYQFB7lwEvPeN8GSaguFDN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381542, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.2, 0.3, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:1
454,454,"and said to his servants, ""This is John the Baptizer. He is risen from the dead. That is why these powers work in him.""",40014002,"ChatCompletion(id='chatcmpl-8dJmoYSlDvqcXhZKmIVTwkfYvrklC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381542, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.3, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:2
455,455,"For Herod had laid hold of John, and bound him, and put him in prison for the sake of Herodias, his brother Philip's wife.",40014003,"ChatCompletion(id='chatcmpl-8dJmoQB2XGK4CHWod35dDNaeGj1EJ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381542, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.2, 0.1, 0.4, 0.5, 0.3, 0.1, 0.5, 0.9, 0.3, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.3, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.5, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.5, 'Religion & Belief': 0.9, 'Sexual': 0.3, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:3
456,456,"For John said to him, ""It is not lawful for you to have her.""",40014004,"ChatCompletion(id='chatcmpl-8dJmpDoUJb6xgvpbazKrUKkEUelDQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381543, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.6, 0.1, 0.3, 0.5, 0.3, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:4
457,457,"When he would have put him to death, he feared the multitude, because they counted him as a prophet.",40014005,"ChatCompletion(id='chatcmpl-8dJmpFFXA41TOThSOwK6a7OEuvJqB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381543, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.9, 0.4, 0.1, 0.3, 0.4, 0.1, 0.3, 0.2, 0.1, 1.0, 0.1, 0.3, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.9, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.3, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:5
458,458,"But when Herod's birthday came, the daughter of Herodias danced among them and pleased Herod.",40014006,"ChatCompletion(id='chatcmpl-8dJmqltPoH7Zu1ysPJ0uiVHpS6anp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381544, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.3, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:6
459,459,Whereupon he promised with an oath to give her whatever she should ask.,40014007,"ChatCompletion(id='chatcmpl-8dJmrFzahmivcljNizd2k5v9jc7gm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381545, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 0.1, 0.2, 0.5, 0.2, 0.1, 0.9, 0.4, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.5, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.4, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:7
460,460,"She, being prompted by her mother, said, ""Give me here on a platter the head of John the Baptizer.""",40014008,"ChatCompletion(id='chatcmpl-8dJmraHEXZT4lftDEXBQj3XDcpPBf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381545, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.3, 0.1, 0.3, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:8
461,461,"The king was grieved, but for the sake of his oaths, and of those who sat at the table with him, he commanded it to be given,",40014009,"ChatCompletion(id='chatcmpl-8dJmsBtky7BhEpRMa7iuB0LM1lI8Q', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381546, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.1, 0.3, 0.2, 0.2, 0.3, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:9
462,462,and he sent and beheaded John in the prison.,40014010,"ChatCompletion(id='chatcmpl-8dJms8X5tBYoP3Y6UBPUDSADZ3zJm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381546, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=307, total_tokens=308))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: MEDIUM
    blocked: true
  }
}
usage_metadata {
  prompt_token_count: 313
  total_token_count: 313
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.7, 'severityScore': 0.8, 'severity': 'HIGH', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.7, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.9, 0.3, 0.1, 0.1, 0.5, 0.2, 0.6, 0.2, 0.5, 0.8, 0.3, 0.5, 0.4, 0.5], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.9, 'Derogatory': 0.3, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.5, 'Religion & Belief': 0.8, 'Sexual': 0.3, 'Toxic': 0.5, 'Violent': 0.4, 'War & Conflict': 0.5}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:10
463,463,"His head was brought on a platter, and given to the young lady: and she brought it to her mother.",40014011,"ChatCompletion(id='chatcmpl-8dJmsuL6Ig7HtV1eFVr5rdVK7E1f1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381546, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.5, 0.3, 0.2, 0.1, 0.2, 0.8, 0.3, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.1, 'Profanity': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.3, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:11
464,464,"His disciples came, and took the body, and buried it; and they went and told Jesus.",40014012,"ChatCompletion(id='chatcmpl-8dJmtTJYjp7KaiKHIhjMpdfFUrTiE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381547, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.7, 0.2, 0.2, 0.3, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.7, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:12
465,465,"Now when Jesus heard this, he withdrew from there in a boat, to a deserted place apart. When the multitudes heard it, they followed him on foot from the cities.",40014013,"ChatCompletion(id='chatcmpl-8dJmuSHx1zTUPBkYARMsv5UHJu2vf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381548, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 2
  total_token_count: 340
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.1, 'Insult': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 14:13
466,466,"Jesus went out, and he saw a great multitude. He had compassion on them, and healed their sick.",40014014,"ChatCompletion(id='chatcmpl-8dJmu6OaP011iVA7xjSRLEEhHXJtQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381548, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.9, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.9, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:14
467,467,"When evening had come, his disciples came to him, saying, ""This place is deserted, and the hour is already late. Send the multitudes away, that they may go into the villages, and buy themselves food.""",40014015,"ChatCompletion(id='chatcmpl-8dJmuPb9V6J8lBRBtxOIi3kxt5owg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381548, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=341, total_tokens=342))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 347
  candidates_token_count: 1
  total_token_count: 348
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.1, 0.5, 0.1, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:15
468,468,"But Jesus said to them, ""They don't need to go away. You give them something to eat.""",40014016,"ChatCompletion(id='chatcmpl-8dJmviGV7EIUXoDQywscsPoERYrUy', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381549, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.9, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.9, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:16
469,469,"They told him, ""We only have here five loaves and two fish.""",40014017,"ChatCompletion(id='chatcmpl-8dJmvocdA8ESOzXwoEJmitSDGnYST', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381549, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.6, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:17
470,470,"He said, ""Bring them here to me.""",40014018,"ChatCompletion(id='chatcmpl-8dJmweLyngoILcHqTLQUoafKLd4kb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381550, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=306, total_tokens=307))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 313
  candidates_token_count: 1
  total_token_count: 314
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.7, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:18
471,471,"He commanded the multitudes to sit down on the grass; and he took the five loaves and the two fish, and looking up to heaven, he blessed, broke and gave the loaves to the disciples, and the disciples gave to the multitudes.",40014019,"ChatCompletion(id='chatcmpl-8dJmxJSUA3iz1Jo11LoG1d3oY1MTL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381551, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=348, total_tokens=349))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 351
  candidates_token_count: 1
  total_token_count: 352
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:19
472,472,"They all ate, and were filled. They took up twelve baskets full of that which remained left over from the broken pieces.",40014020,"ChatCompletion(id='chatcmpl-8dJmxJ8GIzCIUIN37fI5JPmwMiNfK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381551, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.7, 0.3, 0.1, 0.1, 0.8, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.7, 'Insult': 0.3, 'Legal': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:20
473,473,"Those who ate were about five thousand men, besides women and children.",40014021,"ChatCompletion(id='chatcmpl-8dJmyUW9yYuQWlGgXyKre03xe3EFp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381552, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.9, 0.1, 0.3, 0.1, 0.2, 0.1, 0.6, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.9, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.6, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:21
474,474,"Immediately Jesus made the disciples get into the boat, and to go ahead of him to the other side, while he sent the multitudes away.",40014022,"ChatCompletion(id='chatcmpl-8dJmycxfL1lTDUf770P8zUXMUZade', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381552, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:22
475,475,"After he had sent the multitudes away, he went up into the mountain by himself to pray. When evening had come, he was there alone.",40014023,"ChatCompletion(id='chatcmpl-8dJmzmrDHlZyMyt4huLwpIkijbYp4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381553, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 2
  total_token_count: 334
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:23
476,476,"But the boat was now in the middle of the sea, distressed by the waves, for the wind was contrary.",40014024,"ChatCompletion(id='chatcmpl-8dJmzN6fgUkTQA1Gvsew2rVeETWtq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381553, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 2
  total_token_count: 328
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.1, 0.5, 0.1, 0.1, 0.9, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:24
477,477,"In the fourth watch of the night,{The night was equally divided into four watches, so the fourth watch is approximately 3:00 A. M. to sunrise.} Jesus came to them, walking on the sea.",40014025,"ChatCompletion(id='chatcmpl-8dJmzKvX9bHYWK6AsiJ0j7VO7CHQ8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381553, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=341, total_tokens=342))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 348
  candidates_token_count: 2
  total_token_count: 350
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:25
478,478,"When the disciples saw him walking on the sea, they were troubled, saying, ""It's a ghost!"" and they cried out for fear.",40014026,"ChatCompletion(id='chatcmpl-8dJn0vVDoJatToEfCcyIyjM6eT3rO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381554, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 2
  total_token_count: 335
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:26
479,479,"But immediately Jesus spoke to them, saying ""Cheer up! I AM!{see Exodus 3:14.} Don't be afraid.""",40014027,"ChatCompletion(id='chatcmpl-8dJn0WW99KN9F89SBIakTQHU0Whgr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381554, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:27
480,480,"Peter answered him and said, ""Lord, if it is you, command me to come to you on the waters.""",40014028,"ChatCompletion(id='chatcmpl-8dJn1LHeVAc9ExvaCOw6ieAwGJvOW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381555, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 2
  total_token_count: 329
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.3, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 14:28
481,481,"He said, ""Come!"" Peter stepped down from the boat, and walked on the waters to come to Jesus.",40014029,"ChatCompletion(id='chatcmpl-8dJn1czJStKaRbkdPht2fN2l7ndrS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381555, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 2
  total_token_count: 328
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 14:29
482,482,"But when he saw that the wind was strong, he was afraid, and beginning to sink, he cried out, saying, ""Lord, save me!""",40014030,"ChatCompletion(id='chatcmpl-8dJn2UUo9kBeXysMcr6nlhst1lPq3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='20', role='assistant', function_call=None, tool_calls=None))], created=1704381556, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",20,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 2
  total_token_count: 336
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 14:30
483,483,"Immediately Jesus stretched out his hand, took hold of him, and said to him, ""You of little faith, why did you doubt?""",40014031,"ChatCompletion(id='chatcmpl-8dJn2tGbkKZgJRKYZqYeN9M5OfoTx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381556, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.1, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.1, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:31
484,484,"When they got up into the boat, the wind ceased.",40014032,"ChatCompletion(id='chatcmpl-8dJn7fKCNfkvvWdw8crGuRiI8mX7X', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381561, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=308, total_tokens=309))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.1, 0.3, 0.1, 0.1, 0.1, 0.9, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.1, 'Legal': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:32
485,485,"Those who were in the boat came and worshiped him, saying, ""You are truly the Son of God!""",40014033,"ChatCompletion(id='chatcmpl-8dJn7AZ2Df5DqOEiBuVTzG4AfIf18', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381561, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:33
486,486,"When they had crossed over, they came to the land of Gennesaret.",40014034,"ChatCompletion(id='chatcmpl-8dJn8xUWQUwr3zochBCTw4zIunzrs', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381562, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:34
487,487,"When the men of that place recognized him, they sent into all that surrounding region, and brought to him all who were sick,",40014035,"ChatCompletion(id='chatcmpl-8dJn87F6b5BjbdnG0EWWpix74WWSR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381562, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.3, 1.0, 0.1, 0.3, 0.2, 0.1, 0.1, 0.1, 0.9, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.3, 'Health': 1.0, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 14:35
488,488,and they begged him that they might just touch the fringe of his garment. As many as touched it were made whole.,40014036,"ChatCompletion(id='chatcmpl-8dJn9Dv8AJpSjGknjJUk7P4PqQfAs', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381563, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.7, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.6, 0.1, 0.2, 0.6, 0.7, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.6, 'Insult': 0.1, 'Profanity': 0.2, 'Religion & Belief': 0.6, 'Sexual': 0.7, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 14:36
489,489,"Then Pharisees and scribes came to Jesus from Jerusalem, saying,",40015001,"ChatCompletion(id='chatcmpl-8dJn94ZT3PREamXvhkfNNXSkt6ivS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381563, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.3, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.2, 'Insult': 0.3, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:1
490,490,"""Why do your disciples disobey the tradition of the elders? For they don't wash their hands when they eat bread.""",40015002,"ChatCompletion(id='chatcmpl-8dJnAizLsaZ1MfrNnZ8TNv3E9wI7Y', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381564, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.1, 0.6, 0.1, 0.3, 0.2, 0.3, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:2
491,491,"He answered them, ""Why do you also disobey the commandment of God because of your tradition?",40015003,"ChatCompletion(id='chatcmpl-8dJnBTBILFKTW0JVUoiNcLz41G4jH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381565, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.3, 0.1, 0.3, 0.5, 0.2, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:3
492,492,"For God commanded, 'Honor your father and your mother,' and, 'He who speaks evil of father or mother, let him be put to death.'",40015004,"ChatCompletion(id='chatcmpl-8dJnBuWH87UihbhNTobASh6AUA0ai', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381565, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.7, 'severity': 'HIGH', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.9, 0.5, 0.2, 0.6, 0.2, 0.1, 0.3, 0.1, 1.0, 0.2, 0.5, 0.8, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.9, 'Derogatory': 0.5, 'Health': 0.2, 'Insult': 0.6, 'Legal': 0.2, 'Politics': 0.1, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.5, 'Violent': 0.8, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:4
493,493,"But you say, 'Whoever may tell his father or his mother, ""Whatever help you might otherwise have gotten from me is a gift devoted to God,""",40015005,"ChatCompletion(id='chatcmpl-8dJnC9RsMcEiMtXWJax77XW3FpiHq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381566, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.5, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:5
494,494,he shall not honor his father or mother.' You have made the commandment of God void because of your tradition.,40015006,"ChatCompletion(id='chatcmpl-8dJnCuX0nhhr2dqzKQm8tDl28YNvq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381566, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.1, 0.4, 0.1, 0.5, 0.2, 0.3, 0.1, 0.1, 1.0, 0.1, 0.3, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.3, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:6
495,495,"You hypocrites! Well did Isaiah prophesy of you, saying,",40015007,"ChatCompletion(id='chatcmpl-8dJnDEjEFNyjWtCYERt29CNpTk1Iw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381567, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  total_token_count: 318
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.7, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.3, 0.1, 0.7, 0.6, 0.2, 0.1, 1.0, 0.1, 0.5, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.7, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.5, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:7
496,496,"'These people draw near to me with their mouth, And honor me with their lips; But their heart is far from me.",40015008,"ChatCompletion(id='chatcmpl-8dJnDM0uqNk4yEKFXwl858MojswAA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381567, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:8
497,497,"And in vain do they worship me, Teaching as doctrine rules made by men.'""",40015009,"ChatCompletion(id='chatcmpl-8dJnEyaeVzfLXOt4SPgnqEMZubKBO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381568, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.1, 0.1, 0.1, 0.3, 0.2, 0.3, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:9
498,498,"He summoned the multitude, and said to them, ""Hear, and understand.",40015010,"ChatCompletion(id='chatcmpl-8dJnEbqdABb2XrTIpD4TrzKW5f6vd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381568, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:10
499,499,"That which enters into the mouth doesn't defile the man; but that which proceeds out of the mouth, this defiles the man.""",40015011,"ChatCompletion(id='chatcmpl-8dJnFIynxwmgNzsUBFWuqktC78qDp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381569, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.9, 0.1, 0.2, 0.2, 0.2, 0.1, 0.1, 0.9, 0.3, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.9, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.3, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:11
500,500,"Then the disciples came, and said to him, ""Do you know that the Pharisees were offended, when they heard this saying?""",40015012,"ChatCompletion(id='chatcmpl-8dJnFUpkV2t9YXVzuSZifp0j0E71k', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381569, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.4, 0.3, 0.2, 0.6, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Health': 0.4, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:12
501,501,"But he answered, ""Every plant which my heavenly Father didn't plant will be uprooted.",40015013,"ChatCompletion(id='chatcmpl-8dJnG80btIM3Ew1ATQLY6cPIp2xfD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381570, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.3, 0.1, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:13
502,502,"Leave them alone. They are blind guides of the blind. If the blind guide the blind, both will fall into a pit.""",40015014,"ChatCompletion(id='chatcmpl-8dJnGvuz4zmLvTsABU8lu7PSKKd4V', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381570, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.9, 0.1, 0.4, 0.2, 0.1, 0.1, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Health': 0.9, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:14
503,503,"Peter answered him, ""Explain the parable to us.""",40015015,"ChatCompletion(id='chatcmpl-8dJnH7zfCRgAEPDgmCPIpPplVBkqK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381571, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 314
  candidates_token_count: 1
  total_token_count: 315
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 0.9, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:15
504,504,"So Jesus said, ""Do you also still not understand?",40015016,"ChatCompletion(id='chatcmpl-8dJnH599UnJECIzP5srj5FPU5MMWZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381571, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=308, total_tokens=309))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:16
505,505,"Don't you understand that whatever goes into the mouth passes into the belly, and then out of the body?",40015017,"ChatCompletion(id='chatcmpl-8dJnIMHfOgSjHLIUOcrRstGLds2qQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381572, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.9, 0.1, 0.2, 0.1, 0.2, 0.7, 0.3, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.9, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Profanity': 0.2, 'Religion & Belief': 0.7, 'Sexual': 0.3, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:17
506,506,"But the things which proceed out of the mouth come out of the heart, and they defile the man.",40015018,"ChatCompletion(id='chatcmpl-8dJnIjNXYcOU56JTmDq1epoTGcpEn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381572, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.2, 0.6, 0.1, 0.3, 0.2, 0.2, 0.2, 0.1, 0.9, 0.3, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.2, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.3, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:18
507,507,"For out of the heart come forth evil thoughts, murders, adulteries, sexual sins, thefts, false testimony, and blasphemies.",40015019,"ChatCompletion(id='chatcmpl-8dJnJerx4fft8IuonB4IlFwsuGGmT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381573, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.6, 0.1, 0.4, 0.2, 0.4, 0.1, 0.8, 0.5, 0.3], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Profanity': 0.4, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.5, 'Toxic': 0.3}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:19
508,508,"These are the things which defile the man; but to eat with unwashed hands doesn't defile the man.""",40015020,"ChatCompletion(id='chatcmpl-8dJnJYLGORLObnqpUh4hPOIAYUxS9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381573, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.9, 0.1, 0.3, 0.2, 0.2, 0.1, 0.1, 0.7, 0.3, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.9, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.3, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:20
509,509,"Jesus went out from there, and withdrew into the region of Tyre and Sidon.",40015021,"ChatCompletion(id='chatcmpl-8dJnKK3ZCNuFIRHFBb4qjEyFZK09S', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381574, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:21
510,510,"Behold, a Canaanite woman came out from those borders, and cried, saying, ""Have mercy on me, Lord, you son of David! My daughter is severely demonized!""",40015022,"ChatCompletion(id='chatcmpl-8dJnKpcpfEIA1IA9PpN5PODI8FMWS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381574, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=335, total_tokens=336))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 340
  candidates_token_count: 1
  total_token_count: 341
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.6, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.3, 0.1, 0.5, 0.2, 0.6, 0.2, 0.1, 0.9, 0.3, 0.3, 0.1, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.6, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.3, 'Toxic': 0.3, 'Violent': 0.1, 'War & Conflict': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:22
511,511,"But he answered her not a word. His disciples came and begged him, saying, ""Send her away; for she cries after us.""",40015023,"ChatCompletion(id='chatcmpl-8dJnLXFP8LUss4SrAhDCh2RLHLQlF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381575, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:23
512,512,"But he answered, ""I wasn't sent to anyone but the lost sheep of the house of Israel.""",40015024,"ChatCompletion(id='chatcmpl-8dJnLqfM4k3BSThJQZTk7W3In9xTS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381575, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.3, 0.3, 0.6, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.3, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:24
513,513,"But she came and worshiped him, saying, ""Lord, help me.""",40015025,"ChatCompletion(id='chatcmpl-8dJnMDeelrKIB28WNx8bExKz1keyH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381576, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.3, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:25
514,514,"But he answered, ""It is not appropriate to take the children's bread and throw it to the dogs.""",40015026,"ChatCompletion(id='chatcmpl-8dJnM7f8af2R5jGtjYmHV7WLyeHG6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381576, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.6, 0.3, 0.2, 0.3, 0.1, 0.8, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.6, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:26
515,515,"But she said, ""Yes, Lord, but even the dogs eat the crumbs which fall from their masters' table.""",40015027,"ChatCompletion(id='chatcmpl-8dJnNqizyddnakRq8ZcecixtezGrU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381577, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.6, 0.1, 0.3, 0.2, 0.3, 0.1, 0.9, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:27
516,516,"Then Jesus answered her, ""Woman, great is your faith! Be it done to you even as you desire."" And her daughter was healed from that hour.",40015028,"ChatCompletion(id='chatcmpl-8dJnNlxEDOOX0EVeJ5hsVm76Be4ui', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381577, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:28
517,517,"Jesus departed there, and came near to the sea of Galilee; and he went up into the mountain, and sat there.",40015029,"ChatCompletion(id='chatcmpl-8dJnNftqlM0GsdsNsDtKYA6pKEPn7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381577, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:29
518,518,"Great multitudes came to him, having with them the lame, blind, mute, maimed, and many others, and they put them down at his feet. He healed them,",40015030,"ChatCompletion(id='chatcmpl-8dJnOulbD8cqZyLY4V8vzPq1JEphh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381578, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=334, total_tokens=335))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.9, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.9, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:30
519,519,"so that the multitude wondered when they saw the mute speaking, injured whole, lame walking, and blind seeing--and they glorified the God of Israel.",40015031,"ChatCompletion(id='chatcmpl-8dJnOp5Lw5dDetxQ5omeEDrrHCfsm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381578, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 0.6, 0.3, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.6, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:31
520,520,"Jesus summoned his disciples and said, ""I have compassion on the multitude, because they continue with me now three days and have nothing to eat. I don't want to send them away fasting, or they might faint on the way.""",40015032,"ChatCompletion(id='chatcmpl-8dJnPYf4PoCIBj6eXxCkUHG0vWQMO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='20', role='assistant', function_call=None, tool_calls=None))], created=1704381579, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=343, total_tokens=344))",20,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 351
  candidates_token_count: 2
  total_token_count: 353
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.9, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.9, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:32
521,521,"The disciples said to him, ""Where should we get so many loaves in a deserted place as to satisfy so great a multitude?""",40015033,"ChatCompletion(id='chatcmpl-8dJnQLSdf0cValU3DyODTYb66C0bk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381580, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:33
522,522,"Jesus said to them, ""How many loaves do you have?"" They said, ""Seven, and a few small fish.""",40015034,"ChatCompletion(id='chatcmpl-8dJnQvheRVyI3hX0Kxaf2cYjboBtN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381580, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:34
523,523,He commanded the multitude to sit down on the ground;,40015035,"ChatCompletion(id='chatcmpl-8dJnR4mB0X6wzizHezgxmUENqaVOG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381581, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=307, total_tokens=308))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 314
  candidates_token_count: 1
  total_token_count: 315
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:35
524,524,"and he took the seven loaves and the fish. He gave thanks and broke them, and gave to the disciples, and the disciples to the multitudes.",40015036,"ChatCompletion(id='chatcmpl-8dJnRrUCtqfifW1A8tz4eYNGTYdb1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381581, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:36
525,525,"They all ate, and were filled. They took up seven baskets full of the broken pieces that were left over.",40015037,"ChatCompletion(id='chatcmpl-8dJnRC1vNL3nLbMtTwiaCrf7NVV0f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381581, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.9, 0.3, 0.2, 0.2, 0.8, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Health': 0.9, 'Insult': 0.3, 'Legal': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:37
526,526,"Those who ate were four thousand men, besides women and children.",40015038,"ChatCompletion(id='chatcmpl-8dJnStQgjCrTm9hYfHLmW0x8DohJV', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381582, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.9, 0.1, 0.3, 0.1, 0.2, 0.1, 0.6, 0.2, 0.1, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.9, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.6, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:38
527,527,"Then he sent away the multitudes, got into the boat, and came into the borders of Magdala.",40015039,"ChatCompletion(id='chatcmpl-8dJnTYXUV8VUzQkarxRh6AnDqn804', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381583, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.1, 0.3, 0.3, 0.1, 0.3, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 15:39
528,528,"The Pharisees and Sadducees came, and testing him, asked him to show them a sign from heaven.",40016001,"ChatCompletion(id='chatcmpl-8dJnT8MjonLOnpaPIr0wmgP8i8l4A', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381583, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 2
  total_token_count: 327
}
",10,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.4, 0.4, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.4, 'Insult': 0.4, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 16:1
529,529,"But he answered them, ""When it is evening, you say, 'It will be fair weather, for the sky is red.'",40016002,"ChatCompletion(id='chatcmpl-8dJnUzoJiwVPo2Dn1jI3zlwa3szPW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381584, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.6, 0.1, 0.1, 0.8, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.6, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:2
530,530,"In the morning, 'It will be foul weather today, for the sky is red and threatening.' Hypocrites! You know how to discern the appearance of the sky, but you can't discern the signs of the times!",40016003,"ChatCompletion(id='chatcmpl-8dJnU7Ag7qsQhfhmyt8avRNyab9UE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381584, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=342, total_tokens=343))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 349
  candidates_token_count: 1
  total_token_count: 350
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.5, 0.1, 0.6, 0.4, 0.2, 0.1, 0.9, 0.1, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.6, 'Politics': 0.4, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:3
531,531,"An evil and adulterous generation seeks after a sign, and there will be no sign given to it, except the sign of the prophet Jonah."" He left them, and departed.",40016004,"ChatCompletion(id='chatcmpl-8dJnVJC6qsrGrd5RFHqKOHRbOk4b4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381585, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.3, 0.1, 0.4, 0.3, 0.1, 0.3, 1.0, 0.3, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 16:4
532,532,The disciples came to the other side and had forgotten to take bread.,40016005,"ChatCompletion(id='chatcmpl-8dJnVNFt009GjAMuZThykZdA7r7Iz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381585, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:5
533,533,"Jesus said to them, ""Take heed and beware of the yeast of the Pharisees and Sadducees.""",40016006,"ChatCompletion(id='chatcmpl-8dJnVh27XcWXEFp1p2ruk7VrzpUie', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381585, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 2
  total_token_count: 326
}
",10,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.5, 0.4, 0.3, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Health': 0.5, 'Insult': 0.4, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:6
534,534,"They reasoned among themselves, saying, ""We brought no bread.""",40016007,"ChatCompletion(id='chatcmpl-8dJnWx5w9mHwk0fX9xXGWNn597xEr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381586, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.6, 0.2, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.6, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:7
535,535,"Jesus, perceiving it, said, ""Why do you reason among yourselves, you of little faith, 'because you have brought no bread?'",40016008,"ChatCompletion(id='chatcmpl-8dJnW3DxwJtPLeZEaeFMkdUvpP0dn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381586, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.5, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:8
536,536,"Don't you yet perceive, neither remember the five loaves for the five thousand, and how many baskets you took up?",40016009,"ChatCompletion(id='chatcmpl-8dJnXgBaS0ne05SStsKGprhUXQPKT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381587, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.4, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.3, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:9
537,537,"Nor the seven loaves for the four thousand, and how many baskets you took up?",40016010,"ChatCompletion(id='chatcmpl-8dJnXS6vOSEk2qZDmcAOvkQp3hFdg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381587, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.5, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.3, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:10
538,538,"How is it that you don't perceive that I didn't speak to you concerning bread? But beware of the yeast of the Pharisees and Sadducees.""",40016011,"ChatCompletion(id='chatcmpl-8dJnYa2VSQOkj41n4X0OBKvC4mrg3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381588, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.5, 0.1, 0.3, 0.3, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:11
539,539,"Then they understood that he didn't tell them to beware of the yeast of bread, but of the teaching of the Pharisees and Sadducees.",40016012,"ChatCompletion(id='chatcmpl-8dJnYusLWclXZrrIC9n6iSYjkPF7Q', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381588, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.5, 0.3, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.5, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:12
540,540,"Now when Jesus came into the parts of Caesarea Philippi, he asked his disciples, saying, ""Who do men say that I, the Son of Man, am?""",40016013,"ChatCompletion(id='chatcmpl-8dJnZajcptf5qaAiwTykEa5DmTqqR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381589, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:13
541,541,"They said, ""Some say John the Baptizer, some, Elijah, and others, Jeremiah, or one of the prophets.""",40016014,"ChatCompletion(id='chatcmpl-8dJnZJM22UM1LYLqW3W2oq9nlVeej', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381589, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:14
542,542,"He said to them, ""But who do you say that I am?""",40016015,"ChatCompletion(id='chatcmpl-8dJnaOomM0PhEIxfaASrylAIjnaLU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381590, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:15
543,543,"Simon Peter answered, ""You are the Christ, the Son of the living God.""",40016016,"ChatCompletion(id='chatcmpl-8dJna55Hc2Ujbgl8mI3uU2r7I7bR2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381590, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:16
544,544,"Jesus answered him, ""Blessed are you, Simon Bar Jonah, for flesh and blood has not revealed this to you, but my Father who is in heaven.",40016017,"ChatCompletion(id='chatcmpl-8dJnbGha2a8XXFuFrV0HhLTDkL1KA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381591, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:17
545,545,"I also tell you that you are Peter,{Peter's name, Petros in Greek, is the word for a specific rock or stone.} and on this rock{Greek, petra, a rock mass or bedrock.} I will build my assembly, and the gates of Hades will not prevail against it.",40016018,"ChatCompletion(id='chatcmpl-8dJnb0fwqghnWWj6vcqGEoy0w2r5v', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381591, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=361, total_tokens=362))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 364
  candidates_token_count: 1
  total_token_count: 365
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:18
546,546,"I will give to you the keys of the Kingdom of Heaven, and whatever you bind on earth will be bound in heaven; and whatever you release on earth will be released in heaven.""",40016019,"ChatCompletion(id='chatcmpl-8dJncJeNqsjds5aF9neZfhJTM5hQr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381592, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 340
  candidates_token_count: 1
  total_token_count: 341
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.4, 0.1, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:19
547,547,Then he charged the disciples that they should tell no one that he is Jesus the Christ.,40016020,"ChatCompletion(id='chatcmpl-8dJncZS9bjqg6F7vip02E2k65xEqt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381592, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.3, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.2, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:20
548,548,"From that time, Jesus began to show his disciples that he must go to Jerusalem and suffer many things from the elders, chief priests, and scribes, and be killed, and the third day be raised up.",40016021,"ChatCompletion(id='chatcmpl-8dJndCJGYwbqMg6ZueQr7ue0a4In0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381593, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=339, total_tokens=340))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 345
  candidates_token_count: 2
  total_token_count: 347
}
",50,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.4, 0.1, 0.4, 0.1, 0.1, 1.0, 0.2, 0.3, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.4, 'Health': 0.1, 'Insult': 0.4, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 16:21
549,549,"Peter took him aside, and began to rebuke him, saying, ""Far be it from you, Lord! This will never be done to you.""",40016022,"ChatCompletion(id='chatcmpl-8dJndYouLO2qhfqzunhXaMFYq9ICq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381593, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 16:22
550,550,"But he turned, and said to Peter, ""Get behind me, Satan! You are a stumbling-block to me, for you are not setting your mind on the things of God, but on the things of men.""",40016023,"ChatCompletion(id='chatcmpl-8dJneX1Prm5Bwze0Ua0WeIxLsLH1K', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381594, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=340, total_tokens=341))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 348
  candidates_token_count: 1
  total_token_count: 349
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.1, 0.6, 0.2, 0.3, 0.3, 1.0, 0.2, 0.4, 0.1, 0.1], 'categories': ['Derogatory', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.6, 'Politics': 0.2, 'Profanity': 0.3, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.4, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:23
551,551,"Then Jesus said to his disciples, ""If anyone desires to come after me, let him deny himself, and take up his cross, and follow me.",40016024,"ChatCompletion(id='chatcmpl-8dJnewhTgjQkrTuzqp8p3ORTRcnUY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381594, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""80""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 2
  total_token_count: 336
}
",80,"MultiCandidateTextGenerationResponse(text=' 80', _prediction_response=Prediction(predictions=[{'content': ' 80', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.2, 0.1, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 80])",80,Matthew 16:24
552,552,"For whoever desires to save his life will lose it, and whoever will lose his life for my sake will find it.",40016025,"ChatCompletion(id='chatcmpl-8dJnfLEGPBWlUlgk8S6YwGJ4qXPFp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381595, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 2
  total_token_count: 329
}
",50,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.2, 0.1, 0.6, 0.2, 0.2, 0.1, 0.9, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:25
553,553,"For what will it profit a man, if he gains the whole world, and forfeits his life? Or what will a man give in exchange for his life?",40016026,"ChatCompletion(id='chatcmpl-8dJnfiDQ1ZmCiIH1uJv3zVskst2me', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381595, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 2
  total_token_count: 338
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.6, 0.5, 0.1, 0.3, 0.2, 0.1, 0.1, 0.9, 0.2, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.6, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:26
554,554,"For the Son of Man will come in the glory of his Father with his angels, and then he will render to everyone according to his deeds.",40016027,"ChatCompletion(id='chatcmpl-8dJngLRDqlDKkMwR6YsHoVy3s7D6m', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381596, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:27
555,555,"Most assuredly I tell you, there are some standing here who will in no way taste of death, until they see the Son of Man coming in his Kingdom.""",40016028,"ChatCompletion(id='chatcmpl-8dJngc3bwegfpboLqP7VFzWkvpCgW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381596, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.3, 0.1, 0.2, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 16:28
556,556,"After six days, Jesus took with him Peter, James, and John his brother, and brought them up into a high mountain by themselves.",40017001,"ChatCompletion(id='chatcmpl-8dJnhri8p7xsZxdmmEWtLVuEHuPCR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381597, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 2
  total_token_count: 333
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 17:1
557,557,"He was transfigured before them. His face shone like the sun, and his garments became as white as the light.",40017002,"ChatCompletion(id='chatcmpl-8dJnhhpqpd6YZCDlUljHgQVAOAR8f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381597, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:2
558,558,"Behold, Moses and Elijah appeared to them talking with him.",40017003,"ChatCompletion(id='chatcmpl-8dJniD2HVhpMWWRmheq4s8Yd9sNE1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381598, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:3
559,559,"Peter answered, and said to Jesus, ""Lord, it is good for us to be here. If you want, let's make three tents here: one for you, one for Moses, and one for Elijah.""",40017004,"ChatCompletion(id='chatcmpl-8dJniYCwydVxZ3yo7Tt2FIZjLLJdu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381598, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=340, total_tokens=341))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 348
  candidates_token_count: 2
  total_token_count: 350
}
",10,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:4
560,560,"While he was still speaking, behold, a bright cloud overshadowed them. Behold, a voice came out of the cloud, saying, ""This is my beloved Son, in whom I am well pleased. Listen to him.""",40017005,"ChatCompletion(id='chatcmpl-8dJnjlodxdeSZ0rBYYMhVoBVCh7zj', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381599, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=342, total_tokens=343))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 347
  candidates_token_count: 1
  total_token_count: 348
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:5
561,561,"When the disciples heard it, they fell on their faces, and were very afraid.",40017006,"ChatCompletion(id='chatcmpl-8dJnjHiTdV74d5T7igvIAmzNbPUD6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381599, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.3, 0.3, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:6
562,562,"Jesus came and touched them and said, ""Get up, and don't be afraid.""",40017007,"ChatCompletion(id='chatcmpl-8dJnk6agZYnhGdS79NGATTvJ1ICev', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381600, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 2
  total_token_count: 324
}
",10,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:7
563,563,"Lifting up their eyes, they saw no one, except Jesus alone.",40017008,"ChatCompletion(id='chatcmpl-8dJnkJYD8WTHwXZIe5wlYTZPu7x25', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381600, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:8
564,564,"As they were coming down from the mountain, Jesus commanded them, saying, ""Don't tell anyone what you saw, until the Son of Man has risen from the dead.""",40017009,"ChatCompletion(id='chatcmpl-8dJnl8Df5uegG4NhRt7j2QLhpUUGh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381601, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:9
565,565,"His disciples asked him, saying, ""Then why do the scribes say that Elijah must come first?""",40017010,"ChatCompletion(id='chatcmpl-8dJnlku3bEUJkAXMsLINAwzTKNyWt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381601, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.3, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:10
566,566,"Jesus answered them, ""Elijah indeed comes first, and will restore all things,",40017011,"ChatCompletion(id='chatcmpl-8dJnmYFnA4yOJZ45RmwSR3eTRKb3C', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381602, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:11
567,567,"but I tell you that Elijah has come already, and they didn't recognize him, but did to him whatever they wanted to. Even so the Son of Man will also suffer by them.""",40017012,"ChatCompletion(id='chatcmpl-8dJnmLsnw3ye7byeGSNr6DCUgnyyG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381602, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=334, total_tokens=335))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 342
  candidates_token_count: 1
  total_token_count: 343
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.5, 0.1, 0.2, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:12
568,568,Then the disciples understood that he spoke to them of John the Baptizer.,40017013,"ChatCompletion(id='chatcmpl-8dJnnRMGTALNc941mz0KkeMJfVFkU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381603, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:13
569,569,"When they came to the multitude, a man came to him, kneeling down to him, saying,",40017014,"ChatCompletion(id='chatcmpl-8dJnnDB2lz0jVoDoZb4BSwIMCZI8d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381603, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 17:14
570,570,"""Lord, have mercy on my son, for he is epileptic, and suffers grievously; for he often falls into the fire, and often into the water.",40017015,"ChatCompletion(id='chatcmpl-8dJnoYxFkfiUQcTbaNtFQlGLTFrRJ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381604, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 2
  total_token_count: 339
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 1.0, 0.1, 0.2, 0.2, 1.0, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Health': 1.0, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 17:15
571,571,"So I brought him to your disciples, and they could not cure him.""",40017016,"ChatCompletion(id='chatcmpl-8dJnoKPGdNyiNKhgsiQ3rTQLXO3XF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381604, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.9, 0.1, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.9, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:16
572,572,"Jesus answered, ""Faithless and perverse generation! How long will I be with you? How long will I bear with you? Bring him here to me.""",40017017,"ChatCompletion(id='chatcmpl-8dJnoS6yOwWgw0FHeWYlLUlk1MQaq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381604, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.3, 0.5, 0.2, 0.6, 0.1, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:17
573,573,"Jesus rebuked him, the demon went out of him, and the boy was cured from that hour.",40017018,"ChatCompletion(id='chatcmpl-8dJnpXcaBv8M9EGLMg8a0phsej4Tt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381605, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.4, 0.3, 0.1, 0.1, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.4, 'Insult': 0.3, 'Legal': 0.1, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:18
574,574,"Then the disciples came to Jesus privately, and said, ""Why weren't we able to cast it out?""",40017019,"ChatCompletion(id='chatcmpl-8dJnq46t5TH0VLCyXyeZk7rWocexe', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381606, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.1, 0.2, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:19
575,575,"He said to them, ""Because of your unbelief. For most assuredly I tell you, if you have faith as a grain of mustard seed, you will tell this mountain, 'Move from here to there,' and it will move; and nothing will be impossible for you.",40017020,"ChatCompletion(id='chatcmpl-8dJnqU7M78CwbqvSfjofYcdoWhznm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381606, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=353, total_tokens=354))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 358
  candidates_token_count: 1
  total_token_count: 359
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.4, 0.1, 0.2, 0.1, 1.0, 0.1, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.2, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:20
576,576,"But this kind doesn't go out except by prayer and fasting.""",40017021,"ChatCompletion(id='chatcmpl-8dJnrCoJGyJ8YgKHc0N3TcMbHMyCT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381607, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 2
  total_token_count: 319
}
",50,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:21
577,577,"While they were staying in Galilee, Jesus said to them, ""The Son of Man is about to be delivered up into the hands of men,",40017022,"ChatCompletion(id='chatcmpl-8dJnrtNYMepJoJFfuDBeyAEhQ2cin', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381607, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:22
578,578,"and they will kill him, and the third day he will be raised up."" They were exceedingly sorry.",40017023,"ChatCompletion(id='chatcmpl-8dJnsIuC9DGLwJYee3E92yP2kFUCU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381608, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.9, 0.4, 0.3, 0.1, 0.5, 0.5, 0.2, 0.1, 0.3, 1.0, 0.1, 0.5, 0.8, 0.5], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.9, 'Derogatory': 0.4, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.5, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.5, 'Violent': 0.8, 'War & Conflict': 0.5}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:23
579,579,"When they had come to Capernaum, those who collected the didrachma coins{A didrachma is a Greek silver coin worth 2 drachmas, about as much as 2 Roman denarii, or about 2 days wages. It was commonly used to pay the half-shekel temple tax, because 2 drachmas were worth one half shekel of silver.} came to Peter, and said, ""Doesn't your teacher pay the didrachma?""",40017024,"ChatCompletion(id='chatcmpl-8dJnsZmuIyjobJNYzgxYvjH73YIGA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381608, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=400, total_tokens=401))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 403
  candidates_token_count: 1
  total_token_count: 404
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 1.0, 0.3, 0.1, 0.2, 0.1, 0.3, 0.1, 0.9, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 1.0, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:24
580,580,"He said, ""Yes."" When he came into the house, Jesus anticipated him, saying, ""What do you think, Simon? From whom do the kings of the earth receive toll or tribute? From their children, or from strangers?""",40017025,"ChatCompletion(id='chatcmpl-8dJntZ43LivVPJsa6sa50lL5f4PSX', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381609, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=344, total_tokens=345))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 351
  candidates_token_count: 1
  total_token_count: 352
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.2, 0.2, 0.3, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.3, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:25
581,581,"Peter said to him, ""From strangers."" Jesus said to him, ""Therefore the children are exempt.",40017026,"ChatCompletion(id='chatcmpl-8dJntUXO6RxBjaIxMvBStg29MXU0G', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381609, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.1, 0.4, 0.3, 0.5, 0.2, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:26
582,582,"But, lest we cause them to stumble, go to the sea, and cast a hook, and take up the first fish that comes up. When you have opened its mouth, you will find a stater coin.{A stater is a silver coin equivalent to four Attic or two Alexandrian drachmas, or a Jewish shekel: just exactly enough to cover the half-shekel Temple Tax for two people.} Take that, and give it to them for me and you.""",40017027,"ChatCompletion(id='chatcmpl-8dJnu5XC8EfyhFeAxAxvfw6wnevJn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381610, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=396, total_tokens=397))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 399
  candidates_token_count: 1
  total_token_count: 400
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.9, 0.3, 0.1, 0.3, 0.2, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.9, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 17:27
583,583,"In that hour the disciples came to Jesus, saying, ""Who then is greatest in the Kingdom of Heaven?""",40018001,"ChatCompletion(id='chatcmpl-8dJnuqiymhPlWS505oNBm2fGc4746', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381610, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:1
584,584,"Jesus called a little child to himself, and set him in the midst of them,",40018002,"ChatCompletion(id='chatcmpl-8dJnvc99WLlaA6OXY09SsNbZVfTGJ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381611, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:2
585,585,"and said, ""Most assuredly I tell you, unless you turn, and become as little children, you will in no way enter into the Kingdom of Heaven.",40018003,"ChatCompletion(id='chatcmpl-8dJnvD725YkmGRYyNRGTZNxABizpD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381611, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.5, 0.4, 0.2, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.5, 'Insult': 0.4, 'Legal': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:3
586,586,"Whoever therefore humbles himself as this little child, the same is the greatest in the Kingdom of Heaven.",40018004,"ChatCompletion(id='chatcmpl-8dJnwb6QvWXuJKZkkb3mUltmMWFHs', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381612, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:4
587,587,"Whoever receives one such little child in my name receives me,",40018005,"ChatCompletion(id='chatcmpl-8dJnwTpj9TX68D50MN2aIymMcEM0w', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381612, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.2, 0.1, 0.1, 0.4, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Legal': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.4, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:5
588,588,"but whoever causes one of these little ones who believe in me to stumble, it would be better for him that a huge millstone should be hung around his neck, and that he should be sunk in the depths of the sea.",40018006,"ChatCompletion(id='chatcmpl-8dJnxhVk4Z2k3k9mh9f9JbkElMR0o', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381613, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=342, total_tokens=343))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 349
  candidates_token_count: 1
  total_token_count: 350
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.8, 'severity': 'HIGH', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.3, 0.1, 0.2, 0.1, 0.6, 0.1, 0.2, 0.2, 0.3, 0.9, 0.2, 0.5, 0.8, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.6, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.3, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.5, 'Violent': 0.8, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:6
589,589,"""Woe to the world because of occasions of stumbling! For it must be that the occasions come, but woe to that person through whom the occasion comes!",40018007,"ChatCompletion(id='chatcmpl-8dJny7dKRqNy0Zox43c170TuEaiw2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381614, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.5, 0.3, 0.2, 0.8, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.3, 'Profanity': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:7
590,590,"If your hand or your foot causes you to stumble, cut it off, and cast it from you. It is better for you to enter into life maimed or crippled, rather than having two hands or two feet to be cast into the eternal fire.",40018008,"ChatCompletion(id='chatcmpl-8dJnyfmuCHPj3QBk1YIRL8Nkf5BB8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381614, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=348, total_tokens=349))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""70""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 354
  candidates_token_count: 2
  total_token_count: 356
}
",70,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.6, 0.3, 0.2, 0.1, 0.9, 0.2, 0.3, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.3, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 18:8
591,591,"If your eye causes you to stumble, pluck it out, and cast it from you. It is better for you to enter into life with one eye, rather than having two eyes to be cast into the Gehenna{or, Hell} of fire.",40018009,"ChatCompletion(id='chatcmpl-8dJnytK2v9EJMEzgCNWD7a1fDN9Az', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381614, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=348, total_tokens=349))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 354
  candidates_token_count: 2
  total_token_count: 356
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.6, 0.2, 0.2, 1.0, 0.2, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 18:9
592,592,"See that you don't despise one of these little ones, for I tell you that in heaven their angels always see the face of my Father who is in heaven.",40018010,"ChatCompletion(id='chatcmpl-8dJnzbvOdlrOGhGYE5GLoNlO0S39M', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381615, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 1
  total_token_count: 338
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:10
593,593,For the Son of Man came to save that which was lost.,40018011,"ChatCompletion(id='chatcmpl-8dJo0zTZI7A97KvPZfF6TODk05DYe', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381616, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.5, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:11
594,594,"""What do you think? If a man has one hundred sheep, and one of them goes astray, doesn't he leave the ninety-nine, go to the mountains, and seek that which has gone astray?",40018012,"ChatCompletion(id='chatcmpl-8dJo0fgyuwOw4DuNaMGOuqQbJY49M', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381616, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=339, total_tokens=340))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 347
  candidates_token_count: 2
  total_token_count: 349
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 18:12
595,595,"If he finds it, most assuredly I tell you, he rejoices over it more than over the ninety-nine which have not gone astray.",40018013,"ChatCompletion(id='chatcmpl-8dJo06axLLgHS5v47Dell4vJyqgSH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381616, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.1, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.1, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:13
596,596,Even so it is not the will of your Father who is in heaven that one of these little ones should perish.,40018014,"ChatCompletion(id='chatcmpl-8dJo1x1z7yUMIFuaD1SNg1obaRkTY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381617, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.7, 0.3, 0.5, 0.4, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.7, 'Derogatory': 0.3, 'Health': 0.5, 'Insult': 0.4, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:14
597,597,"""If your brother sins against you, go, show him his fault between you and him alone. If he listens to you, you have gained back your brother.",40018015,"ChatCompletion(id='chatcmpl-8dJo1xqqhgQZLgmiye4ggaH9zxtzO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381617, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.4, 0.1, 0.4, 0.2, 0.2, 0.1, 1.0, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:15
598,598,"But if he doesn't listen, take one or two more with you, that at the mouth of two or three witnesses every word may be established.",40018016,"ChatCompletion(id='chatcmpl-8dJo2BwOxWNMBH73HtFjqvuelPHqp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381618, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.2, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:16
599,599,"If he refuses to listen to them, tell it to the assembly. If he refuses to hear the assembly also, let him be to you as a Gentile or a tax collector.",40018017,"ChatCompletion(id='chatcmpl-8dJo2XTUjjDWvcuX5UFKlelFqB5CB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381618, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.6, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.9, 0.3, 0.1, 0.5, 0.5, 0.6, 0.2, 0.1, 0.9, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.6, 'Finance': 0.9, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:17
600,600,"Most assuredly I tell you, whatever things you will bind on earth will be bound in heaven, and whatever things you will release on earth will be released in heaven.",40018018,"ChatCompletion(id='chatcmpl-8dJo3svM627Sd5BqR5QLnTSTeoVIs', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381619, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.2, 0.2, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:18
601,601,"Again, assuredly I tell you, that if two of you will agree on earth concerning anything that they will ask, it will be done for them by my Father who is in heaven.",40018019,"ChatCompletion(id='chatcmpl-8dJo3qHe1YpMqrkmcPemjbZWig2Hd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381619, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=334, total_tokens=335))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 340
  candidates_token_count: 1
  total_token_count: 341
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.1, 0.2, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:19
602,602,"For where two or three are gathered together in my name, there I am in the midst of them.""",40018020,"ChatCompletion(id='chatcmpl-8dJo4tzjjeL6OizjnWCIDmYkMPHXS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381620, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:20
603,603,"Then Peter came and said to him, ""Lord, how often shall my brother sin against me, and I forgive him? Until seven times?""",40018021,"ChatCompletion(id='chatcmpl-8dJo5mMcrVygl5EMTK0oYUCIcu9Ml', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381621, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.3, 0.1, 0.3, 0.2, 0.1, 0.2, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.1, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:21
604,604,"Jesus said to him, ""I don't tell you until seven times, but, until seventy times seven.",40018022,"ChatCompletion(id='chatcmpl-8dJo54wT8NmPu82ouxJKZAq7uP0Go', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381621, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:22
605,605,"Therefore the Kingdom of Heaven is like a certain king, who wanted to reconcile accounts with his servants.",40018023,"ChatCompletion(id='chatcmpl-8dJo5KNd5aO9SnL3zPUwtm6td3I6Z', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381621, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.2, 0.3, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:23
606,606,"When he had begun to reconcile, one was brought to him who owed him ten thousand talents.{Ten thousand talents represents an extremely large sum of money, equivalent to about 60,000,000 denarii, where one denarius was typical of one day's wages for agricultural labor.}",40018024,"ChatCompletion(id='chatcmpl-8dJo6rzFDxbFSA7loJMZiYRsirDdh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381622, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=355, total_tokens=356))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 366
  candidates_token_count: 1
  total_token_count: 367
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.8, 0.1, 0.2, 0.2, 0.3, 0.1, 0.8, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.8, 'Health': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:24
607,607,"But because he couldn't pay, his lord commanded him to be sold, with his wife, his children, and all that he had, and payment to be made.",40018025,"ChatCompletion(id='chatcmpl-8dJo6qfl5X2pM6Y4trxaiVyQRAInu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381622, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.7, 0.2, 0.1, 0.3, 0.5, 0.3, 0.1, 0.1, 1.0, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.7, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:25
608,608,"The servant therefore fell down and kneeled before him, saying, 'Lord, have patience with me, and I will repay you all!'",40018026,"ChatCompletion(id='chatcmpl-8dJo7QnbA58HB0IBsnI4tho9xUUdo', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381623, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:26
609,609,"The lord of that servant, being moved with compassion, released him, and forgave him the debt.",40018027,"ChatCompletion(id='chatcmpl-8dJo80u7XxEqZ1R8u6LrboVgl0UAF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381624, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 1.0, 0.2, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Finance': 1.0, 'Health': 0.2, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:27
610,610,"""But that servant went out, and found one of his fellow servants, who owed him one hundred denarii,{100 denarii was about one sixtieth of a talent.} and he grabbed him, and took him by the throat, saying, 'Pay me what you owe!'",40018028,"ChatCompletion(id='chatcmpl-8dJo8Ww4MuW6UOX0fCYaSAqadGJg3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381624, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=355, total_tokens=356))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 361
  candidates_token_count: 1
  total_token_count: 362
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.7, 0.2, 0.1, 0.3, 0.2, 0.2, 0.1, 0.8, 0.3, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.7, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.3, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:28
611,611,"""So his fellow servant fell down at his feet and begged him, saying, 'Have patience with me, and I will repay you!'",40018029,"ChatCompletion(id='chatcmpl-8dJo847ma7eyhqhPF2TcfdgepWUMk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381624, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.2, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.3, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 18:29
612,612,"He would not, but went and cast him into prison, until he should pay back that which was due.",40018030,"ChatCompletion(id='chatcmpl-8dJo9CbnoKCZKs51M6Qj3Yiv7Ceyh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381625, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.2, 0.3, 0.1, 0.6, 0.4, 0.9, 0.4, 0.2, 0.8, 0.8, 0.2, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.2, 'Finance': 0.3, 'Health': 0.1, 'Illicit Drugs': 0.6, 'Insult': 0.4, 'Legal': 0.9, 'Politics': 0.4, 'Profanity': 0.2, 'Public Safety': 0.8, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:30
613,613,"So when his fellow servants saw what was done, they were exceedingly sorry, and came and told to their lord all that was done.",40018031,"ChatCompletion(id='chatcmpl-8dJo98dBwoIgzjUu0Xz4mQTvDmjBL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381625, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.4, 0.3, 0.2, 0.3, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:31
614,614,"Then his lord called him in, and said to him, 'You wicked servant! I forgave you all that debt, because you begged me.",40018032,"ChatCompletion(id='chatcmpl-8dJoAG10Y7Jw91YrLH3R2hFRc3918', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381626, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  total_token_count: 333
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.6, 0.2, 0.6, 0.1, 0.3, 0.3, 0.1, 0.9, 0.2, 0.4], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.6, 'Health': 0.2, 'Insult': 0.6, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:32
615,615,"Shouldn't you also have had mercy on your fellow servant, even as I had mercy on you?'",40018033,"ChatCompletion(id='chatcmpl-8dJoBrIoZqgAoH6e12mqNQZmwEoM8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381627, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.2, 0.1, 0.2, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.2, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:33
616,616,"His lord was angry, and delivered him to the tormentors, until he should pay all that was due to him.",40018034,"ChatCompletion(id='chatcmpl-8dJoBxKiF0X0LB1NefDeDjyze8pAg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381627, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 0.1, 0.5, 0.5, 0.3, 0.2, 0.1, 1.0, 0.2, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Finance': 0.2, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.5, 'Politics': 0.3, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:34
617,617,"So my heavenly Father will also do to you, if you don't each forgive your brother from your hearts for his misdeeds.""",40018035,"ChatCompletion(id='chatcmpl-8dJoBgT5digeDRcXO7pTSD2fe1euE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381627, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.3, 0.1, 0.2, 0.2, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 18:35
618,618,"It happened when Jesus had finished these words, he departed from Galilee, and came into the borders of Judea beyond the Jordan.",40019001,"ChatCompletion(id='chatcmpl-8dJoCuzFC13T4TV9yLWkWWYpPaqHw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381628, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:1
619,619,"Great multitudes followed him, and he healed them there.",40019002,"ChatCompletion(id='chatcmpl-8dJoC9xKNaOli2nGhlARVz9lAHFgz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381628, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=308, total_tokens=309))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 314
  candidates_token_count: 1
  total_token_count: 315
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.7, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Health': 0.7, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:2
620,620,"Pharisees came to him, testing him, and saying, ""Is it lawful for a man to divorce his wife for any reason?""",40019003,"ChatCompletion(id='chatcmpl-8dJoDCBrUIovYA1aKuqf9e8UxvdB6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381629, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.5, 0.3, 0.5, 0.2, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Health': 0.5, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:3
621,621,"He answered, ""Haven't you read that he who made them from the beginning made them male and female,",40019004,"ChatCompletion(id='chatcmpl-8dJoEZ1wqNUjoxlOTkXLt5AauRhDH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381630, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.5, 0.3, 0.2, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.5, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:4
622,622,"and said, 'For this cause a man shall leave his father and mother, and shall join to his wife; and the two shall become one flesh?'",40019005,"ChatCompletion(id='chatcmpl-8dJoE7n7va9f0YnIPgRuHRSD5BDjk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381630, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:5
623,623,"So that they are no more two, but one flesh. What therefore God has joined together, don't let man tear apart.""",40019006,"ChatCompletion(id='chatcmpl-8dJoELRKV17cihxw5wMb0YM3X2Lqj', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381630, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.4, 0.1, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:6
624,624,"They asked him, ""Why then did Moses command us to give her a bill of divorce, and divorce her?""",40019007,"ChatCompletion(id='chatcmpl-8dJoFSodXmwGV8hk7D7HGNUH4DJ8O', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381631, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.2, 0.3, 0.9, 0.3, 0.9, 0.3, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.3, 'Legal': 0.9, 'Politics': 0.3, 'Religion & Belief': 0.9, 'Sexual': 0.3, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:7
625,625,"He said to them, ""Moses, because of the hardness of your hearts, allowed you to divorce your wives, but from the beginning it has not been so.",40019008,"ChatCompletion(id='chatcmpl-8dJoFhxeRDQHB9uxVfccxzUeZeKB6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381631, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.5, 0.1, 0.3, 0.5, 0.6, 0.1, 1.0, 0.4, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.4, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:8
626,626,"I tell you that whoever divorces his wife, except for sexual immorality, and marries another, commits adultery; and he who marries her when she is divorced commits adultery.""",40019009,"ChatCompletion(id='chatcmpl-8dJoGICt2eSLU7ZrTPxK7L7vJDuYZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381632, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=334, total_tokens=335))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  total_token_count: 337
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.6, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.6, 0.1, 0.4, 0.5, 0.2, 0.2, 0.1, 0.9, 0.6, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.5, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.6, 'Toxic': 0.4, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:9
627,627,"His disciples said to him, ""If this is the case of the man with his wife, it is not expedient to marry.""",40019010,"ChatCompletion(id='chatcmpl-8dJoGyNBbDA9I8XnkS2gQMpucLGcV', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381632, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.6, 0.2, 0.2, 0.2, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.6, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:10
628,628,"But he said to them, ""Not all men can receive this saying, but those to whom it is given.",40019011,"ChatCompletion(id='chatcmpl-8dJoHL1a934VHtjmKd4guxBKDKFoe', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381633, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.5, 0.6, 0.3, 0.2, 0.3, 0.1, 0.9, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.5, 'Health': 0.6, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:11
629,629,"For there are eunuchs who were born that way from their mother's womb, and there are eunuchs who were made eunuchs by men; and there are eunuchs who made themselves eunuchs for the Kingdom of Heaven's sake. He who is able to receive it, let him receive it.""",40019012,"ChatCompletion(id='chatcmpl-8dJoHzPWJusaix5Fb9QvTeQJKhJqw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381633, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=361, total_tokens=362))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 365
  candidates_token_count: 1
  total_token_count: 366
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.5, 0.7, 0.1, 0.4, 0.2, 0.2, 0.2, 1.0, 0.4, 0.3], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.5, 'Health': 0.7, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.4, 'Toxic': 0.3}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:12
630,630,"Then little children were brought to him, that he should lay his hands on them and pray; and the disciples rebuked them.",40019013,"ChatCompletion(id='chatcmpl-8dJoIjNv8GBtv14wso5mVngukIl8x', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381634, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:13
631,631,"But Jesus said, ""Allow the little children, and don't forbid them to come to me; for to such belongs the Kingdom of Heaven.""",40019014,"ChatCompletion(id='chatcmpl-8dJoJZjrRqcOdCi3SgXmjH8ZcPbM3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381635, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.2, 'Legal': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:14
632,632,"He laid his hands on them, and departed from there.",40019015,"ChatCompletion(id='chatcmpl-8dJoJB9RwRPABifPCDLHQXFZghydh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381635, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=308, total_tokens=309))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.5, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:15
633,633,"Behold, one came to him and said, ""Good teacher, what good thing shall I do, that I may have eternal life?""",40019016,"ChatCompletion(id='chatcmpl-8dJoKvb3e3WzwpowAiVpRJHY30QAz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381636, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:16
634,634,"He said to him, ""Why do you call me good? No one is good but one, that is, God. But if you want to enter into life, keep the commandments.""",40019017,"ChatCompletion(id='chatcmpl-8dJoLJ66hD1jgQ2fOtwqfEwtR7oei', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381637, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=335, total_tokens=336))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 341
  candidates_token_count: 1
  total_token_count: 342
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:17
635,635,"He said to him, ""Which ones?"" Jesus said, ""'You shall not murder.' 'You shall not commit adultery.' 'You shall not steal.' 'You shall not offer false testimony.'",40019018,"ChatCompletion(id='chatcmpl-8dJoLXxny8JfYUea4cNozEPLAnq4y', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381637, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=335, total_tokens=336))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 342
  candidates_token_count: 1
  total_token_count: 343
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.2, 0.1, 0.2, 0.1, 1.0, 0.3, 0.2], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:18
636,636,"'Honor your father and mother.' And, 'You shall love your neighbor as yourself.'""",40019019,"ChatCompletion(id='chatcmpl-8dJoMnXb06tys8wQjrzz3Jx9yqpTL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381638, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.2, 0.8, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.1, 'Insult': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:19
637,637,"The young man said to him, ""All these things I have observed from my youth. What do I still lack?""",40019020,"ChatCompletion(id='chatcmpl-8dJoMuPilJJ3GoiVbz7iihvVNEfMK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381638, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.6, 0.1, 0.1, 0.7, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.6, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:20
638,638,"Jesus said to him, ""If you want to be perfect, go, sell what you have, and give to the poor, and you will have treasure in heaven; and come, follow me.""",40019021,"ChatCompletion(id='chatcmpl-8dJoNwrD8lGE5gPDxrL20H6E3G1t1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381639, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=336, total_tokens=337))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 343
  candidates_token_count: 2
  total_token_count: 345
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.2, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.2, 'Health': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 19:21
639,639,"But when the young man heard the saying, he went away sad, for he was one who had great possessions.",40019022,"ChatCompletion(id='chatcmpl-8dJoNNTMnldKHkKoFDgLTeafjeerT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381639, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.1, 0.2, 0.2, 0.1, 0.8, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:22
640,640,"Jesus said to his disciples, ""Most assuredly I say to you, a rich man will enter into the Kingdom of Heaven with difficulty.",40019023,"ChatCompletion(id='chatcmpl-8dJoOdbBleFM8YyRL1sOiKbPinrWW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='50', role='assistant', function_call=None, tool_calls=None))], created=1704381640, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",50,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 2
  total_token_count: 332
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.1, 0.3, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.1, 'Insult': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 19:23
641,641,"Again I tell you, it is easier for a camel to go through a needle's eye, than for a rich man to enter into the Kingdom of God.""",40019024,"ChatCompletion(id='chatcmpl-8dJoOsCdSekVcRilBkeOyUZ6KWmWH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381640, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 2
  total_token_count: 338
}
",50,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.4, 0.1, 0.2, 0.1, 0.3, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.3, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:24
642,642,"When the disciples heard it, they were exceedingly astonished, saying, ""Who then can be saved?""",40019025,"ChatCompletion(id='chatcmpl-8dJoP5iDXr8468lOOenooqK46FzIR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381641, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.3, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:25
643,643,"Looking at them, Jesus said, ""With men this is impossible, but with God all things are possible.""",40019026,"ChatCompletion(id='chatcmpl-8dJoPprp7TG03t0oQ2n3hfk95d9tr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381641, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.5, 0.2, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:26
644,644,"Then Peter answered, ""Behold, we have left everything, and followed you. What then will we have?""",40019027,"ChatCompletion(id='chatcmpl-8dJoQ0wt7p1srezaK684GQFUwtGqL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381642, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 2
  total_token_count: 327
}
",10,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:27
645,645,"Jesus said to them, ""Most assuredly I tell you that you who have followed me, in the regeneration when the Son of Man will sit on the throne of his glory, you also will sit on twelve thrones, judging the twelve tribes of Israel.",40019028,"ChatCompletion(id='chatcmpl-8dJoQXlMvyNBNQB3kS5HigvU4c1af', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381642, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=348, total_tokens=349))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 353
  candidates_token_count: 1
  total_token_count: 354
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.3, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:28
646,646,"Everyone who has left houses, or brothers, or sisters, or father, or mother, or wife, or children, or lands, for my name's sake, will receive one hundred times, and will inherit eternal life.",40019029,"ChatCompletion(id='chatcmpl-8dJoRCLOf4hlBhxwyD6xC2x9zuyOk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381643, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=341, total_tokens=342))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 349
  candidates_token_count: 2
  total_token_count: 351
}
",50,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:29
647,647,But many will be last who are first; and first who are last.,40019030,"ChatCompletion(id='chatcmpl-8dJoRylO2HxT1teVDUgjntzxfNm7e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381643, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 0.8, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 19:30
648,648,"""For the Kingdom of Heaven is like a man who was the master of a household, who went out early in the morning to hire laborers for his vineyard.",40020001,"ChatCompletion(id='chatcmpl-8dJoSMNx5xqJyYpPKhMvFJQQmMTkG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381644, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 2
  total_token_count: 337
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 0.2, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Legal': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:1
649,649,"When he had agreed with the laborers for a denarius{A denarius is a silver Roman coin worth 1/25th of a Roman aureus. This was a common wage for a day of farm labor.} a day, he sent them into his vineyard.",40020002,"ChatCompletion(id='chatcmpl-8dJoSgtgjePtKYl78tYstTwJT40Tl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381644, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=353, total_tokens=354))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 357
  candidates_token_count: 1
  total_token_count: 358
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.7, 0.2, 0.1, 0.2, 0.2, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.7, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:2
650,650,"He went out about the third hour,{Time was measured from sunrise to sunset, so the third hour would be about 9:00 AM.} and saw others standing idle in the marketplace.",40020003,"ChatCompletion(id='chatcmpl-8dJoTPkqPeA7zZj7h0QyT0gVFL82t', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381645, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=335, total_tokens=336))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 342
  candidates_token_count: 1
  total_token_count: 343
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.5, 0.2, 0.2, 0.1, 0.1, 0.9, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:3
651,651,"To them he said, 'You also go into the vineyard, and whatever is right I will give you.' So they went their way.",40020004,"ChatCompletion(id='chatcmpl-8dJoUfJvoVbezfH3RvciklJ7hSXq8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381646, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:4
652,652,"Again he went out about the sixth and the ninth hour,{noon and 3:00 P. M.} and did likewise.",40020005,"ChatCompletion(id='chatcmpl-8dJoUd3WnpHnBCnghnJJtBd6Cv2Tb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381646, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.5, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:5
653,653,"About the eleventh hour{5:00 PM} he went out, and found others standing idle. He said to them, 'Why do you stand here all day idle?'",40020006,"ChatCompletion(id='chatcmpl-8dJoUH1pv5VecA85J5hpv1tO152RC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381646, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 2
  total_token_count: 341
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.3, 0.1, 0.8, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:6
654,654,"""They said to him, 'Because no one has hired us.' ""He said to them, 'You also go into the vineyard, and you will receive whatever is right.'",40020007,"ChatCompletion(id='chatcmpl-8dJoVzNpfTgRlLO3croRA6DTTRcWK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381647, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.4, 0.1, 0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:7
655,655,"When evening had come, the lord of the vineyard said to his steward, 'Call the laborers and pay them their wages, beginning from the last to the first.'",40020008,"ChatCompletion(id='chatcmpl-8dJoV10M81cWuuhDjstgYTHjQLxAd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381647, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.1, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.6, 'Health': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:8
656,656,"""When those who were hired at about the eleventh hour came, they each received a denarius.",40020009,"ChatCompletion(id='chatcmpl-8dJoW9jPktCAlqVYkyPNB64AtrWYK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381648, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.2, 0.1, 0.2, 0.2, 0.2, 0.1, 0.1, 0.9, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.6, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:9
657,657,"When the first came, they supposed that they would receive more; and they likewise each received a denarius.",40020010,"ChatCompletion(id='chatcmpl-8dJoWrN1pMzhe3yyd15kN1b0z94EB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381648, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.6, 0.2, 0.1, 0.2, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.6, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:10
658,658,"When they received it, they murmured against the master of the household,",40020011,"ChatCompletion(id='chatcmpl-8dJoXoN3SXZ7GqL8WV55qBVbpaj7X', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381649, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.3, 0.1, 0.3, 0.2, 0.3, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:11
659,659,"saying, 'These last have spent one hour, and you have made them equal to us, who have borne the burden of the day and the scorching heat!'",40020012,"ChatCompletion(id='chatcmpl-8dJoXE7oyvRysWFMiVjkSBnf5gq7T', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381649, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.5, 0.2, 0.1, 0.7, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:12
660,660,"""But he answered one of them, 'Friend, I am doing you no wrong. Didn't you agree with me for a denarius?",40020013,"ChatCompletion(id='chatcmpl-8dJoY2RRKaBKVKm94GqscZz3poAEG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381650, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.5, 0.2, 0.1, 0.3, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.3, 'Health': 0.5, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:13
661,661,"Take that which is yours, and go your way. It is my desire to give to this last just as much as to you.",40020014,"ChatCompletion(id='chatcmpl-8dJoYhqKfbUA4BvYVGFTQ81oBF0OH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381650, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.2, 0.2, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:14
662,662,"Isn't it lawful for me to do what I want to with what I own? Or is your eye evil, because I am good?'",40020015,"ChatCompletion(id='chatcmpl-8dJoZKKGgvzqnCu9mN36zeTwWyARu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381651, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.7, 0.1, 0.4, 0.2, 0.2, 0.2, 0.8, 0.2, 0.2], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.7, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:15
663,663,"So the last will be first, and the first last. For many are called, but few are chosen.""",40020016,"ChatCompletion(id='chatcmpl-8dJoZyi4pLDWj6bnmZvBjqc1GLKAN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381651, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.1, 0.1, 0.9, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:16
664,664,"As Jesus was going up to Jerusalem, he took the twelve disciples aside, and on the way he said to them,",40020017,"ChatCompletion(id='chatcmpl-8dJoaeEsNVC1B0NEzIrGoYYMMFs54', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='20', role='assistant', function_call=None, tool_calls=None))], created=1704381652, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",20,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 20:17
665,665,"""Behold, we are going up to Jerusalem, and the Son of Man will be delivered to the chief priests and scribes, and they will condemn him to death,",40020018,"ChatCompletion(id='chatcmpl-8dJob0forITzVKMjxKi5ovfBuhBPP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381653, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 2
  total_token_count: 338
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.4, 0.2, 0.1, 0.4, 0.1, 0.2, 0.2, 0.1, 1.0, 0.2, 0.4, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.4, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.4, 'Violent': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:18
666,666,"and will hand him over to the Gentiles to mock, to scourge, and to crucify; and the third day he will be raised up.""",40020019,"ChatCompletion(id='chatcmpl-8dJobX3ekLd8LmtzJ1E5TTnDwZObt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381653, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 2
  total_token_count: 334
}
",50,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.7, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.1, 0.4, 0.1, 0.5, 0.2, 0.3, 1.0, 0.2, 0.3, 0.4, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.5, 'Health': 0.1, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.5, 'Profanity': 0.2, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.4, 'War & Conflict': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 20:19
667,667,"Then the mother of the sons of Zebedee came to him with her sons, kneeling and asking a certain thing of him.",40020020,"ChatCompletion(id='chatcmpl-8dJociHDGYbFdROp2ntco5oqr3Bt1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381654, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:20
668,668,"He said to her, ""What do you want?"" She said to him, ""Command that these, my two sons, may sit, one on your right hand, and one on your left hand, in your Kingdom.""",40020021,"ChatCompletion(id='chatcmpl-8dJocqd8o0m6dl533oV5J1YHXonv0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381654, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=341, total_tokens=342))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 348
  candidates_token_count: 1
  total_token_count: 349
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:21
669,669,"But Jesus answered, ""You don't know what you are asking. Are you able to drink the cup that I am about to drink, and be baptized with the baptism that I am baptized with?"" They said to him, ""We are able.""",40020022,"ChatCompletion(id='chatcmpl-8dJodyAU3TErT2cMwZuS8ah3Rwd7S', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381655, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=346, total_tokens=347))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 354
  candidates_token_count: 2
  total_token_count: 356
}
",20,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 20:22
670,670,"He said to them, ""You will indeed drink my cup, and be baptized with the baptism that I am baptized with, but to sit on my right hand and on my left hand is not mine to give; but it is for whom it has been prepared by my Father.""",40020023,"ChatCompletion(id='chatcmpl-8dJodG2kMOXM89ksEabtbU40tyfEF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381655, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=352, total_tokens=353))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 359
  candidates_token_count: 1
  total_token_count: 360
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:23
671,671,"When the ten heard it, they were indignant with the two brothers.",40020024,"ChatCompletion(id='chatcmpl-8dJoegpYvBpxAq2GliDc7iXtfzdHT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381656, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 0.3, 0.4, 0.5, 0.3, 0.2, 0.9, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.4, 'Legal': 0.5, 'Politics': 0.3, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:24
672,672,"But Jesus summoned them, and said, ""You know that the rulers of the Gentiles lord it over them, and their great ones exercise authority over them.",40020025,"ChatCompletion(id='chatcmpl-8dJoeG5FWOE7l1q9E3w2YpydrXuiq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381656, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.1, 0.3, 0.2, 0.2, 0.6, 0.1, 1.0, 0.2, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:25
673,673,"It shall not be so among you, but whoever desires to become great among you shall be{TR reads ""let him be"" instead of ""shall be""} your servant.",40020026,"ChatCompletion(id='chatcmpl-8dJofJXqqrsJYH0jPST32tiJArzYi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381657, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:26
674,674,"Whoever desires to be first among you shall be your bondservant,",40020027,"ChatCompletion(id='chatcmpl-8dJofCnhFmWwdtZEf253PxyiYUzYm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381657, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:27
675,675,"even as the Son of Man came not to be served, but to serve, and to give his life as a ransom for many.""",40020028,"ChatCompletion(id='chatcmpl-8dJogdlMTNLxsxRNeGAHnMJAGfaVm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381658, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.4, 0.1, 0.2, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:28
676,676,"As they went out from Jericho, a great multitude followed him.",40020029,"ChatCompletion(id='chatcmpl-8dJogCGRr4MSD06tnUuE9D7BQHT72', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381658, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:29
677,677,"Behold, two blind men sitting by the road, when they heard that Jesus was passing by, cried out, ""Lord, have mercy on us, you son of David!""",40020030,"ChatCompletion(id='chatcmpl-8dJohmDPR9m3ukNPMD3oLBbcTabhY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381659, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:30
678,678,"The multitude rebuked them, telling them that they should be quiet, but they cried out even more, ""Lord, have mercy on us, you son of David!""",40020031,"ChatCompletion(id='chatcmpl-8dJohdwOYqcsn6pCSWvbwYgmuOtbj', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381659, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 1
  total_token_count: 338
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:31
679,679,"Jesus stood still, and called them, and asked, ""What do you want me to do for you?""",40020032,"ChatCompletion(id='chatcmpl-8dJoiHHxcfFSkowVTcW1frBjqGZ4b', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381660, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 2
  total_token_count: 327
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:32
680,680,"They told him, ""Lord, that our eyes may be opened.""",40020033,"ChatCompletion(id='chatcmpl-8dJoiGCVxQHbTyVrkkaXg9noI6MA4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381660, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:33
681,681,"Jesus, being moved with compassion, touched their eyes; and immediately their eyes received their sight, and they followed him.",40020034,"ChatCompletion(id='chatcmpl-8dJojfm0zggEBux5EdyA65nynRUAA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381661, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 20:34
682,682,"When they drew near to Jerusalem, and came to Bethsphage,{TR reads ""Bethphage"" instead of ""Bethsphage""} to the Mount of Olives, then Jesus sent two disciples,",40021001,"ChatCompletion(id='chatcmpl-8dJojSlTSxSivFrE3S5bwNGbwVw98', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381661, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=339, total_tokens=340))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 342
  candidates_token_count: 1
  total_token_count: 343
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:1
683,683,"saying to them, ""Go into the village that is opposite you, and immediately you will find a donkey tied, and a colt with her. Untie them, and bring them to me.",40021002,"ChatCompletion(id='chatcmpl-8dJokqaBOu4vragQDgA7vJNB4rN9Z', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381662, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=338, total_tokens=339))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 342
  candidates_token_count: 1
  total_token_count: 343
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.4, 0.1, 0.4, 0.9, 0.4, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.1, 'Insult': 0.4, 'Legal': 0.1, 'Profanity': 0.4, 'Religion & Belief': 0.9, 'Sexual': 0.4, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:2
684,684,"If anyone says anything to you, you shall say, 'The Lord needs them,' and immediately he will send them.""",40021003,"ChatCompletion(id='chatcmpl-8dJokV4M0WzHNv3Wn0mjsJWRao937', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381662, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:3
685,685,"All this was done, that it might be fulfilled which was spoken through the prophet, saying,",40021004,"ChatCompletion(id='chatcmpl-8dJola3OwPTCn4qhwWwrISezTZfiG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381663, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:4
686,686,"""Tell the daughter of Zion, Behold, your King comes to you, Humble, and riding on a donkey, On a colt, the foal of a donkey.""",40021005,"ChatCompletion(id='chatcmpl-8dJolA8kfTcRjhbXvw0u8uhyqoRlr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381663, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=335, total_tokens=336))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 2
  total_token_count: 338
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 21:5
687,687,"The disciples went, and did just as Jesus commanded them,",40021006,"ChatCompletion(id='chatcmpl-8dJoms8AumTG1UymHc1PFdFTz9i5G', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381664, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=308, total_tokens=309))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:6
688,688,"and brought the donkey and the colt, and laid their clothes on them; and he sat on them.",40021007,"ChatCompletion(id='chatcmpl-8dJom5pomPZtYK1E3tiH17rimCOfW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381664, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 0.9, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:7
689,689,"A very great multitude spread their clothes on the road. Others cut branches from the trees, and spread them on the road.",40021008,"ChatCompletion(id='chatcmpl-8dJonnIAmcsTWjaAQ7ufuOz46QaFq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381665, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.4, 0.2, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:8
690,690,"The multitudes who went before him, and who followed kept shouting, ""Hosanna to the son of David! Blessed is he who comes in the name of the Lord! Hosanna in the highest!""",40021009,"ChatCompletion(id='chatcmpl-8dJonZtwB7kMo9hieRMAS6UR84bIU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381665, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=338, total_tokens=339))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 343
  candidates_token_count: 1
  total_token_count: 344
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:9
691,691,"When he had come into Jerusalem, all the city was stirred up, saying, ""Who is this?""",40021010,"ChatCompletion(id='chatcmpl-8dJooozgD57YQuycx0s9uSKXHbYwk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381666, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:10
692,692,"The multitudes said, ""This is the prophet, Jesus, from Nazareth of Galilee.""",40021011,"ChatCompletion(id='chatcmpl-8dJooeLQSg0Hz5c5ZmN2iwoDFNmp0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381666, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:11
693,693,"Jesus entered into the temple of God, and drove out all of those who sold and bought in the temple, and overthrew the money-changers' tables and the seats of those who sold the doves.",40021012,"ChatCompletion(id='chatcmpl-8dJop82R7p61P6EWMI9Rfn8BUzi24', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381667, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=339, total_tokens=340))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 1
  total_token_count: 345
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.3, 0.3, 0.1, 0.3, 0.2, 0.6, 0.2, 0.1, 1.0, 0.2, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Finance': 0.3, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:12
694,694,"He said to them, ""It is written, 'My house shall be called a house of prayer,' but you have made it a den of robbers!""",40021013,"ChatCompletion(id='chatcmpl-8dJopii47DEilB8zjvkfIyiKxFdVK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381667, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.1, 0.1, 0.4, 0.1, 0.1, 0.1, 1.0, 0.1, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Politics': 0.1, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:13
695,695,"The blind and the lame came to him in the temple, and he healed them.",40021014,"ChatCompletion(id='chatcmpl-8dJoqgZPxZMPEHqWoER80ucIQvTZa', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381668, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.9, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.9, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 21:14
696,696,"But when the chief priests and the scribes saw the wonderful things that he did, and the children who were crying in the temple and saying, ""Hosanna to the son of David!"" they were indignant,",40021015,"ChatCompletion(id='chatcmpl-8dJoqB3HB60DS6Xru55IxAK1kB6x5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381668, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=340, total_tokens=341))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 2
  total_token_count: 346
}
",10,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.4, 0.1, 0.2, 0.1, 0.3, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:15
697,697,"and said to him, ""Do you hear what these are saying?"" Jesus said to them, ""Yes. Did you never read, 'Out of the mouth of babes and nursing babies you have perfected praise?'""",40021016,"ChatCompletion(id='chatcmpl-8dJorgi4Q4fMHdbxyNGDoENviAKCO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381669, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=339, total_tokens=340))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 346
  candidates_token_count: 1
  total_token_count: 347
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:16
698,698,"He left them, and went out of the city to Bethany, and lodged there.",40021017,"ChatCompletion(id='chatcmpl-8dJorlFDav4xikLxV4ugLHQu2FBuv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381669, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.3, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:17
699,699,"Now in the morning, as he returned to the city, he was hungry.",40021018,"ChatCompletion(id='chatcmpl-8dJosRO26zKWi4eZNQBFpfkyxph9Q', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381670, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.9, 0.2, 0.1, 0.7, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.9, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:18
700,700,"Seeing a fig tree by the road, he came to it, and found nothing on it but leaves. He said to it, ""Let there be no fruit from you forever!"" Immediately the fig tree withered away.",40021019,"ChatCompletion(id='chatcmpl-8dJotOPysn0QxXLWY7ZZqtOtvLRRY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381671, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=340, total_tokens=341))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 346
  candidates_token_count: 1
  total_token_count: 347
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.6, 0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:19
701,701,"When the disciples saw it, they marveled, saying, ""How did the fig tree immediately wither away?""",40021020,"ChatCompletion(id='chatcmpl-8dJot2wlmAC6ZVw4PaPmO8Y3qnlrd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381671, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.6, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:20
702,702,"Jesus answered them, ""Most assuredly I tell you, if you have faith, and don't doubt, you will not only do what is done to the fig tree, but even if you told this mountain, 'Be taken up and cast into the sea,' it would be done.",40021021,"ChatCompletion(id='chatcmpl-8dJouFz5aVaPhaGVCQaSh2b5o86e6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381672, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=354, total_tokens=355))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 361
  candidates_token_count: 2
  total_token_count: 363
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.3, 0.1, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Health': 0.1, 'Insult': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:21
703,703,"All things, whatever you ask in prayer, believing, you will receive.""",40021022,"ChatCompletion(id='chatcmpl-8dJouHH6hs5XFERPsDC5Puzscwz2n', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381672, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:22
704,704,"When he had come into the temple, the chief priests and the elders of the people came to him as he was teaching, and said, ""By what authority do you do these things? Who gave you this authority?""",40021023,"ChatCompletion(id='chatcmpl-8dJouUQtTj3v5XA9NzLwM29uqJSmT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381672, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=340, total_tokens=341))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 347
  candidates_token_count: 1
  total_token_count: 348
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.2, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Legal': 0.5, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:23
705,705,"Jesus answered them, ""I also will ask you one question, which if you tell me, I likewise will tell you by what authority I do these things.",40021024,"ChatCompletion(id='chatcmpl-8dJovsHcCvcddWzqmbanwIlA1Lp9A', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381673, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:24
706,706,"The baptism of John, where was it from? From heaven or from men?"" They reasoned with themselves, saying, ""If we say, 'From heaven,' he will ask us, 'Why then did you not believe him?'",40021025,"ChatCompletion(id='chatcmpl-8dJowE1fJiz6SCqJHV8HzFIEDu1D8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381674, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=342, total_tokens=343))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 349
  candidates_token_count: 1
  total_token_count: 350
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:25
707,707,"But if we say, 'From men,' we fear the multitude, for all hold John as a prophet.""",40021026,"ChatCompletion(id='chatcmpl-8dJowNNQVOrg8X9h2op8zbGqBpYQk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381674, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.2, 0.3, 0.5, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.3, 'Politics': 0.5, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:26
708,708,"They answered Jesus, and said, ""We don't know."" He also said to them, ""Neither will I tell you by what authority I do these things.",40021027,"ChatCompletion(id='chatcmpl-8dJoxtGXlh8RNYYcsbE1rS9nXwj81', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381675, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 1
  total_token_count: 338
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:27
709,709,"But what do you think? A man had two sons, and he came to the first, and said, 'Son, go work today in my vineyard.'",40021028,"ChatCompletion(id='chatcmpl-8dJoxld7g6lAdfExUd52cAoMM6kt4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381675, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.2, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:28
710,710,"He answered, 'I will not,' but afterward he changed his mind, and went.",40021029,"ChatCompletion(id='chatcmpl-8dJoyX9HOd8Mj92wZTbWBHJjfarfX', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381676, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:29
711,711,"He came to the second, and said the same thing. He answered, 'I go, sir,' but he didn't go.",40021030,"ChatCompletion(id='chatcmpl-8dJoyV9yIY3q2Wsu8Z69AuE64nTII', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381676, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:30
712,712,"Which of the two did the will of his father?"" They said to him, ""The first."" Jesus said to them, ""Most assuredly I tell you that the tax collectors and the prostitutes are entering into the Kingdom of God before you.",40021031,"ChatCompletion(id='chatcmpl-8dJozc5F5OeAz7JlO3HOpqKWcBlj4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381677, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=345, total_tokens=346))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 351
  candidates_token_count: 1
  total_token_count: 352
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.3, 0.4, 0.6, 0.4, 0.5, 0.6, 0.2, 0.5, 1.0, 0.4, 0.3, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Finance': 0.3, 'Health': 0.4, 'Illicit Drugs': 0.6, 'Insult': 0.4, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.5, 'Religion & Belief': 1.0, 'Sexual': 0.4, 'Toxic': 0.3, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:31
713,713,"For John came to you in the way of righteousness, and you didn't believe him, but the tax collectors and the prostitutes believed him. When you saw it, you didn't even repent afterward, that you might believe him.",40021032,"ChatCompletion(id='chatcmpl-8dJoz5oTDeCbTIfftBSxugyRGwwl9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381677, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=342, total_tokens=343))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 351
  candidates_token_count: 1
  total_token_count: 352
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.4, 0.1, 0.4, 0.2, 0.6, 0.1, 0.1, 1.0, 0.3, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.3, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:32
714,714,"""Hear another parable. There was a man who was a master of a household, who planted a vineyard, set a hedge about it, dug a winepress in it, built a tower, leased it out to farmers, and went into another country.",40021033,"ChatCompletion(id='chatcmpl-8dJozzul6mGe9ItGqEy9qi1PpMbzI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381677, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=349, total_tokens=350))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 354
  candidates_token_count: 1
  total_token_count: 355
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.3, 0.2, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:33
715,715,"When the season for the fruit drew near, he sent his servants to the farmers, to receive his fruit.",40021034,"ChatCompletion(id='chatcmpl-8dJp0yosqevVMOzQflfXQve1kky0x', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381678, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:34
716,716,"The farmers took his servants, beat one, killed another, and stoned another.",40021035,"ChatCompletion(id='chatcmpl-8dJp0z7SJku2RBdByIqfvmnxQeV8d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381678, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.7, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.5, 0.3, 0.1, 0.6, 0.5, 0.2, 0.2, 0.5, 0.9, 0.3, 0.5, 0.5, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.5, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.6, 'Legal': 0.5, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.5, 'Religion & Belief': 0.9, 'Sexual': 0.3, 'Toxic': 0.5, 'Violent': 0.5, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:35
717,717,"Again, he sent other servants more than the first: and they treated them the same way.",40021036,"ChatCompletion(id='chatcmpl-8dJp12fQVWUpUCUd6PeBlybbNbHBT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381679, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:36
718,718,"But afterward he sent to them his son, saying, 'They will respect my son.'",40021037,"ChatCompletion(id='chatcmpl-8dJp1ZqqgEEJwEx7kCMLI5w2cXKrl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381679, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.3, 0.2, 0.2, 0.2, 0.1, 0.8, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:37
719,719,"But the farmers, when they saw the son, said among themselves, 'This is the heir. Come, let's kill him, and seize his inheritance.'",40021038,"ChatCompletion(id='chatcmpl-8dJp2gBFyoooO8irhLji5myhwqxmL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381680, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: MEDIUM
    blocked: true
  }
}
usage_metadata {
  prompt_token_count: 336
  total_token_count: 336
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.7, 'severityScore': 0.9, 'severity': 'HIGH', 'category': 'Dangerous Content'}, {'probabilityScore': 0.7, 'severityScore': 0.8, 'severity': 'HIGH', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.4, 0.1, 0.2, 0.7, 0.2, 0.2, 0.3, 0.1, 0.7, 0.2, 0.5, 0.9, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.7, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.5, 'Violent': 0.9, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:38
720,720,"So they took him, and threw him out of the vineyard, and killed him.",40021039,"ChatCompletion(id='chatcmpl-8dJp2gqRP4xXxuyFVOKK8EWgZvQWg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381680, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.7, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.7, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [1.0, 0.4, 0.2, 0.6, 0.2, 0.3, 0.1, 0.9, 0.2, 0.5, 0.5, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 1.0, 'Derogatory': 0.4, 'Health': 0.2, 'Insult': 0.6, 'Legal': 0.2, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.5, 'Violent': 0.5, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:39
721,721,"When therefore the lord of the vineyard comes, what will he do to those farmers?""",40021040,"ChatCompletion(id='chatcmpl-8dJp3SpatH5curZLDOQYPM59cJxl8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381681, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.4, 0.1, 0.2, 0.1, 0.3, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.3, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:40
722,722,"They told him, ""He will miserably destroy those miserable men, and will lease out the vineyard to other farmers, who will give him the fruit in its season.""",40021041,"ChatCompletion(id='chatcmpl-8dJp3lkfgEAVLS3iXCPnOpTRxb48H', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381681, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  total_token_count: 336
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.5, 0.1, 0.6, 0.2, 0.6, 0.2, 0.3, 0.9, 0.2, 0.4, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.5, 'Finance': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.6, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.3, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.4, 'Violent': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:41
723,723,"Jesus said to them, ""Did you never read in the Scriptures, 'The stone which the builders rejected, The same was made the head of the corner. This was from the Lord. It is marvelous in our eyes?'",40021042,"ChatCompletion(id='chatcmpl-8dJp4DHPth3VJRsK3bZVCV6rDCN1g', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381682, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=341, total_tokens=342))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 348
  candidates_token_count: 1
  total_token_count: 349
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:42
724,724,"""Therefore I tell you, the Kingdom of God will be taken away from you, and will be given to a nation bringing forth its fruits.",40021043,"ChatCompletion(id='chatcmpl-8dJp4Y37TMVUmf3WBEsNsyvwsyP7m', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381682, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.5, 0.1, 0.4, 0.1, 0.2, 0.3, 0.1, 1.0, 0.2, 0.3, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:43
725,725,"He who falls on this stone will be broken to pieces, but on whoever it will fall, it will scatter him as dust.""",40021044,"ChatCompletion(id='chatcmpl-8dJp57dGUI31DMmNp2tiS7ObQfTFn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381683, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.5, 0.2, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:44
726,726,"When the chief priests and the Pharisees heard his parables, they perceived that he spoke about them.",40021045,"ChatCompletion(id='chatcmpl-8dJp5B6gQDmbuKWXl0j24BsCNJ3Vz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381683, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:45
727,727,"When they sought to seize him, they feared the multitudes, because they considered him to be a prophet.",40021046,"ChatCompletion(id='chatcmpl-8dJp6Fxy3x0Cod0QBXpqLBjbYDfly', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381684, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.1, 0.2, 0.3, 0.2, 0.6, 0.1, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 21:46
728,728,"Jesus answered and spoke again in parables to them, saying,",40022001,"ChatCompletion(id='chatcmpl-8dJp68TQsPCGq4yiAmyc4L6V9kCW4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381684, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:1
729,729,"""The Kingdom of Heaven is like a certain king, who made a marriage feast for his son,",40022002,"ChatCompletion(id='chatcmpl-8dJp6MmiShqZDtQLsyfzWErLr2Usq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381684, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.5, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.2, 0.2, 0.9, 0.5, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.5, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:2
730,730,"and sent out his servants to call those who were invited to the marriage feast, but they would not come.",40022003,"ChatCompletion(id='chatcmpl-8dJp7dTunJYOakfxFVwwxRywNS1Xf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381685, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.2, 0.1, 0.9, 0.4, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.4, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:3
731,731,"Again he sent out other servants, saying, 'Tell those who are invited, ""Behold, I have made ready my dinner. My oxen and my fatlings are killed, and all things are ready. Come to the marriage feast!""'",40022004,"ChatCompletion(id='chatcmpl-8dJp7HVRn6rZhbKw4aWRZuI3VPqZA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381685, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=346, total_tokens=347))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 351
  candidates_token_count: 1
  total_token_count: 352
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.2, 0.9, 0.5, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.5, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:4
732,732,"But they made light of it, and went their ways, one to his own farm, another to his merchandise,",40022005,"ChatCompletion(id='chatcmpl-8dJp8oU0jWPKYj5eIBibvQz1xOzEd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381686, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.1, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:5
733,733,"and the rest grabbed his servants, and treated them shamefully, and killed them.",40022006,"ChatCompletion(id='chatcmpl-8dJp9gzynLCO1v2TvUlCkEXzsld2S', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381687, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 320
  total_token_count: 320
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.7, 'severity': 'HIGH', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.7, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.6, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.6, 0.1, 0.6, 0.2, 0.2, 0.3, 0.3, 0.9, 0.3, 0.5, 0.5, 0.5], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.6, 'Health': 0.1, 'Insult': 0.6, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.3, 'Public Safety': 0.3, 'Religion & Belief': 0.9, 'Sexual': 0.3, 'Toxic': 0.5, 'Violent': 0.5, 'War & Conflict': 0.5}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:6
734,734,"When the king heard that, he was angry, and he sent his armies, destroyed those murderers, and burned their city.",40022007,"ChatCompletion(id='chatcmpl-8dJpAyN9Lg70i3UmiIH4WWZ09Xn3q', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381688, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.7, 0.4, 0.1, 0.2, 0.1, 0.5, 0.2, 0.6, 0.2, 0.3, 0.9, 0.2, 0.3, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.7, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.3, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.2, 'War & Conflict': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:7
735,735,"""Then he said to his servants, 'The wedding is ready, but those who were invited weren't worthy.",40022008,"ChatCompletion(id='chatcmpl-8dJpANupkLVjQFw7Xb944vZ1LlhVc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381688, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.5, 0.1, 0.3, 0.5, 0.1, 0.3, 0.1, 0.8, 0.2, 0.2], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.5, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.5, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:8
736,736,"Go therefore to the intersections of the highways, and as many as you may find, invite to the marriage feast.'",40022009,"ChatCompletion(id='chatcmpl-8dJpBZoJtGw4xKxeYoJYt60zXSUUC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381689, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.5, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.1, 0.2, 0.7, 0.5, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.2, 'Religion & Belief': 0.7, 'Sexual': 0.5, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:9
737,737,"Those servants went out into the highways, and gathered together as many as they found, both bad and good. The wedding was filled with guests.",40022010,"ChatCompletion(id='chatcmpl-8dJpBPKDGBrpVq1vdjDH4DzULzgzp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381689, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.3, 0.2, 0.7, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.3, 'Profanity': 0.2, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:10
738,738,"But when the king came in to see the guests, he saw there a man who didn't have on wedding clothing,",40022011,"ChatCompletion(id='chatcmpl-8dJpC0lPqOe2lx2itG3DrhtsaGagO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381690, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.6, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.6, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:11
739,739,"and he said to him, 'Friend, how did you come in here not wearing wedding clothing?' He was speechless.",40022012,"ChatCompletion(id='chatcmpl-8dJpCljs8AMZGwvnsotWhwtMAnwsZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381690, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.4, 0.3, 0.1], 'categories': ['Derogatory', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.4, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:12
740,740,"Then the king said to the servants, 'Bind him hand and foot, take him away, and throw him into the outer darkness; there is where the weeping and grinding of teeth will be.'",40022013,"ChatCompletion(id='chatcmpl-8dJpDmmzoeNltTWQ0tMfMfHxSWpe3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381691, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=336, total_tokens=337))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 342
  candidates_token_count: 1
  total_token_count: 343
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.2, 0.3, 0.3, 0.2, 0.1, 1.0, 0.2, 0.3, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.3, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:13
741,741,"For many are called, but few chosen.""",40022014,"ChatCompletion(id='chatcmpl-8dJpDhpr1Y9LCwH8FZQOkEbmbvfzz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381691, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=305, total_tokens=306))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 312
  candidates_token_count: 1
  total_token_count: 313
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.1, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:14
742,742,Then the Pharisees went and took counsel how they might entrap him in his talk.,40022015,"ChatCompletion(id='chatcmpl-8dJpEvZfUlCtJsIuz9q9wlsOceb0I', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381692, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 0.2, 0.1, 0.4, 0.2, 0.6, 0.1, 0.3, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:15
743,743,"They sent their disciples to him, along with the Herodians, saying, ""Teacher, we know that you are honest, and teach the way of God in truth, no matter who you teach, for you aren't partial to anyone.",40022016,"ChatCompletion(id='chatcmpl-8dJpEQDmcc6myjfZOnEn0ZVbLPL5x', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381692, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=345, total_tokens=346))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 352
  candidates_token_count: 1
  total_token_count: 353
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.3, 0.3, 0.4, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.3, 'Politics': 0.4, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:16
744,744,"Tell us therefore, what do you think? Is it lawful to pay taxes to Caesar, or not?""",40022017,"ChatCompletion(id='chatcmpl-8dJpFUMjMQJRzsGsr1QaXXdFXiJnA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381693, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.9, 0.2, 0.1, 0.3, 0.9, 0.8, 0.1, 0.1, 0.6, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.9, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.9, 'Politics': 0.8, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.6, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:17
745,745,"But Jesus perceived their wickedness, and said, ""Why do you test me, you hypocrites?",40022018,"ChatCompletion(id='chatcmpl-8dJpFJX1oTO7yJ8lReCOJGDf4K3CZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381693, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  total_token_count: 323
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.7, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.1, 0.3, 0.7, 0.6, 0.2, 0.1, 1.0, 0.1, 0.5, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.7, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.5, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:18
746,746,"Show me the tax money."" They brought to him a denarius.",40022019,"ChatCompletion(id='chatcmpl-8dJpGYe8hij0fwcknP8FTsPMP2AUC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381694, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 1.0, 0.1, 0.1, 0.2, 0.5, 0.6, 0.1, 0.6, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 1.0, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.5, 'Politics': 0.6, 'Public Safety': 0.1, 'Religion & Belief': 0.6, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:19
747,747,"He asked them, ""Whose is this image and inscription?""",40022020,"ChatCompletion(id='chatcmpl-8dJpGm6rFfYESQpI1pd6YgQBGWjiz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381694, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.1, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Legal': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:20
748,748,"They said to him, ""Caesar's."" Then he said to them, ""Give therefore to Caesar the things that are Caesar's, and to God the things that are God's.""",40022021,"ChatCompletion(id='chatcmpl-8dJpHfgTwvdrejJ14CQZlgM8Pnwtf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381695, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=334, total_tokens=335))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 343
  candidates_token_count: 1
  total_token_count: 344
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 0.2, 0.4, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.4, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:21
749,749,"When they heard it, they marveled, and left him, and went away.",40022022,"ChatCompletion(id='chatcmpl-8dJpHDA4Z4QHeLFV7BgbO4lva3NqD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381695, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.5, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:22
750,750,"On that day Sadducees (those who say that there is no resurrection) came to him. They asked him,",40022023,"ChatCompletion(id='chatcmpl-8dJpIjhtkETBH0RVW6Tft3Ula2Dc6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381696, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.3, 0.1, 0.3, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:23
751,751,"saying, ""Teacher, Moses said, 'If a man dies, having no children, his brother shall marry his wife, and raise up seed for his brother.'",40022024,"ChatCompletion(id='chatcmpl-8dJpIcsODBOzZZntYLVgduJso3kr1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381696, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.1, 0.5, 0.2, 0.2, 0.1, 1.0, 0.3, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Legal': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:24
752,752,"Now there were with us seven brothers. The first married and died, and having no seed left his wife to his brother.",40022025,"ChatCompletion(id='chatcmpl-8dJpJ4MURRUw2QuT1eqsd2OFGcxls', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381697, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.2, 0.1, 0.5, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Legal': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:25
753,753,"In like manner the second also, and the third, to the seventh.",40022026,"ChatCompletion(id='chatcmpl-8dJpJV3OTv3B036e5QL03kw2xhBME', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381697, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.1, 0.1, 0.8, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:26
754,754,"After them all, the woman died.",40022027,"ChatCompletion(id='chatcmpl-8dJpKKmFrqX7AeC3MWKZHC1SAgj1S', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381698, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=304, total_tokens=305))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 311
  candidates_token_count: 1
  total_token_count: 312
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [1.0, 0.3, 0.5, 0.3, 0.2, 0.1, 0.7, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 1.0, 'Derogatory': 0.3, 'Health': 0.5, 'Insult': 0.3, 'Legal': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:27
755,755,"In the resurrection therefore, whose wife will she be of the seven? For they all had her.""",40022028,"ChatCompletion(id='chatcmpl-8dJpLm12vrC0UZzcLC8Qq7NGWJTnt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381699, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.2, 0.2, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:28
756,756,"But Jesus answered them, ""You are mistaken, not knowing the Scriptures, nor the power of God.",40022029,"ChatCompletion(id='chatcmpl-8dJpLjOXOYR2mbvDnpb6kFpNoD9UP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381699, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.3, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.2, 'Insult': 0.3, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:29
757,757,"For in the resurrection they neither marry, nor are given in marriage, but are like God's angels in heaven.",40022030,"ChatCompletion(id='chatcmpl-8dJpM1Kw2sIJrVp3UkTv7ULaAyS7E', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381700, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.2, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:30
758,758,"But concerning the resurrection of the dead, haven't you read that which was spoken to you by God, saying,",40022031,"ChatCompletion(id='chatcmpl-8dJpMeJkvc0CirPH1WpNQZJmme0wZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381700, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.2, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:31
759,759,"'I am the God of Abraham, and the God of Isaac, and the God of Jacob?' God is not the God of the dead, but of the living.""",40022032,"ChatCompletion(id='chatcmpl-8dJpM8vskUEpF85WHiFyeLsC4Dl5Q', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381700, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 1
  total_token_count: 338
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:32
760,760,"When the multitudes heard it, they were astonished at his teaching.",40022033,"ChatCompletion(id='chatcmpl-8dJpNdD6A4OGeAybIB6XiAQr6keVX', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381701, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:33
761,761,"But the Pharisees, when they heard that he had put the Sadducees to silence, gathered themselves together.",40022034,"ChatCompletion(id='chatcmpl-8dJpNGZuFoRFSQt0LCudJYzfE2opK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381701, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.5, 0.4, 0.2, 0.6, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Health': 0.5, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:34
762,762,"One of them, a lawyer, asked him a question, testing him.",40022035,"ChatCompletion(id='chatcmpl-8dJpO8n1b2ztUwyRI2qtKrtPOrdw4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381702, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.3, 0.1, 0.2, 0.5, 0.2, 0.1, 0.1, 0.6, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.3, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.5, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.6, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:35
763,763,"""Teacher, which is the greatest commandment in the law?""",40022036,"ChatCompletion(id='chatcmpl-8dJpOeHR41vwlnCM4QdpClwwYFN5G', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381702, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.2, 0.5, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.5, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:36
764,764,"Jesus said to him, ""'You shall love the Lord your God with all your heart, and with all your soul, and with all your mind.'",40022037,"ChatCompletion(id='chatcmpl-8dJpPLiPw2X73CwDbnTXwcBTeSlQ6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381703, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:37
765,765,This is the first and great commandment.,40022038,"ChatCompletion(id='chatcmpl-8dJpPg6XmK2wjFV6yTm2GhBWjluXa', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381703, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=305, total_tokens=306))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 311
  candidates_token_count: 1
  total_token_count: 312
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.3, 0.1, 1.0, 0.1, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:38
766,766,"A second likewise is this, 'You shall love your neighbor as yourself.'",40022039,"ChatCompletion(id='chatcmpl-8dJpQ5XfKRN6OVRIOnoL3sySQvIkP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381704, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.2, 0.7, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.1, 'Insult': 0.2, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:39
767,767,"The whole law and the prophets depend on these two commandments.""",40022040,"ChatCompletion(id='chatcmpl-8dJpQS3GP9gFDp4OiiOrqRuA3J7M5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381704, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.2, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:40
768,768,"Now while the Pharisees were gathered together, Jesus asked them a question,",40022041,"ChatCompletion(id='chatcmpl-8dJpRflZjr50YLGIhIa8liocdtOEI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381705, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.1, 0.3, 0.1, 0.3, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:41
769,769,"saying, ""What do you think of the Christ? Whose son is he?"" They said to him, ""Of David.""",40022042,"ChatCompletion(id='chatcmpl-8dJpRoZLsyDT4XkvDbO3GoSCxjDH8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381705, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.2, 1.0, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:42
770,770,"He said to them, ""How then does David in the Spirit call him Lord, saying,",40022043,"ChatCompletion(id='chatcmpl-8dJpSDFI3Kl9bwfPujaTNWenYZwLb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381706, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.2, 1.0, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.1, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:43
771,771,"'The Lord said to my Lord, Sit on my right hand, Until I make your enemies a footstool for your feet?'",40022044,"ChatCompletion(id='chatcmpl-8dJpShiIvMGIvdArv3ttnSWqtOPjL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381706, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.1, 0.4, 0.2, 0.2, 0.1, 0.9, 0.2, 0.3, 0.3, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.1, 'Insult': 0.4, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.3, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:44
772,772,"""If then David calls him Lord, how is he his son?""",40022045,"ChatCompletion(id='chatcmpl-8dJpTm1tQqQSNBeLVEnQ54pJRTUFu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381707, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:45
773,773,"No one was able to answer him a word, neither dared any man from that day forth ask him any more questions.",40022046,"ChatCompletion(id='chatcmpl-8dJpUL1CoGnU8AyXjG5MX3519gEo9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381708, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.5, 0.2, 0.2, 0.1, 0.9, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 22:46
774,774,"Then Jesus spoke to the multitudes and to his disciples,",40023001,"ChatCompletion(id='chatcmpl-8dJpU8gXHFB2p9ZkIASIFrE3RnvIF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381708, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=308, total_tokens=309))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 314
  candidates_token_count: 1
  total_token_count: 315
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:1
775,775,"saying, ""The scribes and the Pharisees sat on Moses' seat.",40023002,"ChatCompletion(id='chatcmpl-8dJpVoic0B7BqnZWA0YTroyuzW30O', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381709, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.3, 0.1, 0.6, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:2
776,776,"All things therefore whatever they tell you to observe, observe and do, but don't do their works; for they say, and don't do.",40023003,"ChatCompletion(id='chatcmpl-8dJpVa3F3kfDoOjEb6JpESsWSpRj5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381709, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.3, 0.1, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.3, 'Legal': 0.1, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:3
777,777,"For they bind heavy burdens that are grievous to be borne, and lay them on men's shoulders; but they themselves will not lift a finger to help them.",40023004,"ChatCompletion(id='chatcmpl-8dJpWQTaWEnyw8qx4TMfVZrEnNUnk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381710, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.5, 0.3, 0.1, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.2, 'Health': 0.5, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 23:4
778,778,"But all their works they do to be seen by men. They make their phylacteries broad, enlarge the fringes of their garments,",40023005,"ChatCompletion(id='chatcmpl-8dJpWiuzBjfMYN1Sn4b9zfjRVHSrh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381710, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.6, 0.1, 0.2, 0.1, 0.2, 0.1, 0.9, 0.4, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.4, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:5
779,779,"and love the place of honor at feasts, the best seats in the synagogues,",40023006,"ChatCompletion(id='chatcmpl-8dJpXeb3ZeObPxqBYsrgUjJlryLt0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381711, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.1, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:6
780,780,"the salutations in the marketplaces, and to be called 'Rabbi, Rabbi' by men.",40023007,"ChatCompletion(id='chatcmpl-8dJpXHOaWTIfeg4s47on8296s1kAY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381711, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.5, 0.2, 0.1, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:7
781,781,"But don't you be called 'Rabbi,' for one is your teacher, the Christ, and all of you are brothers.",40023008,"ChatCompletion(id='chatcmpl-8dJpYzPAkCs6NsaZQ6jR8QTpIkiYR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381712, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:8
782,782,"Call no man on the earth your father, for one is your Father, he who is in heaven.",40023009,"ChatCompletion(id='chatcmpl-8dJpYiOYfzRMtQRJIJV3HkMKyTQh2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381712, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.3, 0.1, 1.0, 0.1, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:9
783,783,"Neither be called masters, for one is your master, the Christ.",40023010,"ChatCompletion(id='chatcmpl-8dJpZWuVCGFU89AcsLhovIkIYbz8N', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381713, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.3, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.3, 'Legal': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:10
784,784,But he who is greatest among you will be your servant.,40023011,"ChatCompletion(id='chatcmpl-8dJpZlb4Wgv3usul9XvqmgG39RA1p', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381713, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=308, total_tokens=309))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:11
785,785,"Whoever exalts himself will be humbled, and whoever humbles himself will be exalted.",40023012,"ChatCompletion(id='chatcmpl-8dJpZ91ISsNOJE6PsZ6eYU0fUsa5C', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381713, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:12
786,786,"""Woe to you, scribes and Pharisees, hypocrites! For you devour widows' houses, and as a pretense you make long prayers. Therefore you will receive greater condemnation.",40023013,"ChatCompletion(id='chatcmpl-8dJpaU6khwKdRqZ40rb8SIqo4R8NE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381714, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=336, total_tokens=337))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  total_token_count: 339
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.7, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.1, 0.4, 0.7, 0.1, 0.6, 0.3, 0.1, 1.0, 0.1, 0.5, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.5, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.7, 'Legal': 0.1, 'Politics': 0.6, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.5, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:13
787,787,"""But woe to you, scribes and Pharisees, hypocrites! Because you shut up the Kingdom of Heaven against men; for you don't enter in yourselves, neither do you allow those who are entering in to enter.{Some Greek manuscripts reverse the order of verses 13 and 14, and some omit verse 13, numbering verse 14 as 13.}",40023014,"ChatCompletion(id='chatcmpl-8dJpbA6E4xVpDmhCn4LS8g3sjqinc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381715, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=373, total_tokens=374))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 382
  candidates_token_count: 1
  total_token_count: 383
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.4, 0.1, 0.6, 0.1, 0.6, 0.2, 0.1, 1.0, 0.1, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.6, 'Legal': 0.1, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.3, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:14
788,788,"Woe to you, scribes and Pharisees, hypocrites! For you travel around by sea and land to make one proselyte; and when he becomes one, you make him twice as much of a son of Gehenna as yourselves.",40023015,"ChatCompletion(id='chatcmpl-8dJpbblFmoHrvETAKDbYtrFGoFPMU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381715, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=347, total_tokens=348))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 350
  total_token_count: 350
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.7, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.1, 0.3, 0.7, 0.6, 0.3, 0.1, 1.0, 0.1, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.5, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.7, 'Politics': 0.6, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:15
789,789,"""Woe to you, you blind guides, who say, 'Whoever swears by the temple, it is nothing; but whoever swears by the gold of the temple, he is obligated.'",40023016,"ChatCompletion(id='chatcmpl-8dJpb0tF69JyJdeIV0Ntt0Q9goBlp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381715, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=336, total_tokens=337))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 341
  candidates_token_count: 1
  total_token_count: 342
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.3, 0.1, 0.4, 0.2, 0.3, 0.2, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:16
790,790,"You blind fools! For which is greater, the gold, or the temple that sanctifies the gold?",40023017,"ChatCompletion(id='chatcmpl-8dJpc7OVAKesZori1vNrEDnWIUsfN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381716, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.5, 0.1, 0.6, 0.1, 0.6, 0.3, 0.1, 0.9, 0.1, 0.4, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.3, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.6, 'Legal': 0.1, 'Politics': 0.6, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:17
791,791,"'Whoever swears by the altar, it is nothing; but whoever swears by the gift that is on it, he is obligated.'",40023018,"ChatCompletion(id='chatcmpl-8dJpc5Zd3wRtozCnXpBOpUTAoENQD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381716, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.1, 0.2, 0.5, 0.2, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.5, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:18
792,792,"You blind fools! For which is greater, the gift, or the altar that sanctifies the gift?",40023019,"ChatCompletion(id='chatcmpl-8dJpdsyYGy0UbP1VGWZht1aWZgnpQ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381717, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.5, 0.1, 0.6, 0.3, 0.3, 0.9, 0.2, 0.4, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.6, 'Politics': 0.3, 'Profanity': 0.3, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:19
793,793,"He therefore who swears by the altar, swears by it, and by everything on it.",40023020,"ChatCompletion(id='chatcmpl-8dJpeD99RXTUS3u930iFsNsMJ2p8U', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381718, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.1, 0.2, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:20
794,794,"He who swears by the temple, swears by it, and by him who was living in it.",40023021,"ChatCompletion(id='chatcmpl-8dJpeWL1YyOVJagnl6zl7a1pcmbxI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381718, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.4, 0.1, 0.3, 0.2, 0.4, 0.1, 1.0, 0.2, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.4, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:21
795,795,"He who swears by heaven, swears by the throne of God, and by him who sits on it.",40023022,"ChatCompletion(id='chatcmpl-8dJpeuk7ZqXGcYgCVSKG5pa0065ln', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381718, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.1, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:22
796,796,"""Woe to you, scribes and Pharisees, hypocrites! For you tithe mint, dill, and cumin,{ cumin is an aromatic seed from Cuminum cyminum, resembling caraway in flavor and appearance. It is used as a spice.} and have left undone the weightier matters of the law: justice, mercy, and faith. But you ought to have done these, and not to have left the other undone.",40023023,"ChatCompletion(id='chatcmpl-8dJpfi34l0JwTGvSkKHdddWjJn1rG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381719, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=389, total_tokens=390))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 389
  total_token_count: 389
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.1, 0.5, 0.1, 0.6, 0.1, 0.6, 0.3, 0.1, 1.0, 0.1, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.5, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.6, 'Legal': 0.1, 'Politics': 0.6, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:23
797,797,"You blind guides, who strain out a gnat, and swallow a camel!",40023024,"ChatCompletion(id='chatcmpl-8dJpfGz5ra5fKza57yEwb30e6eP9H', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381719, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.7, 0.3, 0.3, 0.9, 0.4, 0.2], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.7, 'Insult': 0.3, 'Profanity': 0.3, 'Religion & Belief': 0.9, 'Sexual': 0.4, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:24
798,798,"""Woe to you, scribes and Pharisees, hypocrites! For you clean the outside of the cup and of the platter, but within they are full of extortion and unrighteousness.{TR reads ""self-indulgence"" instead of ""unrighteousness""}",40023025,"ChatCompletion(id='chatcmpl-8dJpgqHn9eQjloQPncPnGDsVyapzh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381720, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=354, total_tokens=355))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 356
  total_token_count: 356
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.5, 0.1, 0.3, 0.1, 0.6, 0.1, 0.6, 0.2, 0.1, 1.0, 0.1, 0.4, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.5, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.6, 'Legal': 0.1, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:25
799,799,"You blind Pharisee, first clean the inside of the cup and of the platter, that the outside of it may become clean also.",40023026,"ChatCompletion(id='chatcmpl-8dJpgT3BW8I8XpoZ8cyCtG7c3dTDY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381720, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.7, 0.1, 0.3, 0.2, 0.3, 0.1, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.7, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:26
800,800,"""Woe to you, scribes and Pharisees, hypocrites! For you are like whitened tombs, which outwardly appear beautiful, but inwardly are full of dead men's bones, and of all uncleanness.",40023027,"ChatCompletion(id='chatcmpl-8dJphyxUZGZkYUWzdG5dZwbCwFCbx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381721, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=343, total_tokens=344))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 346
  total_token_count: 346
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.7, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.4, 0.7, 0.6, 0.3, 0.1, 1.0, 0.1, 0.5, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.5, 'Health': 0.4, 'Insult': 0.7, 'Politics': 0.6, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.5, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:27
801,801,"Even so you also outwardly appear righteous to men, but inwardly you are full of hypocrisy and iniquity.",40023028,"ChatCompletion(id='chatcmpl-8dJphTP0VdBthi1jvcwLjrKBgNIVX', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381721, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  total_token_count: 323
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.3, 0.6, 0.1, 0.6, 0.2, 0.1, 0.9, 0.1, 0.5, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.2, 'Health': 0.3, 'Insult': 0.6, 'Legal': 0.1, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.5, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:28
802,802,"""Woe to you, scribes and Pharisees, hypocrites! For you build the tombs of the prophets, and decorate the tombs of the righteous,",40023029,"ChatCompletion(id='chatcmpl-8dJpiCPARReOcbc4tbQPf5Lj42LqE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381722, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  total_token_count: 333
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.7, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.3, 0.7, 0.6, 0.3, 0.1, 1.0, 0.1, 0.5, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.5, 'Health': 0.3, 'Insult': 0.7, 'Politics': 0.6, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.5, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:29
803,803,"and say, 'If we had lived in the days of our fathers, we wouldn't have been partakers with them in the blood of the prophets.'",40023030,"ChatCompletion(id='chatcmpl-8dJpisZgxelsPT6MYAg4zFv9FkZZU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381722, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.6, 0.1, 0.3, 0.2, 0.6, 0.1, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:30
804,804,Therefore you testify to yourselves that you are children of those who killed the prophets.,40023031,"ChatCompletion(id='chatcmpl-8dJpj7v7obiiXIwe4bwIZAwaZW7XP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381723, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.9, 0.5, 0.2, 0.1, 0.5, 0.2, 0.6, 0.2, 0.3, 1.0, 0.2, 0.5, 0.4, 0.7], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.9, 'Derogatory': 0.5, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.5, 'Violent': 0.4, 'War & Conflict': 0.7}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:31
805,805,"Fill up, then, the measure of your fathers.",40023032,"ChatCompletion(id='chatcmpl-8dJpjMTMxN0BOrCjzizoYQH0muvQE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381723, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=307, total_tokens=308))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 314
  candidates_token_count: 1
  total_token_count: 315
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.5, 0.4, 0.2, 0.4, 0.2, 0.7, 0.2, 0.2], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.2, 'Health': 0.5, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.4, 'Profanity': 0.2, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:32
806,806,"You serpents, you offspring of vipers, how will you escape the judgment of Gehenna?",40023033,"ChatCompletion(id='chatcmpl-8dJpkNfbQtTahxfeDudNry5dL3EMI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381724, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.3, 0.4, 0.2, 0.3, 0.1, 1.0, 0.2, 0.3, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.4, 'Politics': 0.2, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:33
807,807,"Therefore, behold, I send to you prophets, wise men, and scribes. Some of them you will kill and crucify; and some of them you will scourge in your synagogues, and persecute from city to city;",40023034,"ChatCompletion(id='chatcmpl-8dJpk9sTsLuVVFKtMLUQ55B1KG9C7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381724, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=345, total_tokens=346))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: MEDIUM
    blocked: true
  }
}
usage_metadata {
  prompt_token_count: 349
  total_token_count: 349
}
",-1,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.7, 'severityScore': 0.8, 'severity': 'HIGH', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.6, 0.2, 0.2, 0.3, 1.0, 0.2, 0.5, 0.8, 0.5], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.5, 'Health': 0.2, 'Insult': 0.6, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.5, 'Violent': 0.8, 'War & Conflict': 0.5}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 23:34
808,808,"that on you may come all the righteous blood shed on the earth, from the blood of righteous Abel to the blood of Zachariah son of Barachiah, whom you killed between the sanctuary and the altar.",40023035,"ChatCompletion(id='chatcmpl-8dJplo6GzoGPlabKzdjVD8122Ajj7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381725, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=339, total_tokens=340))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 345
  candidates_token_count: 1
  total_token_count: 346
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.3, 0.2, 0.3, 0.1, 0.2, 0.2, 0.1, 1.0, 0.2, 0.3, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.3, 'Health': 0.2, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:35
809,809,"Most assuredly I tell you, all these things will come upon this generation.",40023036,"ChatCompletion(id='chatcmpl-8dJplECFVDWe0AhvyclcESEOkWdIi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381725, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:36
810,810,"""Jerusalem, Jerusalem, who kills the prophets, and stones those who are sent to her! How often would I have gathered your children together, even as a hen gathers her chickens under her wings, and you would not!",40023037,"ChatCompletion(id='chatcmpl-8dJpmPYmlLRpTdhp55M1yeDdMqJZz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381726, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=342, total_tokens=343))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 348
  candidates_token_count: 1
  total_token_count: 349
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.4, 0.3, 0.1, 0.4, 0.4, 0.2, 0.1, 1.0, 0.2, 0.4, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.4, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Politics': 0.4, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.4, 'Violent': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:37
811,811,"Behold, your house is left to you desolate.",40023038,"ChatCompletion(id='chatcmpl-8dJpmGO7ni6xnFlsTMnvCnQ2TbiJO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381726, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=308, total_tokens=309))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 313
  candidates_token_count: 1
  total_token_count: 314
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.2, 0.3, 0.2, 0.7, 0.1, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.3, 'Profanity': 0.2, 'Religion & Belief': 0.7, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:38
812,812,"For I tell you, you will not see me from now on, until you say, 'Blessed is he who comes in the name of the Lord!'""",40023039,"ChatCompletion(id='chatcmpl-8dJpmcbnHO40UFWYzmtGCRwcflKEP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381726, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 23:39
813,813,"Jesus went out from the temple, and was going on his way. His disciples came to him to show him the buildings of the temple.",40024001,"ChatCompletion(id='chatcmpl-8dJpnpSzTLNJSY6nFczxeH55KLgVW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381727, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:1
814,814,"But he answered them, ""Don't you see all of these things? Most assuredly I tell you, there will not be left here one stone on another, that will not be thrown down.""",40024002,"ChatCompletion(id='chatcmpl-8dJpneUsPDMRfIwcwvi6pQXmBBkIC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381727, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=336, total_tokens=337))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 343
  candidates_token_count: 1
  total_token_count: 344
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.5, 0.1, 0.1, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:2
815,815,"As he sat on the Mount of Olives, the disciples came to him privately, saying, ""Tell us, when will these things be? What is the sign of your coming, and of the end of the age?""",40024003,"ChatCompletion(id='chatcmpl-8dJponH3NivTquZWzp6bs4ZZd9rZE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381728, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=341, total_tokens=342))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 347
  candidates_token_count: 1
  total_token_count: 348
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:3
816,816,"Jesus answered them, ""Be careful that no one leads you astray.",40024004,"ChatCompletion(id='chatcmpl-8dJpoRW519CRxaxXwHlA3Wa69iIYJ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381728, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:4
817,817,"For many will come in my name, saying, 'I am the Christ,' and will lead many astray.",40024005,"ChatCompletion(id='chatcmpl-8dJppM12HdFQElyobhHV8bZs5JfRB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381729, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.3, 0.1, 1.0, 0.1, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.2, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:5
818,818,"You will hear of wars and rumors of wars. See that you aren't troubled, for all this must happen, but the end is not yet.",40024006,"ChatCompletion(id='chatcmpl-8dJppxIOmYAUAdTZKknBX0PLy7sCI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381729, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 2
  total_token_count: 336
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.4, 0.2, 0.1, 0.2, 0.1, 0.1, 0.9, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:6
819,819,"For nation will rise against nation, and kingdom against kingdom; and there will be famines, plagues, and earthquakes in various places.",40024007,"ChatCompletion(id='chatcmpl-8dJpqkDTl8CgIB1vMrLMgzPEycNgv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381730, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 2
  total_token_count: 332
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.5, 0.3, 0.2, 0.2, 0.1, 0.9, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 24:7
820,820,But all these things are the beginning of birth pains.,40024008,"ChatCompletion(id='chatcmpl-8dJpqgVbG4fBKYPqDlFVxEEIIfFHD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381730, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=307, total_tokens=308))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 314
  candidates_token_count: 1
  total_token_count: 315
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.9, 0.2, 0.2, 0.2, 0.6, 0.3, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.9, 'Insult': 0.2, 'Legal': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.6, 'Sexual': 0.3, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:8
821,821,"Then they will deliver you up to oppression, and will kill you. You will be hated by all of the nations for my name's sake.",40024009,"ChatCompletion(id='chatcmpl-8dJprddGm62DuwH2c1CCMyPRCfYNS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381731, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 333
  total_token_count: 333
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.8, 'severity': 'HIGH', 'category': 'Dangerous Content'}, {'probabilityScore': 0.7, 'severityScore': 0.7, 'severity': 'HIGH', 'category': 'Harassment'}, {'probabilityScore': 0.6, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.7, 0.6, 0.2, 0.7, 0.2, 0.3, 0.4, 0.3, 0.8, 0.2, 0.5, 0.9, 0.5], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.7, 'Derogatory': 0.6, 'Health': 0.2, 'Insult': 0.7, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.4, 'Public Safety': 0.3, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.5, 'Violent': 0.9, 'War & Conflict': 0.5}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:9
822,822,"Then many will stumble, and will deliver up one another, and will hate one another.",40024010,"ChatCompletion(id='chatcmpl-8dJpsMAID6qup7fqBrByX6Vx8PhiL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381732, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.2, 0.5, 0.2, 0.7, 0.2, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Health': 0.2, 'Insult': 0.5, 'Profanity': 0.2, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:10
823,823,"Many false prophets will arise, and will lead many astray.",40024011,"ChatCompletion(id='chatcmpl-8dJpsK70AHVRoJ5N6LNrxdUXZKMCG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381732, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.2, 0.5, 0.2, 0.1, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Health': 0.2, 'Insult': 0.5, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:11
824,824,"Because iniquity will be multiplied, the love of many will grow cold.",40024012,"ChatCompletion(id='chatcmpl-8dJptlO5jWrAmNSMP6EFMxmJXA9rd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381733, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.2, 0.1, 0.9, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:12
825,825,"But he who endures to the end, the same will be saved.",40024013,"ChatCompletion(id='chatcmpl-8dJptRUUujjQW32egS1zKbTKFFPdt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381733, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 2
  total_token_count: 319
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.1, 0.5, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:13
826,826,"This Gospel of the Kingdom will be preached in the whole world for a testimony to all the nations, and then the end will come.",40024014,"ChatCompletion(id='chatcmpl-8dJpuigdcPhw9J0IuJd5PA3eF0xtY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381734, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:14
827,827,"""When, therefore, you see the abomination of desolation, which was spoken of through Daniel the prophet, standing in the holy place (let the reader understand),",40024015,"ChatCompletion(id='chatcmpl-8dJpux6SVU6cawaImuHEr5Ct6IrRX', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381734, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.4, 0.1, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:15
828,828,then let those who are in Judea flee to the mountains.,40024016,"ChatCompletion(id='chatcmpl-8dJpv1J20hRg6YzGIcY3JmkDjmRe0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381735, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.4, 0.2, 0.1, 0.1, 1.0, 0.1, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.5, 'Health': 0.1, 'Insult': 0.4, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:16
829,829,Let him who is on the housetop not go down to take out things that are in his house.,40024017,"ChatCompletion(id='chatcmpl-8dJpvJ1MPsRY9c3D6IT4i8jOnBhJa', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381735, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.5, 0.3, 0.2, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:17
830,830,Let him who is in the field not return back to get his clothes.,40024018,"ChatCompletion(id='chatcmpl-8dJpwvaVnVH3UhoQ74ubYXUM5IOU6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381736, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.5, 0.3, 0.2, 0.7, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.3, 'Profanity': 0.2, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:18
831,831,But woe to those who are with child and to nursing mothers in those days!,40024019,"ChatCompletion(id='chatcmpl-8dJpwRrJlux8Wd1jjgis7QAXQQ1ds', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381736, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.4, 0.1, 1.0, 0.1, 0.4, 0.2, 0.2, 0.2, 0.4, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 1.0, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.4, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:19
832,832,"Pray that your flight will not be in the winter, nor on a Sabbath,",40024020,"ChatCompletion(id='chatcmpl-8dJpzYwbnWT6fn9GMeottrIj6DASr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381739, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:20
833,833,"for then there will be great oppression, such as has not been from the beginning of the world until now, no, nor ever will be.",40024021,"ChatCompletion(id='chatcmpl-8dJpzgTQNXZJwxwWFzI2iE9qHqYoT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381739, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.5, 0.1, 0.3, 0.2, 0.3, 0.2, 0.8, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.2, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:21
834,834,"Unless those days had been shortened, no flesh would have been saved. But for the sake of the chosen ones, those days will be shortened.",40024022,"ChatCompletion(id='chatcmpl-8dJq0tm26LtypL0IYgPOCZQNadv7X', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381740, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.7, 0.2, 0.2, 0.1, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.7, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:22
835,835,"""Then if any man tells you, 'Behold, here is the Christ,' or, 'There,' don't believe it.",40024023,"ChatCompletion(id='chatcmpl-8dJq0t6GNvMc1FoI5K1qlhhL9tLer', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381740, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.3, 0.1, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:23
836,836,"For there will arise false christs, and false prophets, and they will show great signs and wonders, so as to lead astray, if possible, even the chosen ones.",40024024,"ChatCompletion(id='chatcmpl-8dJq1qvNzIunZcPKDnobIpquinBSy', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381741, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.3, 0.2, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:24
837,837,"""Behold, I have told you beforehand.",40024025,"ChatCompletion(id='chatcmpl-8dJq109QOYX8aV0PJ1bqHYB9LhBhd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381741, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=306, total_tokens=307))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 312
  candidates_token_count: 1
  total_token_count: 313
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.2, 0.2, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:25
838,838,"If therefore they tell you, 'Behold, he is in the wilderness,' don't go out; 'Behold, he is in the inner chambers,' don't believe it.",40024026,"ChatCompletion(id='chatcmpl-8dJq4BU6BOnMOPXP2lAe0CIJF8Phf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381744, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 340
  candidates_token_count: 2
  total_token_count: 342
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 24:26
839,839,"For as the lightning comes forth from the east, and is seen even to the west, so will be the coming of the Son of Man.",40024027,"ChatCompletion(id='chatcmpl-8dJq4V9FN4pgWa9xnQrAVEpNdh3uH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381744, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:27
840,840,"For wherever the carcass is, there will the vultures{or, eagles} be gathered together.",40024028,"ChatCompletion(id='chatcmpl-8dJq5XFvVPM0cnwFI96YSftKrg5Cp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381745, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.1, 0.4, 0.3, 0.1, 0.2, 0.2, 0.1, 0.9, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:28
841,841,"But immediately after the oppression of those days, the sun will be darkened, the moon will not give its light, the stars will fall from the sky, and the powers of the heavens will be shaken;",40024029,"ChatCompletion(id='chatcmpl-8dJq5FQ4kj8STrkUVq4sJLXUFqctz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381745, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=338, total_tokens=339))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 1
  total_token_count: 345
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:29
842,842,"and then the sign of the Son of Man will appear in the sky. Then all the tribes of the earth will mourn, and they will see the Son of Man coming on the clouds of the sky with power and great glory.",40024030,"ChatCompletion(id='chatcmpl-8dJq6J9lu21zIMfPV0H0V8F0Zml0M', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381746, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=342, total_tokens=343))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 349
  candidates_token_count: 1
  total_token_count: 350
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:30
843,843,"He will send out his angels with a great sound of a trumpet, and they will gather together his chosen ones from the four winds, from one end of the sky to the other.",40024031,"ChatCompletion(id='chatcmpl-8dJq6dJTYghu4VRLwfHZFhpHd0YUm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381746, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 340
  candidates_token_count: 1
  total_token_count: 341
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:31
844,844,"""Now from the fig tree learn this parable. When its branch has now become tender, and puts forth its leaves, you know that the summer is near.",40024032,"ChatCompletion(id='chatcmpl-8dJq7t7Mp4BwzrV3AG59O1hin63XV', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381747, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:32
845,845,"Even so you also, when you see all these things, know that it is near, even at the doors.",40024033,"ChatCompletion(id='chatcmpl-8dJq7rn0E66VGvnjbDp1zjGli9Vvq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381747, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:33
846,846,"Most assuredly I tell you, this generation{The word for ""generation"" (genea) can also be translated as ""race.""} will not pass away, until all these things are accomplished.",40024034,"ChatCompletion(id='chatcmpl-8dJq7yqbmqYZqEg99xJNwFnlgBoCr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381747, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=337, total_tokens=338))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 343
  candidates_token_count: 1
  total_token_count: 344
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.5, 0.1, 0.3, 0.5, 0.2, 0.1, 0.9, 0.1, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:34
847,847,"Heaven and earth will pass away, but my words will not pass away.",40024035,"ChatCompletion(id='chatcmpl-8dJq8dKDcZfks8jQhrGAXwmLrGkd7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381748, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.5, 0.1, 0.2, 0.2, 0.9, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:35
848,848,"But no one knows of that day and hour, not even the angels of heaven, but my Father only.",40024036,"ChatCompletion(id='chatcmpl-8dJq9iFNT94BLkWjk0do7sUoevISc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381749, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:36
849,849,"""As the days of Noah were, so will be the coming of the Son of Man.",40024037,"ChatCompletion(id='chatcmpl-8dJq9F1TfledaPRhVk8wbKL248xbx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381749, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:37
850,850,"For as in those days which were before the flood they were eating and drinking, marrying and giving in marriage, until the day that Noah entered into the ark,",40024038,"ChatCompletion(id='chatcmpl-8dJqAfTomOuNjOgyOdIpvlS7ZX5hh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381750, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:38
851,851,"and they didn't know until the flood came, and took them all away, so will be the coming of the Son of Man.",40024039,"ChatCompletion(id='chatcmpl-8dJqAITpYG0SziIa0ixMIhFTQqz8x', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381750, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.3, 0.3, 0.1, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:39
852,852,Then two men will be in the field: one will be taken and one will be left;,40024040,"ChatCompletion(id='chatcmpl-8dJqBRUtq3JjHotNjBNSJZCNWnlqw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381751, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:40
853,853,"two women grinding at the mill, one will be taken and one will be left.",40024041,"ChatCompletion(id='chatcmpl-8dJqBXcDtnHi1UK2WBo9BwS7uGusp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381751, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.5, 0.6, 0.1, 0.4, 0.2, 0.2, 0.1, 0.7, 0.5, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.5, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.5, 'Toxic': 0.3, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:41
854,854,"Watch therefore, for you don't know in what hour your Lord comes.",40024042,"ChatCompletion(id='chatcmpl-8dJqCQiaJn2BcK2v5nHGcj4PnwOnP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381752, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:42
855,855,"But know this, that if the master of the house had known in what watch of the night the thief was coming, he would have watched, and would not have allowed his house to be broken into.",40024043,"ChatCompletion(id='chatcmpl-8dJqCxu3ylEheaVHnIfcHX3AJMAZi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381752, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=337, total_tokens=338))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 1
  total_token_count: 345
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.2, 0.1, 0.3, 0.5, 0.2, 0.1, 0.6, 0.9, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.6, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:43
856,856,"Therefore also be ready, for in an hour that you don't expect, the Son of Man will come.",40024044,"ChatCompletion(id='chatcmpl-8dJqD910xXQ4EpoG5vz9IfsIwlkua', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381753, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:44
857,857,"""Who then is the faithful and wise servant, whom his lord has set over his household, to give them their food in due season?",40024045,"ChatCompletion(id='chatcmpl-8dJqDJSjTsHcOYDZaoJRJkoETqR5O', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381753, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.6, 0.2, 0.2, 0.1, 0.9, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:45
858,858,Blessed is that servant whom his lord finds doing so when he comes.,40024046,"ChatCompletion(id='chatcmpl-8dJqEhabOVKfEuJUxjDMFtbiocUhT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381754, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:46
859,859,Most assuredly I tell you that he will set him over all that he has.,40024047,"ChatCompletion(id='chatcmpl-8dJqEdZqg727d70gquO8uoo6okoH2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381754, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 0.2, 0.9, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:47
860,860,"But if that evil servant should say in his heart, 'My lord is delaying his coming,'",40024048,"ChatCompletion(id='chatcmpl-8dJqF0Rhv9QxUFxcTC8dnYSdPfHz9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381755, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.4, 0.2, 0.1, 1.0, 0.1, 0.2], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:48
861,861,"and begins to beat his fellow-servants, and eat and drink with the drunken,",40024049,"ChatCompletion(id='chatcmpl-8dJqGNAgvnOgRdvDKnHfUhC85nwtP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381756, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 321
  total_token_count: 321
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.8, 'severity': 'HIGH', 'category': 'Dangerous Content'}, {'probabilityScore': 0.7, 'severityScore': 0.7, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.4, 0.3, 0.1, 0.7, 0.2, 0.3, 0.1, 0.6, 0.4, 0.5, 0.9, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.4, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.7, 'Legal': 0.2, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 0.6, 'Sexual': 0.4, 'Toxic': 0.5, 'Violent': 0.9, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:49
862,862,"the lord of that servant will come in a day when he doesn't expect it, and in an hour when he doesn't know it,",40024050,"ChatCompletion(id='chatcmpl-8dJqGyuFeo2kQ75oL7knBslhtsr80', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381756, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:50
863,863,"and will cut him in pieces, and appoint his portion with the hypocrites; there is where the weeping and grinding of teeth will be.",40024051,"ChatCompletion(id='chatcmpl-8dJqGnOkdQZAui5Jjk24A8GRDYdFR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381756, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 331
  total_token_count: 331
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.8, 'severity': 'HIGH', 'category': 'Dangerous Content'}, {'probabilityScore': 0.7, 'severityScore': 0.7, 'severity': 'HIGH', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.4, 0.6, 0.1, 0.7, 0.2, 0.3, 0.3, 0.1, 1.0, 0.3, 0.5, 0.9, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.4, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.7, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.5, 'Violent': 0.9, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 24:51
864,864,"""Then the Kingdom of Heaven will be like ten virgins, who took their lamps, and went out to meet the bridegroom.",40025001,"ChatCompletion(id='chatcmpl-8dJqHldSt9Ok7fwdmrsswCT1nfjgm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381757, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.6, 0.2, 0.2, 0.2, 1.0, 0.5, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.6, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.5, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:1
865,865,"Five of them were foolish, and five were wise.",40025002,"ChatCompletion(id='chatcmpl-8dJqHWp9W94j0d6VTzSh8LpFNNupM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381757, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=307, total_tokens=308))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 314
  total_token_count: 314
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.2, 0.6, 0.3, 0.1, 0.7, 0.1, 0.3], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.6, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.1, 'Toxic': 0.3}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:2
866,866,"Those who were foolish, when they took their lamps, took no oil with them,",40025003,"ChatCompletion(id='chatcmpl-8dJqIwFCZ3A5JH1PetEazJZjJxKaU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381758, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.1, 0.6, 0.1, 0.6, 0.3, 0.1, 0.9, 0.1, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.6, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:3
867,867,but the wise took oil in their vessels with their lamps.,40025004,"ChatCompletion(id='chatcmpl-8dJqIviUPVakucmJ7mQF2PXyXOwc8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381758, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=308, total_tokens=309))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.6, 0.1, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:4
868,868,"Now while the bridegroom delayed, they all slumbered and slept.",40025005,"ChatCompletion(id='chatcmpl-8dJqJsCmkVlqrWFaj1fr9W3YB6Ve0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381759, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.2, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:5
869,869,"But at midnight there was a cry, 'Behold! The bridegroom is coming! Come out to meet him!'",40025006,"ChatCompletion(id='chatcmpl-8dJqJRSHmoma1bNm41znNZJGxEPE2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381759, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.1, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.1, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:6
870,870,"Then all those virgins arose, and trimmed their lamps.",40025007,"ChatCompletion(id='chatcmpl-8dJqKRhp746cDzvfnRFuoGRPY2gOG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381760, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 314
  candidates_token_count: 1
  total_token_count: 315
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.5, 0.6, 0.3, 0.2, 1.0, 0.5, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.5, 'Health': 0.6, 'Insult': 0.3, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.5, 'Toxic': 0.3, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:7
871,871,"The foolish said to the wise, 'Give us some of your oil, for our lamps are going out.'",40025008,"ChatCompletion(id='chatcmpl-8dJqKEhivNjBO7JgIPa84J1fSWvzA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381760, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.6, 0.1, 0.5, 0.3, 0.1, 0.9, 0.2, 0.2], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:8
872,872,"But the wise answered, saying, 'What if there isn't enough for us and you? You go rather to those who sell, and buy for yourselves.'",40025009,"ChatCompletion(id='chatcmpl-8dJqLmrpl2iQ1hrQq7BETmWpR1iM9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381761, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.4, 0.2, 0.2, 0.1, 0.8, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.3, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:9
873,873,"While they went away to buy, the bridegroom came, and those who were ready went in with him to the marriage feast, and the door was shut.",40025010,"ChatCompletion(id='chatcmpl-8dJqL4AhsUM4Ga7B3zsSY0dHYtt1O', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381761, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.2, 0.2, 0.2, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:10
874,874,"Afterward the other virgins also came, saying, 'Lord, Lord, open to us.'",40025011,"ChatCompletion(id='chatcmpl-8dJqMD7DZ4k8XGqGiBLGoovrBcAAH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381762, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.5, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.6, 0.2, 0.1, 1.0, 0.5, 0.2], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.5, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:11
875,875,"But he answered, 'Most assuredly I tell you, I don't know you.'",40025012,"ChatCompletion(id='chatcmpl-8dJqMt8moeLKCrIjsDm89P22MsNc8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381762, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.3, 0.2, 0.1, 0.7, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:12
876,876,"Watch therefore, for you don't know the day nor the hour in which the Son of Man is coming.",40025013,"ChatCompletion(id='chatcmpl-8dJqSIFcBjn9AJjdWf6GthfQa99Nv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381768, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:13
877,877,"""For it is like a man, going into another country, who called his own servants, and entrusted his goods to them.",40025014,"ChatCompletion(id='chatcmpl-8dJqSCTwCB0xMYZAi8YTcsaMyZEYw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381768, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 0.2, 0.2, 0.1, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.3, 'Health': 0.2, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:14
878,878,"To one he gave five talents, to another two, to another one; to each according to his own ability. Then he went on his journey.",40025015,"ChatCompletion(id='chatcmpl-8dJqT5rlK0BQGsIEHSbiP35bHm9vE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381769, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:15
879,879,"Immediately he who received the five talents went and traded with them, and made another five talents.",40025016,"ChatCompletion(id='chatcmpl-8dJqTYtuhypEsoGeRD5UAC4G26koy', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381769, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.2, 'Health': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:16
880,880,In like manner he also who got the two gained another two.,40025017,"ChatCompletion(id='chatcmpl-8dJqUryAtZZb3zahNLHyTsAv3bMCi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381770, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.5, 0.1, 0.2, 0.2, 0.2, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.3, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:17
881,881,"But he who received the one went away and dug in the earth, and hid his lord's money.",40025018,"ChatCompletion(id='chatcmpl-8dJqUxclYjYaQWaRHU4UMDEZZi4U3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381770, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.9, 0.1, 0.3, 0.6, 0.1, 0.5, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.9, 'Health': 0.1, 'Insult': 0.3, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.5, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:18
882,882,"""Now after a long time the lord of those servants came, and reconciled accounts with them.",40025019,"ChatCompletion(id='chatcmpl-8dJqVkRvG94pjisiAjJXQaTQAb9AU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381771, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.6, 0.3, 0.1, 0.2, 0.2, 0.3, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.6, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:19
883,883,"He who received the five talents came and brought another five talents, saying, 'Lord, you delivered to me five talents. Behold, I have gained another five talents besides them.'",40025020,"ChatCompletion(id='chatcmpl-8dJqVdlH2ge88X6zb7RQz8vRRFhdE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381771, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:20
884,884,"""His lord said to him, 'Well done, good and faithful servant. You have been faithful over a few things, I will set you over many things. Enter into the joy of your lord.'",40025021,"ChatCompletion(id='chatcmpl-8dJqWF7aUTuNorin3K9g1RB1ora4x', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381772, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=337, total_tokens=338))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 1
  total_token_count: 345
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:21
885,885,"""He also who got the two talents came and said, 'Lord, you delivered to me two talents. Behold, I have gained another two talents besides them.'",40025022,"ChatCompletion(id='chatcmpl-8dJqWg76mrLn4uaqXBD6ebwjzp46K', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381772, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:22
886,886,"""His lord said to him, 'Well done, good and faithful servant. You have been faithful over a few things, I will set you over many things. Enter into the joy of your lord.'",40025023,"ChatCompletion(id='chatcmpl-8dJqXYpoXNJFImZiGpPNzpWdu5tff', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381773, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=337, total_tokens=338))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 1
  total_token_count: 345
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:23
887,887,"""He also who had received the one talent came and said, 'Lord, I knew you that you are a hard man, reaping where you did not sow, and gathering where you did not scatter.",40025024,"ChatCompletion(id='chatcmpl-8dJqXcEeOltASerVUmByrJ78ioEHC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381773, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=337, total_tokens=338))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 1
  total_token_count: 345
}
",0,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.1, 0.3, 0.2, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.1, 'Insult': 0.3, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 25:24
888,888,"I was afraid, and went away and hid your talent in the earth. Behold, you have what is yours.'",40025025,"ChatCompletion(id='chatcmpl-8dJqYfDk3VeufnGSeUXAHh3wcSxQs', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381774, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:25
889,889,"""But his lord answered him, 'You wicked and slothful servant. You knew that I reap where I didn't sow, and gather where I didn't scatter.",40025026,"ChatCompletion(id='chatcmpl-8dJqZmDkzp3veQnnyRNesUs6lCzJZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381775, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  total_token_count: 339
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.7, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.2, 0.7, 0.6, 0.3, 1.0, 0.2, 0.4], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.7, 'Politics': 0.6, 'Profanity': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:26
890,890,"You ought therefore to have deposited my money with the bankers, and at my coming I should have received back my own with interest.",40025027,"ChatCompletion(id='chatcmpl-8dJqZgJtOsnEeTN9qgMIibwSWI3pE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381775, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 1.0, 0.1, 0.2, 0.1, 0.2, 0.1, 0.1, 0.7, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 1.0, 'Health': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:27
891,891,"Take away therefore the talent from him, and give it to him who has the ten talents.",40025028,"ChatCompletion(id='chatcmpl-8dJqaVfED2Tw3T32xmEeE4ANSCYOp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381776, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.3, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:28
892,892,"For to everyone who has will be given, and he will have abundance, but from him who has not, even that which he has will be taken away.",40025029,"ChatCompletion(id='chatcmpl-8dJqaohtwssBisH4G2XJ3oqtkURm7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381776, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:29
893,893,"Throw out the unprofitable servant into the outer darkness, where there will be weeping and gnashing of teeth.'",40025030,"ChatCompletion(id='chatcmpl-8dJqb1la8Bi2Org4ZNcB8h6MvuYfc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381777, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.2, 0.1, 0.5, 0.2, 0.1, 0.9, 0.2, 0.3, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:30
894,894,"""But when the Son of Man comes in his glory, and all the holy angels with him, then he will sit on the throne of his glory.",40025031,"ChatCompletion(id='chatcmpl-8dJqbftA52kuLafnUobP71naMQ2OM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381777, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 1
  total_token_count: 335
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:31
895,895,"Before him all the nations will be gathered, and he will separate them one from another, as a shepherd separates the sheep from the goats.",40025032,"ChatCompletion(id='chatcmpl-8dJqcyGQ4Vl3SkAuEzjj0HfrxcbYu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381778, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.5, 0.2, 0.1, 0.3, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:32
896,896,"He will set the sheep on his right hand, but the goats on the left.",40025033,"ChatCompletion(id='chatcmpl-8dJqcxmmcXkYPgmhnBjZ9hsumaS6A', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381778, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:33
897,897,"Then the King will tell those on his right hand, 'Come, blessed of my Father, inherit the Kingdom prepared for you from the foundation of the world;",40025034,"ChatCompletion(id='chatcmpl-8dJqd060TgKB4rnAHVprBW96ASc6t', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381779, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.4, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:34
898,898,"for I was hungry, and you gave me food to eat; I was thirsty, and you gave me drink; I was a stranger, and you took me in;",40025035,"ChatCompletion(id='chatcmpl-8dJqdu8MGFOy0UtDnQVbtAxZIudWO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='50', role='assistant', function_call=None, tool_calls=None))], created=1704381779, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",50,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 2
  total_token_count: 339
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.6, 0.1, 0.2, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.6, 'Insult': 0.1, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:35
899,899,"naked, and you clothed me; I was sick, and you visited me; I was in prison, and you came to me.'",40025036,"ChatCompletion(id='chatcmpl-8dJqe3fy7reS957RxJRw644aGmtZG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381780, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  total_token_count: 330
}
",-1,"MultiCandidateTextGenerationResponse(text='', _prediction_response=Prediction(predictions=[{'content': '', 'citationMetadata': None, 'safetyAttributes': {'blocked': True, 'errors': [250.0]}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=True, errors=(250,), safety_attributes={}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[TextGenerationResponse(text='', is_blocked=True, errors=(250,), safety_attributes={}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]))])",-1,Matthew 25:36
900,900,"""Then the righteous will answer him, saying, 'Lord, when did we see you hungry, and feed you; or thirsty, and give you a drink?",40025037,"ChatCompletion(id='chatcmpl-8dJqe7ESsDd6PeEd7fxumWhC4OHqz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381780, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.6, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:37
901,901,"When did we see you as a stranger, and take you in; or naked, and clothe you?",40025038,"ChatCompletion(id='chatcmpl-8dJqfEZyWLE7y0YVcloO810Nr5yC8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381781, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  total_token_count: 324
}
",-1,"MultiCandidateTextGenerationResponse(text='', _prediction_response=Prediction(predictions=[{'content': '', 'citationMetadata': None, 'safetyAttributes': {'blocked': True, 'errors': [250.0]}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=True, errors=(250,), safety_attributes={}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[TextGenerationResponse(text='', is_blocked=True, errors=(250,), safety_attributes={}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]))])",-1,Matthew 25:38
902,902,"When did we see you sick, or in prison, and come to you?'",40025039,"ChatCompletion(id='chatcmpl-8dJqfFb4j5rc81OLZDdElVqDJpbgH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381781, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.2, 0.1, 0.9, 0.6, 0.4, 0.5, 0.2, 0.1, 0.5, 0.3, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.9, 'Illicit Drugs': 0.6, 'Insult': 0.4, 'Legal': 0.5, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.5, 'Religion & Belief': 0.3, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:39
903,903,"""The King will answer them, 'Most assuredly I tell you, inasmuch as you did it to one of the least of these my brothers{The word for ""brothers"" here may be also correctly translated ""brothers and sisters"" or ""siblings.""}, you did it to me.'",40025040,"ChatCompletion(id='chatcmpl-8dJqgjje3qgU5Lt7sKhwz51z6uOg9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381782, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=357, total_tokens=358))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 360
  candidates_token_count: 1
  total_token_count: 361
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.1, 0.2, 0.2, 0.1, 0.9, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:40
904,904,"Then he will say also to those on the left hand, 'Depart from me, you cursed, into the eternal fire which is prepared for the devil and his angels;",40025041,"ChatCompletion(id='chatcmpl-8dJqgAJRn9YUuzP9cPDtvZJxZ6JCa', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381782, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 1
  total_token_count: 338
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.5, 0.3, 1.0, 0.2, 0.3, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Health': 0.2, 'Insult': 0.5, 'Profanity': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:41
905,905,"for I was hungry, and you didn't give me food to eat; I was thirsty, and you gave me no drink;",40025042,"ChatCompletion(id='chatcmpl-8dJqhu3XsPIE6KMT6w1WxFBKx2MPA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381783, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.9, 0.2, 0.2, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.9, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:42
906,906,"I was a stranger, and you didn't take me in; naked, and you didn't clothe me; sick, and in prison, and you didn't visit me.'",40025043,"ChatCompletion(id='chatcmpl-8dJqhw7shx6VH68UCI5SThUT33CPG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381783, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",10,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 341
  total_token_count: 341
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.7, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.6, 0.3, 0.2, 0.2, 0.3, 0.1, 0.5, 0.7, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.3, 'Public Safety': 0.1, 'Religion & Belief': 0.5, 'Sexual': 0.7, 'Toxic': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:43
907,907,"""Then they will also answer, saying, 'Lord, when did we see you hungry, or thirsty, or a stranger, or naked, or sick, or in prison, and didn't help you?'",40025044,"ChatCompletion(id='chatcmpl-8dJqi8GmcDLWfvf7gZp9nKjxWg4OV', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381784, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=338, total_tokens=339))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 346
  candidates_token_count: 2
  total_token_count: 348
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.2, 0.1, 0.1, 0.9, 0.4, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Legal': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.4, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 25:44
908,908,"""Then he will answer them, saying, 'Most assuredly I tell you, inasmuch as you didn't do it to one of the least of these, you didn't do it to me.'",40025045,"ChatCompletion(id='chatcmpl-8dJqiC2DPn1pCLdC25WHOHk6OMS3B', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381784, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=338, total_tokens=339))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 1
  total_token_count: 345
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.5, 0.1, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:45
909,909,"These will go away into eternal punishment, but the righteous into eternal life.""",40025046,"ChatCompletion(id='chatcmpl-8dJqj1nrcJKz8OODmfk1AVzzOUZDv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381785, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.5, 0.1, 0.2, 0.1, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 25:46
910,910,"It happened, when Jesus had finished all these words, that he said to his disciples,",40026001,"ChatCompletion(id='chatcmpl-8dJqjigamlHszvu25olx1ito6L7oi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381785, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:1
911,911,"""You know that after two days the Passover is coming, and the Son of Man will be delivered up to be crucified.""",40026002,"ChatCompletion(id='chatcmpl-8dJqkhQhn8D35j6x2vbVgCeDI56aT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381786, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.2, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:2
912,912,"Then the chief priests, the scribes, and the elders of the people were gathered together in the court of the high priest, who was called Caiaphas.",40026003,"ChatCompletion(id='chatcmpl-8dJqkvFT2jZvOvyYTaO9jFbqiI5Rh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381786, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:3
913,913,"They took counsel together that they might take Jesus by deceit, and kill him.",40026004,"ChatCompletion(id='chatcmpl-8dJql3G5n6BcooN836xUHdJTcuALe', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381787, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: MEDIUM
    blocked: true
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: MEDIUM
    blocked: true
  }
}
usage_metadata {
  prompt_token_count: 319
  total_token_count: 319
}
",-1,"MultiCandidateTextGenerationResponse(text='', _prediction_response=Prediction(predictions=[{'content': '', 'citationMetadata': None, 'safetyAttributes': {'blocked': True, 'errors': [253.0]}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=True, errors=(253,), safety_attributes={}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[TextGenerationResponse(text='', is_blocked=True, errors=(253,), safety_attributes={}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]))])",-1,Matthew 26:4
914,914,"But they said, ""Not during the feast, lest a riot occur among the people.""",40026005,"ChatCompletion(id='chatcmpl-8dJqlon54GVokzhtj0OQf1CGal4pt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381787, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.5, 0.2, 0.2, 0.6, 0.1, 0.8, 0.1, 0.1, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.6, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:5
915,915,"Now when Jesus was in Bethany, in the house of Simon the leper,",40026006,"ChatCompletion(id='chatcmpl-8dJqmYD0Vh2Od1RRXPfMpbgMPsfJ2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381788, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:6
916,916,"a woman came to him having an alabaster jar of very expensive ointment, and she poured it on his head as he sat at the table.",40026007,"ChatCompletion(id='chatcmpl-8dJqmKODxN2ixI2He3HnLRCEnUFCM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381788, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.9, 0.1, 0.2, 0.2, 0.7, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.9, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.7, 'Sexual': 0.4, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:7
917,917,"But when his disciples saw this, they were indignant, saying, ""Why this waste?",40026008,"ChatCompletion(id='chatcmpl-8dJqnh96uOu278DjWe8Kow1YhvDag', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381789, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.4, 0.4, 0.2, 0.1, 1.0, 0.1, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.4, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:8
918,918,"For this ointment might have been sold for much, and given to the poor.""",40026009,"ChatCompletion(id='chatcmpl-8dJqnWBC1IdluJkzMZqHLp6Dqkg9X', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381789, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.6, 0.6, 0.1, 0.3, 0.1, 0.2, 0.1, 0.9, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.6, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:9
919,919,"But Jesus, knowing this, said to them, ""Why do you trouble the woman? Because she has done a good work for me.",40026010,"ChatCompletion(id='chatcmpl-8dJqoh4StHkL923F808Ue7GyG9pY6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381790, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.5, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.5, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:10
920,920,For you always have the poor with you; but you don't always have me.,40026011,"ChatCompletion(id='chatcmpl-8dJqp5MSFcbrXsx4KwpE9xjryAPOi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381791, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.5, 0.5, 0.2, 0.2, 0.4, 0.1, 0.3], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.5, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 0.4, 'Sexual': 0.1, 'Toxic': 0.3}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:11
921,921,"For in pouring this ointment on my body, she did it to prepare me for burial.",40026012,"ChatCompletion(id='chatcmpl-8dJqp41xRtdTrCQLMq1YBcLAD5zw0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381791, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [1.0, 0.2, 0.6, 0.1, 0.2, 0.1, 0.9, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 1.0, 'Derogatory': 0.2, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:12
922,922,"Most assuredly I tell you, wherever this Gospel is preached in the whole world, what this woman has done will also be spoken of as a memorial of her.""",40026013,"ChatCompletion(id='chatcmpl-8dJqqTcf8DAhUU73uk4aXP0m66x7R', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381792, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.2, 0.1, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Illicit Drugs', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:13
923,923,"Then one of the twelve, who was called Judas Iscariot, went to the chief priests,",40026014,"ChatCompletion(id='chatcmpl-8dJqruiufTUT0jYSKPwki5oXXi0C2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381793, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.1, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:14
924,924,"and said, ""What are you willing to give me, that I should deliver him to you?"" They weighed out for him thirty pieces of silver.",40026015,"ChatCompletion(id='chatcmpl-8dJqs4fBcugEiRWk8scWl6bY9SppX', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381794, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.1, 0.1, 0.2, 0.1, 0.1, 0.1, 0.7, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.6, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:15
925,925,From that time he sought opportunity to betray him.,40026016,"ChatCompletion(id='chatcmpl-8dJqsgoIUNqNL7CnCmNPXtJ6NQRjT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381794, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=306, total_tokens=307))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 313
  candidates_token_count: 1
  total_token_count: 314
}
",0,"MultiCandidateTextGenerationResponse(text=' 50', _prediction_response=Prediction(predictions=[{'content': ' 50', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.3, 0.4, 0.6, 0.1, 0.6, 0.6, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Finance': 0.3, 'Insult': 0.4, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.6, 'Religion & Belief': 0.6, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 50])",50,Matthew 26:16
926,926,"Now on the first day of unleavened bread, the disciples came to Jesus, saying to him, ""Where do you want us to prepare for you to eat the Passover?""",40026017,"ChatCompletion(id='chatcmpl-8dJquvMonz85r3dUo7o8ikuG6bEq5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381796, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.6, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:17
927,927,"He said, ""Go into the city to a certain person, and tell him, 'The Teacher says, ""My time is at hand. I will keep the Passover at your house with my disciples.""'""",40026018,"ChatCompletion(id='chatcmpl-8dJqvb0GhxQknm4hcsmPtXO3xpwoN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381797, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=339, total_tokens=340))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 345
  candidates_token_count: 1
  total_token_count: 346
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:18
928,928,"The disciples did as Jesus commanded them, and they prepared the Passover.",40026019,"ChatCompletion(id='chatcmpl-8dJqwmqhL7vckC6AU6hEDORgXdXNA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381798, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:19
929,929,"Now when evening had come, he was reclining at the table with the twelve disciples.",40026020,"ChatCompletion(id='chatcmpl-8dJqwuOZHE0Je0mk8xd2SLmf42Ual', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381798, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:20
930,930,"As they were eating, he said, ""Most assuredly I tell you that one of you will betray me.""",40026021,"ChatCompletion(id='chatcmpl-8dJqw8dsP26pxU1UkO5QjdBIYLPyu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381798, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.7, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.2, 0.5, 0.3, 0.1, 0.5, 1.0, 0.2, 0.3, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.5, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.5, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:21
931,931,"They were exceedingly sorrowful, and each began to ask him, ""It isn't me, is it, Lord?""",40026022,"ChatCompletion(id='chatcmpl-8dJqx6a3HPoi7PXShOTCMLFQY6hEp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381799, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:22
932,932,"He answered, ""He who dipped his hand with me in the dish, the same will betray me.",40026023,"ChatCompletion(id='chatcmpl-8dJqy6r40SKSEea6LFO652CM3SRxx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381800, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.1, 0.5, 0.1, 0.4, 0.1, 0.3, 0.2, 0.2, 0.9, 0.3, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.2, 'Public Safety': 0.2, 'Religion & Belief': 0.9, 'Sexual': 0.3, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:23
933,933,"The Son of Man goes, even as it is written of him, but woe to that man through whom the Son of Man is betrayed! It would be better for that man if he had not been born.""",40026024,"ChatCompletion(id='chatcmpl-8dJqy2kgFvuyiHRhQflZeTvvoTpTd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381800, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=339, total_tokens=340))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 345
  candidates_token_count: 1
  total_token_count: 346
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.5, 0.3, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:24
934,934,"Judas, who betrayed him, answered, ""It isn't me, is it, Rabbi?"" He said to him, ""You said it.""",40026025,"ChatCompletion(id='chatcmpl-8dJqzQnPrTDB2fFfvPa7cFP6yscI5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381801, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 0.3, 0.1, 0.4, 0.1, 0.6, 0.1, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:25
935,935,"As they were eating, Jesus took bread, gave thanks for{TR reads ""blessed"" instead of ""gave thanks for""} it, and broke it. He gave to the disciples, and said, ""Take, eat; this is my body.""",40026026,"ChatCompletion(id='chatcmpl-8dJqzpEit0eqJRt36KZF7LnJ4nJa7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381801, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=349, total_tokens=350))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 353
  candidates_token_count: 1
  total_token_count: 354
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.9, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.9, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:26
936,936,"He took the cup, gave thanks, and gave to them, saying, ""All of you drink it,",40026027,"ChatCompletion(id='chatcmpl-8dJr0hVyRrHzQyLJ9Bg2ygfRY5qn4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381802, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.6, 0.1, 0.2, 0.1, 0.8, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:27
937,937,"for this is my blood of the new covenant, which is poured out for many for the remission of sins.",40026028,"ChatCompletion(id='chatcmpl-8dJr0UNQ5ghxM3nVR0gFotBKGesLg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381802, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:28
938,938,"But I tell you that I will not drink of this fruit of the vine from now on, until that day when I drink it anew with you in my Father's Kingdom.""",40026029,"ChatCompletion(id='chatcmpl-8dJr1aphRwB9bKDLvaUsJmhU2yIck', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381803, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 339
  candidates_token_count: 1
  total_token_count: 340
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.1, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:29
939,939,"When they had sung a hymn, they went out to the Mount of Olives.",40026030,"ChatCompletion(id='chatcmpl-8dJr15iYTGgvHCEJP0m8oQDlep8By', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381803, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:30
940,940,"Then Jesus said to them, ""All of you will be made to stumble because of me tonight, for it is written, 'I will strike the shepherd, and the sheep of the flock will be scattered.'",40026031,"ChatCompletion(id='chatcmpl-8dJr2g1Ld7QN3hv6rwTgUsjWFHMSz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381804, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=338, total_tokens=339))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 345
  candidates_token_count: 2
  total_token_count: 347
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.4, 0.1, 1.0, 0.1, 0.3, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Health': 0.2, 'Insult': 0.4, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.3, 'Violent': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:31
941,941,"But after I am raised up, I will go before you into Galilee.""",40026032,"ChatCompletion(id='chatcmpl-8dJr2UoR7BzociCLgbpbPOHHhtMTn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381804, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:32
942,942,"But Peter answered him, ""Even if all will be made to stumble because of you, I will never be made to stumble.""",40026033,"ChatCompletion(id='chatcmpl-8dJr3XxDDm36wk54zrhMsDT6e0Ymq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381805, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:33
943,943,"Jesus said to him, ""Most assuredly I tell you that tonight, before the rooster crows, you will deny me three times.""",40026034,"ChatCompletion(id='chatcmpl-8dJr3PX3Pf5CISIN5fB1Qo9b7fCSd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381805, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 2
  total_token_count: 331
}
",10,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.3, 0.3, 0.2, 0.3, 0.1, 1.0, 0.3, 0.2, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.2, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:34
944,944,"Peter said to him, ""Even if I must die with you, I will not deny you."" All of the disciples also said likewise.",40026035,"ChatCompletion(id='chatcmpl-8dJr4Cdh7y5xFry29ORMvvz4IfQc4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381806, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.3, 0.3, 0.2, 0.1, 1.0, 0.2, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:35
945,945,"Then Jesus came with them to a place called Gethsemane, and said to his disciples, ""Sit here, while I go there and pray.""",40026036,"ChatCompletion(id='chatcmpl-8dJr4nUeJD4fAeeEIQHQP9uA0PJCf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381806, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 2
  total_token_count: 336
}
",20,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.1, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:36
946,946,"He took with him Peter and the two sons of Zebedee, and began to be sorrowful and severely troubled.",40026037,"ChatCompletion(id='chatcmpl-8dJr5YdRp9OtgxdZL0vax7l6gVRRs', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381807, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 2
  total_token_count: 328
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 26:37
947,947,"Then he said to them, ""My soul is exceedingly sorrowful, even to death. Stay here, and watch with me.""",40026038,"ChatCompletion(id='chatcmpl-8dJr52Meir6zXLMKee5QAq705cLI9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381807, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.1, 0.5, 0.2, 0.1, 0.9, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:38
948,948,"He went forward a little, fell on his face, and prayed, saying, ""My Father, if it is possible, let this cup pass away from me; nevertheless, not what I desire, but what you desire.""",40026039,"ChatCompletion(id='chatcmpl-8dJr6EjgBLIZy7PPxDJIWGcgKpM5f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381808, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=341, total_tokens=342))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 348
  candidates_token_count: 2
  total_token_count: 350
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.2, 0.3, 0.2, 0.1, 1.0, 0.3, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 26:39
949,949,"He came to the disciples, and found them sleeping, and said to Peter, ""What, couldn't you watch with me for one hour?",40026040,"ChatCompletion(id='chatcmpl-8dJr67eVIqhCbO2o4gVPXrW6PFZWc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381808, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 2
  total_token_count: 335
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 26:40
950,950,"Watch and pray, that you don't enter into temptation. The spirit indeed is willing, but the flesh is weak.""",40026041,"ChatCompletion(id='chatcmpl-8dJr7FhDR7FyCCSqzOHVGLBGfekt3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381809, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""50""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 2
  total_token_count: 330
}
",50,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:41
951,951,"Again, a second time he went away, and prayed, saying, ""My Father, if this cup can't pass away from me unless I drink it, your desire be done.""",40026042,"ChatCompletion(id='chatcmpl-8dJr7kgAVAR7MfXBosPdsZVehmk6z', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381809, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 341
  candidates_token_count: 2
  total_token_count: 343
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.5, 0.1, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 26:42
952,952,"He came again and found them sleeping, for their eyes were heavy.",40026043,"ChatCompletion(id='chatcmpl-8dJr8wAcLKU2JpGnMjNDBTQTpkeDf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381810, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.7, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.7, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:43
953,953,"He left them again, went away, and prayed a third time, saying the same words.",40026044,"ChatCompletion(id='chatcmpl-8dJr9haaN4lwt8BcjFv505eZAQHvL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381811, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:44
954,954,"Then he came to his disciples, and said to them, ""Sleep on now, and take your rest. Behold, the hour is at hand, and the Son of Man is betrayed into the hands of sinners.",40026045,"ChatCompletion(id='chatcmpl-8dJr9zPPW3GtuzFkvwY9cILgFWPNT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381811, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=341, total_tokens=342))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 346
  candidates_token_count: 1
  total_token_count: 347
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.3, 0.3, 0.1, 1.0, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:45
955,955,"Arise, let's be going. Behold, he who betrays me is at hand.""",40026046,"ChatCompletion(id='chatcmpl-8dJrA87RKiT8t9x0MSFzp0OjhRXQh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381812, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.1, 0.1, 0.3, 0.3, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:46
956,956,"While he was still speaking, behold, Judas, one of the twelve, came, and with him a great multitude with swords and clubs, from the chief priest and elders of the people.",40026047,"ChatCompletion(id='chatcmpl-8dJrAPQhczbAMCSHiyv7uKU6vRZXA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381812, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=335, total_tokens=336))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 341
  candidates_token_count: 1
  total_token_count: 342
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 0.2, 0.4, 0.2, 0.6, 0.2, 0.1, 1.0, 0.2, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:47
957,957,"Now he who betrayed him gave them a sign, saying, ""Whoever I kiss, he is the one. Seize him.""",40026048,"ChatCompletion(id='chatcmpl-8dJrBJcY3kAFszf90M39LgxSAoj5j', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381813, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.7, 'severity': 'HIGH', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.3, 0.1, 0.5, 0.2, 0.2, 0.1, 0.8, 0.4, 0.4, 0.5, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.3, 'Health': 0.1, 'Insult': 0.5, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.4, 'Toxic': 0.4, 'Violent': 0.5, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:48
958,958,"Immediately he came to Jesus, and said, ""Hail, Rabbi!"" and kissed him.",40026049,"ChatCompletion(id='chatcmpl-8dJrBmBZtrO3RqpuPvM1DUKzH4q9u', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381813, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.1, 1.0, 0.3, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:49
959,959,"Jesus said to him, ""Friend, why are you here?"" Then they came and laid hands on Jesus, and took him.",40026050,"ChatCompletion(id='chatcmpl-8dJrCqYUL1AKlUR7Or2AZ3F9D4DJI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381814, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:50
960,960,"Behold, one of those who were with Jesus stretched out his hand, and drew his sword, and struck the servant of the high priest, and struck off his ear.",40026051,"ChatCompletion(id='chatcmpl-8dJrCIljUuJnIyoya4DUGKKzeuuY8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381814, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 2
  total_token_count: 339
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.7, 0.3, 0.3, 0.4, 0.2, 0.2, 1.0, 0.1, 0.3, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.7, 'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.4, 'Politics': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.3, 'Violent': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 26:51
961,961,"Then Jesus said to him, ""Put your sword back into its place, for all those who take the sword will die by the sword.",40026052,"ChatCompletion(id='chatcmpl-8dJrD8ODQVviUGrFQkJmjrvHSKbDd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381815, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: MEDIUM
    blocked: true
  }
}
usage_metadata {
  prompt_token_count: 331
  total_token_count: 331
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.7, 'severityScore': 0.8, 'severity': 'HIGH', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.7, 0.4, 0.1, 0.4, 0.1, 0.1, 0.1, 1.0, 0.1, 0.4, 0.5, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.7, 'Derogatory': 0.4, 'Health': 0.1, 'Insult': 0.4, 'Legal': 0.1, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.4, 'Violent': 0.5, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:52
962,962,"Or do you think that I couldn't ask my Father, and he would even now send me more than twelve legions of angels?",40026053,"ChatCompletion(id='chatcmpl-8dJrD3ILQU2RAZPDibtUWjC0mJTHU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381815, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:53
963,963,"How then would the Scriptures be fulfilled that it must be so?""",40026054,"ChatCompletion(id='chatcmpl-8dJrE5S0GkmfzqoJ5CXB4sV55jJSf', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381816, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:54
964,964,"In that hour Jesus said to the multitudes, ""Have you come out as against a robber with swords and clubs to seize me? I sat daily in the temple teaching, and you didn't arrest me.",40026055,"ChatCompletion(id='chatcmpl-8dJrERB3MadPJKqSKMuD40V2Zdsc9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381816, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=338, total_tokens=339))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 345
  candidates_token_count: 1
  total_token_count: 346
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.1, 0.1, 0.4, 0.5, 0.6, 0.1, 0.6, 1.0, 0.2, 0.2, 0.1, 0.5], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.6, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.1, 'War & Conflict': 0.5}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:55
965,965,"But all this has happened, that the Scriptures of the prophets might be fulfilled."" Then all the disciples left him, and fled.",40026056,"ChatCompletion(id='chatcmpl-8dJrFxo6z3H0tf3WqDSPNLQa2FfNd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381817, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 0.3, 0.1, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Health': 0.1, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:56
966,966,"Those who had taken Jesus led him away to Caiaphas the high priest, where the scribes and the elders were gathered together.",40026057,"ChatCompletion(id='chatcmpl-8dJrFneOAbkhf19NejDHPTHwDbeYY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381817, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.3, 0.2, 0.3, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:57
967,967,"But Peter followed him from a distance, to the court of the high priest, and entered in and sat with the officers, to see the end.",40026058,"ChatCompletion(id='chatcmpl-8dJrGCnrq73sh7CpSAEU22IpQw82j', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381818, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.3, 0.1, 0.2, 0.5, 0.3, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.5, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:58
968,968,"Now the chief priests, the elders, and the whole council sought false testimony against Jesus, that they might put him to death;",40026059,"ChatCompletion(id='chatcmpl-8dJrGAVFoqGQeU3BgJ7YGxc19mcj8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381818, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.7, 'severityScore': 0.8, 'severity': 'HIGH', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.4, 0.2, 0.1, 0.5, 0.2, 0.3, 0.2, 0.3, 1.0, 0.2, 0.4, 0.4, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.4, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.2, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.4, 'Violent': 0.4, 'War & Conflict': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 26:59
969,969,"and they found none. Even though many false witnesses came forward, they found none. But at last two false witnesses came forward,",40026060,"ChatCompletion(id='chatcmpl-8dJrGBqn88oAz65vP0B7LGG3p5ssw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381818, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.1, 0.3, 0.5, 0.2, 0.6, 0.7, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.2, 'Public Safety': 0.6, 'Religion & Belief': 0.7, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:60
970,970,"and said, ""This man said, 'I am able to destroy the temple of God, and to build it in three days.'""",40026061,"ChatCompletion(id='chatcmpl-8dJrHRCzcTwcG6dXI0DG2TwP9cSkk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381819, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.4, 0.1, 0.4, 0.2, 0.2, 0.1, 1.0, 0.1, 0.3, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.4, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.3, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:61
971,971,"The high priest stood up, and said to him, ""Have you no answer? What is this that these testify against you?""",40026062,"ChatCompletion(id='chatcmpl-8dJrH6AurY0I8MCpED6pT0bXRTgnH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381819, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.3, 0.1, 0.3, 0.5, 0.6, 0.1, 0.3, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:62
972,972,"But Jesus held his peace. The high priest answered him, ""I adjure you by the living God, that you tell us whether you are the Christ, the Son of God.""",40026063,"ChatCompletion(id='chatcmpl-8dJrIiWdtUbNAOD3TjUosn4IUCjfR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381820, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 340
  candidates_token_count: 1
  total_token_count: 341
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:63
973,973,"Jesus said to him, ""You have said it. Nevertheless, I tell you, henceforth you will see the Son of Man sitting at the right hand of Power, and coming on the clouds of the sky.""",40026064,"ChatCompletion(id='chatcmpl-8dJrIDgwtV7arb50oGyWb4Xdbx9RK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381820, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=339, total_tokens=340))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 345
  candidates_token_count: 1
  total_token_count: 346
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.1, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:64
974,974,"Then the high priest tore his clothing, saying, ""He has spoken blasphemy! Why do we need any more witnesses? Behold, now you have heard his blasphemy.",40026065,"ChatCompletion(id='chatcmpl-8dJrJZIzyg4BgE4AcwM7ZywQnFuJY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381821, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.1, 0.5, 0.2, 0.6, 0.2, 0.1, 1.0, 0.1, 0.3, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.4, 'Health': 0.1, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.3, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:65
975,975,"What do you think?"" They answered, ""He is worthy of death!""",40026066,"ChatCompletion(id='chatcmpl-8dJrJN7BZ7MwTbpQKsxBJVeUoj9Sy', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381821, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [1.0, 0.4, 0.4, 0.1, 0.5, 0.2, 0.1, 0.8, 0.1, 0.5, 0.8, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 1.0, 'Derogatory': 0.4, 'Health': 0.4, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.5, 'Violent': 0.8, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:66
976,976,"Then they spit in his face and beat him with their fists, and some slapped him,",40026067,"ChatCompletion(id='chatcmpl-8dJrKMx7nJlnhOshLwsiKrAidTtGm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381822, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 2
  total_token_count: 323
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.7, 'severityScore': 0.7, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.6, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.1, 0.1, 0.6, 0.2, 0.2, 0.4, 0.5, 0.6, 0.4, 0.5, 0.9], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.4, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.6, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.4, 'Public Safety': 0.5, 'Religion & Belief': 0.6, 'Sexual': 0.4, 'Toxic': 0.5, 'Violent': 0.9}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 26:67
977,977,"saying, ""Prophesy to us, you Christ! Who hit you?""",40026068,"ChatCompletion(id='chatcmpl-8dJrKwaJCuTo0ozEglzskoc8nhqSg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381822, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.7, 0.3, 0.2, 0.1, 0.4, 0.2, 0.1, 0.1, 1.0, 0.2, 0.4, 0.5], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.7, 'Derogatory': 0.3, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.4, 'Violent': 0.5}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:68
978,978,"Now Peter was sitting outside in the court, and a maid came to him, saying, ""You were also with Jesus, the Galilean!""",40026069,"ChatCompletion(id='chatcmpl-8dJrLZEGwEhDi82QAGlNOyJpnmkZi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381823, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:69
979,979,"But he denied it before them all, saying, ""I don't know what you are talking about.""",40026070,"ChatCompletion(id='chatcmpl-8dJrLHO729cjZv6GVIzqDGaEbSBYc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381823, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.5, 0.1, 0.3, 0.2, 0.6, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:70
980,980,"When he had gone out onto the porch, someone else saw him, and said to those who were there, ""This man also was with Jesus of Nazareth.""",40026071,"ChatCompletion(id='chatcmpl-8dJrMH0fOL8bdKKHng2HRBy5YsBLk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381824, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 335
  candidates_token_count: 1
  total_token_count: 336
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:71
981,981,"Again he denied it with an oath, ""I don't know the man.""",40026072,"ChatCompletion(id='chatcmpl-8dJrMvp8F1DSOodQ0hFnJaORBlucs', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381824, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.2, 0.1, 0.3, 0.8, 0.6, 0.1, 0.3, 1.0, 0.2, 0.1, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.8, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:72
982,982,"After a little while those who stood by came and said to Peter, ""Surely you are also one of them, for your speech makes you known.""",40026073,"ChatCompletion(id='chatcmpl-8dJrNSPd57rr3yjzAtKMSVdto8lPd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381825, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.2, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:73
983,983,"Then he began to curse and to swear, ""I don't know the man!"" Immediately the rooster crowed.",40026074,"ChatCompletion(id='chatcmpl-8dJrNXBUpRDW5GjTi2Yyu6RNvN7QC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381825, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.4, 0.4, 0.1, 0.2, 0.2, 0.1, 0.9, 0.1, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.4, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 26:74
984,984,"Peter remembered the word which Jesus had said to him, ""Before the rooster crows, you will deny me three times."" He went out and wept bitterly.",40026075,"ChatCompletion(id='chatcmpl-8dJrO4V1ntyr634nL4noOhcOg768b', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381826, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 334
  candidates_token_count: 2
  total_token_count: 336
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.3, 0.1, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.3, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 26:75
985,985,"Now when morning had come, all the chief priests and the elders of the people took counsel against Jesus to put him to death:",40027001,"ChatCompletion(id='chatcmpl-8dJrOmdYuod1ADiOVbn4ZYaELHoGI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381826, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.7, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.9, 0.4, 0.2, 0.1, 0.5, 0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.5, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.9, 'Derogatory': 0.4, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.5, 'Violent': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:1
986,986,"and they bound him, and led him away, and delivered him up to Pontius Pilate, the governor.",40027002,"ChatCompletion(id='chatcmpl-8dJrPLOBGtOkdwtybnCY4OqO5XpQi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381827, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=319, total_tokens=320))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.1, 0.4, 0.5, 0.6, 0.1, 0.3, 1.0, 0.2, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:2
987,987,"Then Judas, who betrayed him, when he saw that Jesus was condemned, felt remorse, and brought back the thirty pieces of silver to the chief priests and elders,",40027003,"ChatCompletion(id='chatcmpl-8dJrPgecRWKaNKxeH3TfATslVwkP9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381827, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.1, 0.1, 0.3, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.2, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 27:3
988,988,"saying, ""I have sinned in that I betrayed innocent blood."" But they said, ""What is that to us? You see to it.""",40027004,"ChatCompletion(id='chatcmpl-8dJrQlwpFy1iGb9vv6WNARF7C6L49', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381828, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.2, 0.5, 0.1, 0.2, 0.2, 0.3, 0.1, 0.1, 1.0, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Finance': 0.2, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:4
989,989,"He threw down the pieces of silver in the sanctuary, and departed. He went away and hanged himself.",40027005,"ChatCompletion(id='chatcmpl-8dJrQZ0YaMfqkG0mpqOpX9xhqPNAc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381828, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [1.0, 0.3, 0.1, 0.1, 0.5, 0.2, 0.3, 0.3, 1.0, 0.2, 0.4, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 1.0, 'Derogatory': 0.3, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.2, 'Profanity': 0.3, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.4, 'Violent': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:5
990,990,"The chief priests took the pieces of silver, and said, ""It's not lawful to put them into the treasury, since it is the price of blood.""",40027006,"ChatCompletion(id='chatcmpl-8dJrQ6w8cGsD4DFtnaDbG5B02ImL1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381828, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 1
  total_token_count: 337
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.8, 0.6, 0.1, 0.2, 0.5, 0.4, 0.1, 0.3, 1.0, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.8, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.5, 'Politics': 0.4, 'Profanity': 0.1, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:6
991,991,"They took counsel, and bought the potter's field with them, to bury strangers in.",40027007,"ChatCompletion(id='chatcmpl-8dJrRxhCkhjMLgioggKqeE4SzCOEl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381829, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [1.0, 0.5, 0.1, 0.1, 0.5, 0.5, 0.6, 0.2, 0.6, 0.9, 0.2, 0.3, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 1.0, 'Derogatory': 0.5, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.6, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.2, 'War & Conflict': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:7
992,992,"Therefore that field was called ""The Field of Blood"" to this day.",40027008,"ChatCompletion(id='chatcmpl-8dJrSgKw6XLQCZB7jhHut7dQK7yar', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381830, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.1, 0.7, 0.1, 0.2, 0.1, 0.2, 0.1, 0.8, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Finance': 0.1, 'Health': 0.7, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:8
993,993,"Then that which was spoken through Jeremiah the prophet was fulfilled, saying, ""They took the thirty pieces of silver, The price of him upon whom a price had been set, Whom some of the children of Israel priced,",40027009,"ChatCompletion(id='chatcmpl-8dJrSVqwAIqdGxZ7H6HnZOAGu522s', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381830, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=341, total_tokens=342))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 347
  candidates_token_count: 1
  total_token_count: 348
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.7, 0.4, 0.2, 0.1, 0.3, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.7, 'Health': 0.4, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:9
994,994,"And they gave them for the potter's field, As the Lord commanded me.""",40027010,"ChatCompletion(id='chatcmpl-8dJrSv7qv5C3HNbViZFfbzyzf9Stb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381830, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:10
995,995,"Now Jesus stood before the governor: and the governor asked him, saying, ""Are you the King of the Jews?"" Jesus said to him, ""So you say.""",40027011,"ChatCompletion(id='chatcmpl-8dJrTsWHq9ArPiXPwtsE0qYRL31mp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381831, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 1
  total_token_count: 338
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.1, 0.4, 0.3, 0.2, 0.4, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.4, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:11
996,996,"When he was accused by the chief priests and elders, he answered nothing.",40027012,"ChatCompletion(id='chatcmpl-8dJrT4u5EOSVoXnCbtHBMBHoigHhM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381831, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.2, 0.1, 0.4, 0.5, 0.6, 0.1, 0.6, 1.0, 0.1, 0.1, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.6, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:12
997,997,"Then Pilate said to him, ""Don't you hear how many things they testify against you?""",40027013,"ChatCompletion(id='chatcmpl-8dJrUjKeeindH8tuBY6gsI407EMPb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381832, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.1, 0.1, 0.3, 0.5, 0.6, 0.1, 0.3, 1.0, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:13
998,998,"He gave him no answer, not even one word, so that the governor marveled greatly.",40027014,"ChatCompletion(id='chatcmpl-8dJrU97PEBQobpDwJoZ72EX7pdVXO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381832, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.3, 0.4, 0.2, 0.2, 0.6, 0.1, 0.1, 0.9, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.3, 'Health': 0.4, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:14
999,999,"Now at the feast the governor was accustomed to release to the multitude one prisoner, whom they desired.",40027015,"ChatCompletion(id='chatcmpl-8dJrVPTPDXVsKFVYgpf64r0Xb3lW4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381833, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.2, 0.1, 0.3, 0.5, 0.4, 0.1, 0.6, 0.9, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.4, 'Profanity': 0.1, 'Public Safety': 0.6, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:15
1000,1000,"They had then a notable prisoner, called Barabbas.",40027016,"ChatCompletion(id='chatcmpl-8dJrWa5hBNhaKmIiPgMUVXVSdJvtI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381834, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=308, total_tokens=309))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.1, 0.1, 0.1, 0.4, 0.5, 0.6, 0.1, 0.6, 1.0, 0.2, 0.1, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.6, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:16
1001,1001,"When therefore they were gathered together, Pilate said to them, ""Whom do you want me to release to you? Barabbas, or Jesus, who is called Christ?""",40027017,"ChatCompletion(id='chatcmpl-8dJrWtFlk6smPLLRa4PPtydWbITIv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381834, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=333, total_tokens=334))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.1, 0.3, 0.5, 0.6, 0.1, 0.1, 1.0, 0.3, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:17
1002,1002,For he knew that because of envy they had delivered him up.,40027018,"ChatCompletion(id='chatcmpl-8dJrXWIoDogTnhM7Xw0lpc1xhqsjB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381835, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.1, 0.3, 0.2, 0.4, 0.1, 0.8, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.2, 'Health': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.4, 'Profanity': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:18
1003,1003,"While he was sitting on the judgment seat, his wife sent to him, saying, ""Have nothing to do with that righteous man, for I have suffered many things this day in a dream because of him.""",40027019,"ChatCompletion(id='chatcmpl-8dJrXO8LXE1WF9H1RT0t57IsyVhO5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381835, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=338, total_tokens=339))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 345
  candidates_token_count: 1
  total_token_count: 346
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.3, 0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:19
1004,1004,"Now the chief priests and the elders persuaded the multitudes to ask for Barabbas, and destroy Jesus.",40027020,"ChatCompletion(id='chatcmpl-8dJrYlzmPOzGhntPIyvNBeX4Tivuo', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381836, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 0.4, 0.1, 0.6, 0.2, 0.1, 1.0, 0.1, 0.3, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.4, 'Health': 0.2, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.3, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:20
1005,1005,"But the governor answered them, ""Which of the two do you want me to release to you?"" They said, ""Barabbas!""",40027021,"ChatCompletion(id='chatcmpl-8dJrYpzQD23FXfYLtThnII8K8zPM8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381836, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.4, 0.2, 0.2, 0.1, 0.4, 0.5, 0.9, 0.1, 0.5, 1.0, 0.2, 0.2, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.4, 'Finance': 0.2, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.5, 'Politics': 0.9, 'Profanity': 0.1, 'Public Safety': 0.5, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:21
1006,1006,"Pilate said to them, ""What then shall I do to Jesus, who is called Christ?"" They all said to him, ""Let him be crucified!""",40027022,"ChatCompletion(id='chatcmpl-8dJrZx8tbmNq8Y985HucW5ljmL5oc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381837, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",0,"candidates {
  content {
    role: ""model""
  }
  finish_reason: SAFETY
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: MEDIUM
    blocked: true
  }
}
usage_metadata {
  prompt_token_count: 334
  total_token_count: 334
}
",-1,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.7, 'severityScore': 0.7, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.4, 0.1, 0.5, 0.2, 0.2, 0.1, 1.0, 0.2, 0.4, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.4, 'Health': 0.1, 'Insult': 0.5, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.4, 'Violent': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:22
1007,1007,"But the governor said, ""Why? What evil has he done?"" But they cried out exceedingly, saying, ""Let him be crucified!""",40027023,"ChatCompletion(id='chatcmpl-8dJrZL7fxYD0WjSyMSNftpi4BtvgF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381837, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.6, 'severityScore': 0.7, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.4, 0.1, 0.1, 0.5, 0.3, 0.2, 0.1, 1.0, 0.2, 0.4, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.4, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Politics': 0.3, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.4, 'Violent': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:23
1008,1008,"So when Pilate saw that nothing was being gained, but rather that a disturbance was starting, he took water, and washed his hands before the multitude, saying, ""I am innocent of the blood of this righteous person. You see to it.""",40027024,"ChatCompletion(id='chatcmpl-8dJraCPQuTdk9dLxiC6hsInERLI5w', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381838, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=346, total_tokens=347))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 352
  candidates_token_count: 1
  total_token_count: 353
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.3, 0.2, 0.5, 0.6, 0.1, 0.1, 1.0, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:24
1009,1009,"All the people answered, ""May his blood be on us, and on our children!""",40027025,"ChatCompletion(id='chatcmpl-8dJrauFiXQU63vn8TwltsNaKlB4hy', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381838, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.6, 0.1, 0.3, 0.1, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:25
1010,1010,"Then he released to them Barabbas, but Jesus he flogged and delivered to be crucified.",40027026,"ChatCompletion(id='chatcmpl-8dJrbHH8n0n7Dbw4T0Sb2eAQwwugC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381839, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.4, 0.1, 0.1, 0.5, 0.2, 0.2, 0.2, 0.1, 1.0, 0.2, 0.3, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.4, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:26
1011,1011,"Then the governor's soldiers took Jesus into the Praetorium, and gathered the whole garrison together against him.",40027027,"ChatCompletion(id='chatcmpl-8dJrbAr1P85hUZjlAFMupkFvOsAON', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381839, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.3, 0.2, 0.6, 0.1, 0.1, 1.0, 0.1, 0.2, 0.1, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.2, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.1, 'War & Conflict': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:27
1012,1012,"They stripped him, and put a scarlet robe on him.",40027028,"ChatCompletion(id='chatcmpl-8dJrcjFJ2x3xIWzwu8YHHUJEHv8gA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381840, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 315
  candidates_token_count: 1
  total_token_count: 316
}
",0,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.3, 0.3, 0.2, 0.2, 1.0, 0.4, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Health': 0.3, 'Insult': 0.3, 'Profanity': 0.2, 'Public Safety': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.4, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 27:28
1013,1013,"They braided a crown of thorns and put it on his head, and a reed in his right hand; and they kneeled down before him, and mocked him, saying, ""Hail, King of the Jews!""",40027029,"ChatCompletion(id='chatcmpl-8dJrcAu9cQXovdUNPIXGyAnq2LtBb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381840, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=343, total_tokens=344))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""10""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 346
  candidates_token_count: 2
  total_token_count: 348
}
",10,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.6, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.1, 0.3, 0.5, 0.2, 0.2, 0.1, 1.0, 0.2, 0.3, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.6, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.5, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 27:29
1014,1014,"They spat on him, and took the reed and struck him on the head.",40027030,"ChatCompletion(id='chatcmpl-8dJrdww6NG3QxWBe6xqIe7VFlGHWT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381841, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.3, 0.5, 0.4, 0.1, 0.2, 0.1, 0.8, 0.3, 0.4, 0.5], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.3, 'Health': 0.5, 'Insult': 0.4, 'Legal': 0.1, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.3, 'Toxic': 0.4, 'Violent': 0.5}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:30
1015,1015,"When they had mocked him, they took the robe off of him, and put his clothes on him, and led him away to crucify him.",40027031,"ChatCompletion(id='chatcmpl-8dJrdz4BkfUdio7ZRkUfOkLGujbql', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381841, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 2
  total_token_count: 335
}
",20,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.7, 'severity': 'HIGH', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.4, 0.5, 0.1, 0.2, 0.3, 0.5, 1.0, 0.2, 0.4, 0.4, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.4, 'Insult': 0.5, 'Legal': 0.1, 'Politics': 0.2, 'Profanity': 0.3, 'Public Safety': 0.5, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.4, 'Violent': 0.4, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 27:31
1016,1016,"As they came out, they found a man of Cyrene, Simon by name, and they compelled him to go with them, that he might carry his cross.",40027032,"ChatCompletion(id='chatcmpl-8dJreds1adfTdjENrUQX8vDzPleUW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381842, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""20""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 336
  candidates_token_count: 2
  total_token_count: 338
}
",20,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.1, 0.1, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.1, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 27:32
1017,1017,"They came to a place called ""Golgotha,"" that is to say, ""The place of a skull.""",40027033,"ChatCompletion(id='chatcmpl-8dJrewAByHOmEt6lQ3dVOhh8dEKuN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381842, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.2, 0.5, 0.3, 0.1, 0.2, 0.1, 1.0, 0.2, 0.2], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.3, 'Legal': 0.1, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:33
1018,1018,"They gave him sour wine to drink mixed with gall. When he had tasted it, he would not drink.",40027034,"ChatCompletion(id='chatcmpl-8dJrfkNneok6wfWuLs0Dl18YixoPz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381843, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 325
  candidates_token_count: 1
  total_token_count: 326
}
",0,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.9, 0.6, 0.3, 0.2, 0.2, 0.1, 0.4, 0.3, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.2, 'Health': 0.9, 'Illicit Drugs': 0.6, 'Insult': 0.3, 'Legal': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.4, 'Sexual': 0.3, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 27:34
1019,1019,"When they had crucified him, they divided his clothing among them, casting lots,{TR adds ""that it might be fulfilled which was spoken by the prophet: 'They divided my garments among them, and for my clothing they cast lots;'"" [see Psalm 22:18 and John 19:24]}",40027035,"ChatCompletion(id='chatcmpl-8dJrgx55VHqbDmvzH0aZuOk2qytBS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381844, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=359, total_tokens=360))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 369
  candidates_token_count: 1
  total_token_count: 370
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.3, 0.2, 0.1, 0.3, 0.2, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.3, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:35
1020,1020,and they sat and watched him there.,40027036,"ChatCompletion(id='chatcmpl-8dJrgrJZ8WSOeJgBzWg0dJNCOeZnB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381844, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=304, total_tokens=305))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 311
  candidates_token_count: 1
  total_token_count: 312
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.6, 0.2, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.6, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:36
1021,1021,"They set up over his head the accusation against him written, ""THIS IS JESUS, THE KING OF THE JEWS.""",40027037,"ChatCompletion(id='chatcmpl-8dJrh66p8Anhzx0DdkcNMzUffg7ql', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381845, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.6, 'severityScore': 0.7, 'severity': 'HIGH', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.6, 0.1, 0.2, 0.1, 0.5, 0.5, 0.3, 0.3, 0.3, 1.0, 0.2, 0.4, 0.1, 0.5], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.6, 'Finance': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.5, 'Politics': 0.3, 'Profanity': 0.3, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.4, 'Violent': 0.1, 'War & Conflict': 0.5}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:37
1022,1022,"Then there were two robbers crucified with him, one on his right hand and one on the left.",40027038,"ChatCompletion(id='chatcmpl-8dJri8odMCsVB42L135C8SgzdJAiO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381846, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: LOW
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.3, 0.2, 0.1, 0.4, 0.5, 0.1, 0.1, 0.6, 1.0, 0.2, 0.3, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.3, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.5, 'Politics': 0.1, 'Profanity': 0.1, 'Public Safety': 0.6, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.3, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:38
1023,1023,"Those who passed by blasphemed him, wagging their heads,",40027039,"ChatCompletion(id='chatcmpl-8dJri2X9jFwRhm0fvBjzARTnCn9jk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381846, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.7, 0.5, 0.1, 0.1, 0.5, 0.2, 0.6, 0.1, 0.1, 1.0, 0.1, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.7, 'Derogatory': 0.5, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:39
1024,1024,"and saying, ""You who destroy the temple, and build it in three days, save yourself! If you are the Son of God, come down from the cross!""",40027040,"ChatCompletion(id='chatcmpl-8dJritKcL9akJzSCIV4ADEdX3PcOo', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381846, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=330, total_tokens=331))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 1
  total_token_count: 338
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.3, 0.1, 0.3, 0.1, 1.0, 0.1, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.3, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:40
1025,1025,"Likewise the chief priests also mocking, with the scribes, the Pharisees,{TR omits ""the Pharisees""} and the elders, said,",40027041,"ChatCompletion(id='chatcmpl-8dJrjLh07wLCveHI2aX8LBgm8SBP6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381847, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=329, total_tokens=330))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.4, 0.3, 0.4, 0.1, 0.6, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.4, 'Health': 0.3, 'Insult': 0.4, 'Legal': 0.1, 'Politics': 0.6, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:41
1026,1026,"""He saved others, but he can't save himself. If he is the King of Israel, let him come down from the cross now, and we will believe in him.",40027042,"ChatCompletion(id='chatcmpl-8dJrkEWAdifT7QZvEWiznwujVJze6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381848, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 340
  candidates_token_count: 1
  total_token_count: 341
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.4, 0.2, 0.3, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:42
1027,1027,"He trusts in God. Let God deliver him now, if he wants him; for he said, 'I am the Son of God.'""",40027043,"ChatCompletion(id='chatcmpl-8dJrlyDyQGD762v08UHNl46fxCrMc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381849, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.3, 'Insult': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:43
1028,1028,The robbers also who were crucified with him cast on him the same reproach.,40027044,"ChatCompletion(id='chatcmpl-8dJrl3kOqZoHbcfRMcpA6dCoTZgez', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381849, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.7, 0.4, 0.1, 0.1, 0.5, 0.2, 0.3, 0.1, 0.6, 1.0, 0.1, 0.3, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.7, 'Derogatory': 0.4, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.6, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.3, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:44
1029,1029,Now from the sixth hour{noon} there was darkness over all the land until the ninth hour.{3:00 P. M.},40027045,"ChatCompletion(id='chatcmpl-8dJrm2EGAqhx5rAi3VSjphHTGK9t8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381850, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.5, 0.1, 0.1, 0.1, 0.9, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Health': 0.5, 'Insult': 0.1, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:45
1030,1030,"About the ninth hour Jesus cried with a loud voice, saying, ""Eli, Eli, lima{TR reads ""lama"" instead of ""lima""} sabachthani?"" That is, ""My God, my God, why have you forsaken me?""",40027046,"ChatCompletion(id='chatcmpl-8dJrmXdQuwT9HbgIUl36vrKXxOGWt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381850, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=351, total_tokens=352))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 354
  candidates_token_count: 1
  total_token_count: 355
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.1, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:46
1031,1031,"Some of them who stood there, when they heard it, said, ""This man is calling Elijah.""",40027047,"ChatCompletion(id='chatcmpl-8dJrn0VZrRRBVsdg2EhXKNDARuiko', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381851, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=317, total_tokens=318))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:47
1032,1032,"Immediately one of them ran, and took a sponge, and filled it with vinegar, and put it on a reed, and gave him a drink.",40027048,"ChatCompletion(id='chatcmpl-8dJrnLnoTNHg2ls8V2W8sBF6TqFqa', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381851, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.6, 0.1, 0.2, 0.2, 0.1, 0.8, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.6, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:48
1033,1033,"The rest said, ""Let him be. Let's see whether Elijah comes to save him.""",40027049,"ChatCompletion(id='chatcmpl-8dJrnZe9Y0hnKI7pRo8hUctapjWtp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381851, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.4, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:49
1034,1034,"Jesus cried again with a loud voice, and yielded up his spirit.",40027050,"ChatCompletion(id='chatcmpl-8dJroCCsgaoG2YEPINzIxXEQa4c2s', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381852, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:50
1035,1035,"Behold, the veil of the temple was torn in two from the top to the bottom. The earth quaked and the rocks were split.",40027051,"ChatCompletion(id='chatcmpl-8dJro1XD3WTR4Wx3Io2NJxn03f4Jj', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381852, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 331
  candidates_token_count: 1
  total_token_count: 332
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.5, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:51
1036,1036,"The tombs were opened, and many bodies of the saints who had fallen asleep were raised;",40027052,"ChatCompletion(id='chatcmpl-8dJrprJxsMMGibLoImeUyZEZRs5PA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381853, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.8, 0.2, 0.2, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.8, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Legal': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:52
1037,1037,"and coming out of the tombs after his resurrection, they entered into the holy city and appeared to many.",40027053,"ChatCompletion(id='chatcmpl-8dJrqhEI0DvfGURvPRCnA9tiL87I2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381854, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.5, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:53
1038,1038,"Now the centurion, and those who were with him watching Jesus, when they saw the earthquake, and the things that were done, feared exceedingly, saying, ""Truly this was the Son of God.""",40027054,"ChatCompletion(id='chatcmpl-8dJrqWIhL0vPCG388NvCYwO3ON7lq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381854, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=339, total_tokens=340))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 344
  candidates_token_count: 1
  total_token_count: 345
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:54
1039,1039,"Many women were there watching from afar, who had followed Jesus from Galilee, serving him.",40027055,"ChatCompletion(id='chatcmpl-8dJrrCQHzRZgqvLAGONkiU8X5yYnS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381855, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=315, total_tokens=316))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 321
  candidates_token_count: 1
  total_token_count: 322
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.1, 0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:55
1040,1040,"Among them were Mary Magdalene, Mary the mother of James and Joses, and the mother of the sons of Zebedee.",40027056,"ChatCompletion(id='chatcmpl-8dJrrIJVFWvcwwhLhEvkI6gTq0ATS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381855, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:56
1041,1041,"When evening had come, a rich man from Arimathaea, named Joseph, who himself was also Jesus' disciple came.",40027057,"ChatCompletion(id='chatcmpl-8dJrsHTz4aVSvSUVCComdekSMXYoR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381856, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.3, 0.2, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:57
1042,1042,"This man went to Pilate, and asked for Jesus' body. Then Pilate commanded the body to be given up.",40027058,"ChatCompletion(id='chatcmpl-8dJrsk9wd89Yp25zm8gR4UN5XILLM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381856, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 326
  candidates_token_count: 1
  total_token_count: 327
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.7, 0.2, 0.3, 0.3, 0.2, 0.3, 0.1, 0.1, 1.0, 0.3, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.7, 'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.3, 'Toxic': 0.2, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:58
1043,1043,"Joseph took the body, and wrapped it in a clean linen cloth,",40027059,"ChatCompletion(id='chatcmpl-8dJrsYjhphTHmM1X9dXsGJL9Fuxje', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381856, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=310, total_tokens=311))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 317
  candidates_token_count: 1
  total_token_count: 318
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.1, 0.6, 0.2, 0.1, 0.9, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.1, 'Health': 0.6, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.9, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:59
1044,1044,"and laid it in his own new tomb, which he had hewn out in the rock, and he rolled a great stone to the door of the tomb, and departed.",40027060,"ChatCompletion(id='chatcmpl-8dJrtczipCcaFmHN0QKF6JkKS9B8T', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381857, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.3, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.3, 'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:60
1045,1045,"Mary Magdalene was there, and the other Mary, sitting opposite the tomb.",40027061,"ChatCompletion(id='chatcmpl-8dJrtnp1bkfKKdKeMvTBCbzOooxHt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381857, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=313, total_tokens=314))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:61
1046,1046,"Now on the next day, which was the day after the Preparation Day, the chief priests and the Pharisees were gathered together to Pilate,",40027062,"ChatCompletion(id='chatcmpl-8dJrulqNbY1BxjG7K2HGeShDAIppH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381858, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=326, total_tokens=327))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.1, 0.3, 0.2, 0.6, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:62
1047,1047,"saying, ""Sir, we remember what that deceiver said while he was still alive: 'After three days I will rise again.'",40027063,"ChatCompletion(id='chatcmpl-8dJru3exadWHAfjkGIapCkyrSShyA', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381858, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=324, total_tokens=325))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.4, 0.4, 0.3, 0.1, 0.1, 1.0, 0.1, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.4, 'Insult': 0.4, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:63
1048,1048,"Command therefore that the tomb be made secure until the third day, lest perhaps his disciples come at night and steal him away, and tell the people, 'He is risen from the dead;' and the last deception will be worse than the first.""",40027064,"ChatCompletion(id='chatcmpl-8dJrvT11X6xdjoUS2todxmIpPgjUR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381859, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=345, total_tokens=346))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 352
  candidates_token_count: 1
  total_token_count: 353
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.3, 0.1, 0.2, 0.2, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'Violent': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:64
1049,1049,"Pilate said to them, ""You have a guard. Go, make it as secure as you can.""",40027065,"ChatCompletion(id='chatcmpl-8dJrvKVfkNOZflM5YzU2aTzefBxxY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381859, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=318, total_tokens=319))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.5, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.1, 0.2, 0.1, 0.3, 0.5, 0.6, 0.1, 0.6, 1.0, 0.2, 0.1, 0.1, 0.4], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.5, 'Politics': 0.6, 'Profanity': 0.1, 'Public Safety': 0.6, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'Violent': 0.1, 'War & Conflict': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:65
1050,1050,"So they went with the guard and made the tomb secure, sealing the stone.",40027066,"ChatCompletion(id='chatcmpl-8dJrwUlxoi1X8ngj0oBkCi8LAQZm6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381860, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=312, total_tokens=313))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 319
  candidates_token_count: 1
  total_token_count: 320
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.2, 0.1, 0.3, 0.2, 0.2, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 27:66
1051,1051,"Now after the Sabbath, as it began to dawn on the first day of the week, Mary Magdalene and the other Mary came to see the tomb.",40028001,"ChatCompletion(id='chatcmpl-8dJrwuOoY25SXKdI4zjOXHpwVuNDe', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381860, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=328, total_tokens=329))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 333
  candidates_token_count: 1
  total_token_count: 334
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.3, 0.2, 0.1, 0.1, 1.0, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:1
1052,1052,"Behold, there was a great earthquake, for an angel of the Lord descended from the sky, and came and rolled away the stone from the door, and sat on it.",40028002,"ChatCompletion(id='chatcmpl-8dJrxyvDa3MYnEK1bk8FeCkieUtFO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381861, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=332, total_tokens=333))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 338
  candidates_token_count: 1
  total_token_count: 339
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.6, 0.2, 0.1, 0.3, 0.2, 0.2, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.6, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:2
1053,1053,"His appearance was like lightning, and his clothing white as snow.",40028003,"ChatCompletion(id='chatcmpl-8dJrxCWGbU2zJA6nNxIMvGwk4Five', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381861, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=309, total_tokens=310))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 316
  candidates_token_count: 1
  total_token_count: 317
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.3, 0.2, 0.1, 0.7, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.3, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 0.7, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:3
1054,1054,"For fear of him, the guards shook, and became like dead men.",40028004,"ChatCompletion(id='chatcmpl-8dJryol76wNa3agg2inkWn3qyBzYy', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381862, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.7, 0.3, 0.4, 0.3, 0.2, 0.2, 0.2, 0.1, 0.8, 0.1, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.7, 'Derogatory': 0.3, 'Health': 0.4, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.1, 'Religion & Belief': 0.8, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:4
1055,1055,"The angel answered the women, ""Don't be afraid, for I know that you seek Jesus, who has been crucified.",40028005,"ChatCompletion(id='chatcmpl-8dJryD4ktT48qcmLRtibh3P2GuKY6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381862, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 20', _prediction_response=Prediction(predictions=[{'content': ' 20', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.2, 0.1, 1.0, 0.2, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic', 'Violent']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Health': 0.1, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1, 'Violent': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 20])",20,Matthew 28:5
1056,1056,"He is not here, for he has risen, just like he said. Come, see the place where the Lord was lying.",40028006,"ChatCompletion(id='chatcmpl-8dJrzH1n6jhdN9nuLdzuLaMd7XVL0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381863, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=322, total_tokens=323))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 329
  candidates_token_count: 1
  total_token_count: 330
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.3, 0.2, 0.1, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.3, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:6
1057,1057,"Go quickly and tell his disciples, 'He has risen from the dead, and behold, he goes before you into Galilee; there you will see him.' Behold, I have told you.""",40028007,"ChatCompletion(id='chatcmpl-8dJs0a3zNoFX7NGAImR2DJmi5djN9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381864, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=336, total_tokens=337))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 341
  candidates_token_count: 1
  total_token_count: 342
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.3, 0.2, 0.1, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.3, 'Insult': 0.2, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:7
1058,1058,"They departed quickly from the tomb with fear and great joy, and ran to bring his disciples word.",40028008,"ChatCompletion(id='chatcmpl-8dJs0XWbhVdfER3oe6gTBBAjr2jRt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381864, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 323
  candidates_token_count: 1
  total_token_count: 324
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.4, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Health', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.1, 'Derogatory': 0.2, 'Health': 0.4, 'Insult': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:8
1059,1059,"As they went to tell his disciples, behold, Jesus met them, saying, ""Rejoice!"" They came and took hold of his feet, and worshiped him.",40028009,"ChatCompletion(id='chatcmpl-8dJs0TYuUM80c028A12t8x7ZZZvv9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381864, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=331, total_tokens=332))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 337
  candidates_token_count: 1
  total_token_count: 338
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:9
1060,1060,"Then Jesus said to them, ""Don't be afraid. Go tell my brothers{The word for ""brothers"" here may be also correctly translated ""brothers and sisters"" or ""siblings.""} that they should go into Galilee, and there they will see me.""",40028010,"ChatCompletion(id='chatcmpl-8dJs1NOABqSJFP3LTtI0DjwAXdQ5A', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381865, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=352, total_tokens=353))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 357
  candidates_token_count: 1
  total_token_count: 358
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:10
1061,1061,"Now while they were going, behold, some of the guards came into the city, and told the chief priests all the things that had happened.",40028011,"ChatCompletion(id='chatcmpl-8dJs1LCfYMC2W8nPFBkWjfX3buVWi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381865, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.1, 0.2, 0.1, 0.3, 0.2, 0.3, 0.1, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.3, 'Legal': 0.2, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:11
1062,1062,"When they were assembled with the elders, and had taken counsel, they gave a large amount of silver to the soldiers,",40028012,"ChatCompletion(id='chatcmpl-8dJs2GQLpsyVHcZirgTm6HGTmwVfh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381866, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=320, total_tokens=321))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 327
  candidates_token_count: 1
  total_token_count: 328
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.8, 0.2, 0.1, 0.2, 0.1, 0.3, 0.1, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.8, 'Health': 0.2, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.1, 'Politics': 0.3, 'Profanity': 0.1, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:12
1063,1063,"saying, ""Say that his disciples came by night, and stole him away while we slept.",40028013,"ChatCompletion(id='chatcmpl-8dJs3cjbV8A5g71bTZEUNPaaMtebj', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381867, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: LOW
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 322
  candidates_token_count: 1
  total_token_count: 323
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.4, 'severityScore': 0.6, 'severity': 'MEDIUM', 'category': 'Dangerous Content'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.4, 'severityScore': 0.4, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.4, 0.1, 0.1, 0.1, 0.5, 0.2, 0.2, 0.2, 0.5, 1.0, 0.2, 0.2, 0.1], 'categories': ['Death, Harm & Tragedy', 'Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Death, Harm & Tragedy': 0.2, 'Derogatory': 0.4, 'Finance': 0.1, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.5, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.2, 'Public Safety': 0.5, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.2, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:13
1064,1064,"If this comes to the governor's ears, we will persuade him and make you free of worry.""",40028014,"ChatCompletion(id='chatcmpl-8dJs4BibkbBeXu0rKHz6BqdqnS6Ql', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381868, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=316, total_tokens=317))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 324
  candidates_token_count: 1
  total_token_count: 325
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.6, 0.5, 0.1, 0.2, 0.2, 0.8, 0.1, 1.0, 0.1, 0.1, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.6, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.8, 'Public Safety': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1, 'War & Conflict': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:14
1065,1065,"So they took the money and did as they were told. This saying was spread abroad among the Jews, and continues until this day.",40028015,"ChatCompletion(id='chatcmpl-8dJs4MTftDY1AabWItHw65aA1k6tn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381868, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=323, total_tokens=324))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 330
  candidates_token_count: 1
  total_token_count: 331
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.2, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.4, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Harassment'}, {'probabilityScore': 0.5, 'severityScore': 0.4, 'severity': 'MEDIUM', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.5, 0.8, 0.5, 0.1, 0.4, 0.2, 0.6, 0.2, 0.3, 1.0, 0.1, 0.2, 0.4], 'categories': ['Derogatory', 'Finance', 'Health', 'Illicit Drugs', 'Insult', 'Legal', 'Politics', 'Profanity', 'Public Safety', 'Religion & Belief', 'Sexual', 'Toxic', 'War & Conflict']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.5, 'Finance': 0.8, 'Health': 0.5, 'Illicit Drugs': 0.1, 'Insult': 0.4, 'Legal': 0.2, 'Politics': 0.6, 'Profanity': 0.2, 'Public Safety': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.2, 'War & Conflict': 0.4}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:15
1066,1066,"But the eleven disciples went into Galilee, to the mountain where Jesus had sent them.",40028016,"ChatCompletion(id='chatcmpl-8dJs5eNUwGjR84JM7Yv4xy6Jgh3ot', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381869, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=314, total_tokens=315))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 320
  candidates_token_count: 1
  total_token_count: 321
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:16
1067,1067,"When they saw him, they bowed down to him, but some doubted.",40028017,"ChatCompletion(id='chatcmpl-8dJs5VwKnwUT5Kt8HNnVapWkPrs5s', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381869, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=311, total_tokens=312))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 318
  candidates_token_count: 1
  total_token_count: 319
}
",0,"MultiCandidateTextGenerationResponse(text=' 10', _prediction_response=Prediction(predictions=[{'content': ' 10', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.1, 0.2, 0.2, 0.2, 1.0, 0.2, 0.1], 'categories': ['Derogatory', 'Finance', 'Health', 'Insult', 'Politics', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Finance': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Politics': 0.2, 'Religion & Belief': 1.0, 'Sexual': 0.2, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 10])",10,Matthew 28:17
1068,1068,"Jesus came to them and spoke to them, saying, ""All authority has been given to me in heaven and on earth.",40028018,"ChatCompletion(id='chatcmpl-8dJs6YvUW85HH9iW8Q14LNOOIfbRt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='0', role='assistant', function_call=None, tool_calls=None))], created=1704381870, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=321, total_tokens=322))",0,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 328
  candidates_token_count: 1
  total_token_count: 329
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.1, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.1, 0.2, 0.2, 0.2, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Legal', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.1, 'Health': 0.2, 'Insult': 0.2, 'Legal': 0.2, 'Politics': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:18
1069,1069,"Therefore go, and make disciples of all nations, baptizing them in the name of the Father and of the Son and of the Holy Spirit,",40028019,"ChatCompletion(id='chatcmpl-8dJs6AFHxFTHBZNFRrP7F2N9uOUmv', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381870, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=325, total_tokens=326))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.3, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Dangerous Content'}, {'probabilityScore': 0.3, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.3, 'severity': 'LOW', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.2, 0.3, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.2, 'Insult': 0.3, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:19
1070,1070,"teaching them to observe all things that I commanded you. Behold, I am with you always, even to the end of the age."" Amen.",40028020,"ChatCompletion(id='chatcmpl-8dJs7swQtgspcRtUrItgBKfwKrINx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='10', role='assistant', function_call=None, tool_calls=None))], created=1704381871, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=327, total_tokens=328))",10,"candidates {
  content {
    role: ""model""
    parts {
      text: ""0""
    }
  }
  finish_reason: STOP
  safety_ratings {
    category: HARM_CATEGORY_HARASSMENT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_HATE_SPEECH
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_SEXUALLY_EXPLICIT
    probability: NEGLIGIBLE
  }
  safety_ratings {
    category: HARM_CATEGORY_DANGEROUS_CONTENT
    probability: NEGLIGIBLE
  }
}
usage_metadata {
  prompt_token_count: 332
  candidates_token_count: 1
  total_token_count: 333
}
",0,"MultiCandidateTextGenerationResponse(text=' 0', _prediction_response=Prediction(predictions=[{'content': ' 0', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'blocked': False, 'safetyRatings': [{'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content'}, {'probabilityScore': 0.2, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Harassment'}, {'probabilityScore': 0.2, 'severityScore': 0.2, 'severity': 'NEGLIGIBLE', 'category': 'Hate Speech'}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit'}], 'scores': [0.2, 0.5, 0.2, 0.1, 1.0, 0.1, 0.1], 'categories': ['Derogatory', 'Health', 'Insult', 'Profanity', 'Religion & Belief', 'Sexual', 'Toxic']}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, errors=(), safety_attributes={'Derogatory': 0.2, 'Health': 0.5, 'Insult': 0.2, 'Profanity': 0.1, 'Religion & Belief': 1.0, 'Sexual': 0.1, 'Toxic': 0.1}, grounding_metadata=GroundingMetadata(citations=[], search_queries=[]), candidates=[ 0])",0,Matthew 28:20
